{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_423_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Tune Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forgot to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gONzKJ6-BjqA",
        "outputId": "d9395787-5371-4a18-8b5b-05a0ad70ca99"
      },
      "source": [
        "!git clone https://github.com/keras-team/keras-tuner\n",
        "!pip install keras-tuner"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'keras-tuner' already exists and is not an empty directory.\n",
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=7e91e45918e6db8b9f667f63c1b9b173f21535c9e14f49d3dcd6d2855dd0fe93\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=d78151c91489cbf507f2c1ceb35b2b3d4c19130e2644238a60cf333b04b38d94\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNw6g9eqAwG0"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from keras.activations import relu, sigmoid\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjhJSmnLAwG3"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    def load_quickdraw10():\n",
        "      URL_ = \"\"\n",
        "      path_to_zip = get_file('./quickdraw10.npz', origina=URL_, extract=False)\n",
        "      data = np.load(path_to_zip)\n",
        "\n",
        "      max_pixel_value = 255\n",
        "      X = data['arr_0']/max_pixel_value\n",
        "      Y-data['arr_1']\n",
        "\n",
        "      return train_test_split(X, Y, shuffle=True)\n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O64k7KrJAwG5",
        "outputId": "6d47c91f-202e-4c2d-92ba-22af12e9a20a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25427968/25421363 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdCX_9AqAwG6",
        "outputId": "a1680f49-373b-4e9a-d816-914623c9f155"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHBQoSPpAwG8",
        "outputId": "e8930b31-76ce-4d18-a0fe-21dcd88a7599"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzhgbpGsAwG8"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a complied keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUkvofL0AwHD"
      },
      "source": [
        "## Explore create_model\n",
        "\n",
        "Let's build a few different models in order to understand how the above code works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5buxT6tAwHF"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "FgJrGfBwAwHG"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1uszJOwAwHH",
        "outputId": "42b638d4-9b9a-4afa-c465-a00b9e1e3ca0"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEo52cG_AwHJ"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0722533c325d699f4842e874e43720e",
          "grade": false,
          "grade_id": "cell-99d563a291231a7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "N15GmV-9AwHK"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZz_0fexAwHL",
        "outputId": "81586cdf-9610-494e-bdc5-0f0b88c13f61"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
        "# The output layer must have 10 nodes because there are 10 labels to predict \n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 545)               273045    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 589)               321594    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 634)               374060    \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 678)               430530    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 723)               490917    \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 767)               555308    \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 812)               623616    \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 856)               695928    \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                8570      \n",
            "=================================================================\n",
            "Total params: 4,166,068\n",
            "Trainable params: 4,166,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUwKSVpfAwHM",
        "outputId": "0844956c-fe86-4988-90ab-566bae8d3cb0"
      },
      "source": [
        "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
        "\n",
        "model = create_model(n_layers=50, first_layer_nodes=500, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=True)\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_80 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 492)               246492    \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 484)               238612    \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 476)               230860    \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 468)               223236    \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 460)               215740    \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 452)               208372    \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 443)               200679    \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 435)               193140    \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 427)               186172    \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 419)               179332    \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 411)               172620    \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 403)               166036    \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 394)               159176    \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 386)               152470    \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 378)               146286    \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 370)               140230    \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 362)               134302    \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 354)               128502    \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 345)               122475    \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 337)               116602    \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 329)               111202    \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 321)               105930    \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 313)               100786    \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 305)               95770     \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 296)               90576     \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 288)               85536     \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 280)               80920     \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 272)               76432     \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 264)               72072     \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 256)               67840     \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 247)               63479     \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 239)               59272     \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 231)               55440     \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 223)               51736     \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 215)               48160     \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 207)               44712     \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 198)               41184     \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 190)               37810     \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 182)               34762     \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 174)               31842     \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 166)               29050     \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 158)               26386     \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 149)               23691     \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 141)               21150     \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 133)               18886     \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 125)               16750     \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 117)               14742     \n",
            "_________________________________________________________________\n",
            "dense_128 (Dense)            (None, 109)               12862     \n",
            "_________________________________________________________________\n",
            "dense_129 (Dense)            (None, 10)                1100      \n",
            "=================================================================\n",
            "Total params: 5,473,912\n",
            "Trainable params: 5,473,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYvYGpf1GFmH",
        "outputId": "907527a5-cd8a-4914-a5aa-393eeee1bd9f"
      },
      "source": [
        "model = create_model(n_layers=10, first_layer_nodes=100, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=True)\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_130 (Dense)            (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_131 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_132 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 160,310\n",
            "Trainable params: 160,310\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGWqOpsKFlFh",
        "outputId": "9ed5f082-71b5-4c41-9023-5a21c8cbcf9e"
      },
      "source": [
        "model = create_model(n_layers=10, first_layer_nodes=100, last_layer_nodes=300, act_funct='relu', negative_node_incrementation=True)\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_206 (Dense)            (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_207 (Dense)            (None, 123)               12423     \n",
            "_________________________________________________________________\n",
            "dense_208 (Dense)            (None, 145)               17980     \n",
            "_________________________________________________________________\n",
            "dense_209 (Dense)            (None, 167)               24382     \n",
            "_________________________________________________________________\n",
            "dense_210 (Dense)            (None, 189)               31752     \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 212)               40280     \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 234)               49842     \n",
            "_________________________________________________________________\n",
            "dense_213 (Dense)            (None, 256)               60160     \n",
            "_________________________________________________________________\n",
            "dense_214 (Dense)            (None, 278)               71446     \n",
            "_________________________________________________________________\n",
            "dense_215 (Dense)            (None, 10)                2790      \n",
            "=================================================================\n",
            "Total params: 389,555\n",
            "Trainable params: 389,555\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mePqfALUGUiC",
        "outputId": "842967c6-52a3-4d87-a121-81ad455628af"
      },
      "source": [
        "model = create_model(n_layers=10, first_layer_nodes=200, last_layer_nodes=50, act_funct='relu', negative_node_incrementation=False)\n",
        "model.summary()\n",
        "\n",
        "#Set negative_node_incrementation=True to make layers output shape shrink over epochs.\n",
        "#Set =False to make the shape grow over epochs\n",
        "#In both cases, first layer > last layer"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_224 (Dense)            (None, 200)               157000    \n",
            "_________________________________________________________________\n",
            "dense_225 (Dense)            (None, 217)               43617     \n",
            "_________________________________________________________________\n",
            "dense_226 (Dense)            (None, 234)               51012     \n",
            "_________________________________________________________________\n",
            "dense_227 (Dense)            (None, 250)               58750     \n",
            "_________________________________________________________________\n",
            "dense_228 (Dense)            (None, 267)               67017     \n",
            "_________________________________________________________________\n",
            "dense_229 (Dense)            (None, 284)               76112     \n",
            "_________________________________________________________________\n",
            "dense_230 (Dense)            (None, 300)               85500     \n",
            "_________________________________________________________________\n",
            "dense_231 (Dense)            (None, 317)               95417     \n",
            "_________________________________________________________________\n",
            "dense_232 (Dense)            (None, 334)               106212    \n",
            "_________________________________________________________________\n",
            "dense_233 (Dense)            (None, 10)                3350      \n",
            "=================================================================\n",
            "Total params: 743,987\n",
            "Trainable params: 743,987\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUdppZ0PAwHN"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IwgHlXHAwHO"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "606b85d0ba4531836f97caf6850297f8",
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "HGe_07FEAwHP"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=2, first_layer_nodes=500, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFq0nnPRAwHQ",
        "outputId": "f7f17843-f958-4639-9a8e-4f5e352bbde7"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_234 (Dense)            (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_235 (Dense)            (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiCSO_sbAwHQ"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J9_fdF5AwHS"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EQa8zJDAwHU",
        "outputId": "a63db395-ca58-49ad-af05-cb3b304f0c07"
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 2ms/step - loss: 0.8277 - accuracy: 0.7491\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4570 - accuracy: 0.8659\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3620 - accuracy: 0.8920\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4603 - accuracy: 0.8662\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8314 - accuracy: 0.7443\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4493 - accuracy: 0.8665\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3496 - accuracy: 0.8958\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4830 - accuracy: 0.8555\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8244 - accuracy: 0.7452\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4515 - accuracy: 0.8625\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3608 - accuracy: 0.8914\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4726 - accuracy: 0.8641\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8009 - accuracy: 0.7525\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4348 - accuracy: 0.8672\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3373 - accuracy: 0.8959\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4542 - accuracy: 0.8654\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7964 - accuracy: 0.7501\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4341 - accuracy: 0.8666\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3350 - accuracy: 0.8968\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4569 - accuracy: 0.8701\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7983 - accuracy: 0.7509\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4344 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3462 - accuracy: 0.8915\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4725 - accuracy: 0.8616\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8304 - accuracy: 0.7445\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4565 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3588 - accuracy: 0.8929\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4660 - accuracy: 0.8651\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8229 - accuracy: 0.7526\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4536 - accuracy: 0.8660\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3530 - accuracy: 0.8958\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4678 - accuracy: 0.8646\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8271 - accuracy: 0.7476\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4513 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3505 - accuracy: 0.8949\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4714 - accuracy: 0.8633\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7991 - accuracy: 0.7549\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4332 - accuracy: 0.8686\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3412 - accuracy: 0.8952\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4514 - accuracy: 0.8663\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7885 - accuracy: 0.7553\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4258 - accuracy: 0.8719\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3323 - accuracy: 0.8974\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4499 - accuracy: 0.8668\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7988 - accuracy: 0.7488\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4236 - accuracy: 0.8706\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3312 - accuracy: 0.8972\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4584 - accuracy: 0.8676\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8617 - accuracy: 0.7362\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4774 - accuracy: 0.8600\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3923 - accuracy: 0.8844\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4663 - accuracy: 0.8638\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8557 - accuracy: 0.7402\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4686 - accuracy: 0.8625\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3801 - accuracy: 0.8856\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4748 - accuracy: 0.8634\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8609 - accuracy: 0.7333\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4707 - accuracy: 0.8584\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3772 - accuracy: 0.8868\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4871 - accuracy: 0.8584\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8391 - accuracy: 0.7419\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4629 - accuracy: 0.8613\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3647 - accuracy: 0.8880\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4596 - accuracy: 0.8636\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8313 - accuracy: 0.7410\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4540 - accuracy: 0.8625\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3534 - accuracy: 0.8916\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4751 - accuracy: 0.8618\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8224 - accuracy: 0.7415\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4601 - accuracy: 0.8604\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3573 - accuracy: 0.8881\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4767 - accuracy: 0.8614\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8642 - accuracy: 0.7343\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4859 - accuracy: 0.8579\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3921 - accuracy: 0.8858\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4690 - accuracy: 0.8636\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8685 - accuracy: 0.7325\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4816 - accuracy: 0.8566\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3842 - accuracy: 0.8870\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4753 - accuracy: 0.8622\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8611 - accuracy: 0.7399\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4769 - accuracy: 0.8582\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3821 - accuracy: 0.8871\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.8623\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8229 - accuracy: 0.7454\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4535 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3632 - accuracy: 0.8890\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4994 - accuracy: 0.8559\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8416 - accuracy: 0.7387\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4564 - accuracy: 0.8624\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3535 - accuracy: 0.8941\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4706 - accuracy: 0.8660\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8343 - accuracy: 0.7403\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4598 - accuracy: 0.8596\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3567 - accuracy: 0.8919\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4749 - accuracy: 0.8569\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done  24 out of  24 | elapsed:  4.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.7357 - accuracy: 0.7727\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.4090 - accuracy: 0.8754\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.3236 - accuracy: 0.8999\n",
            "Best: 0.8669066826502482 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8619333306948344, Stdev: 0.0046107396478919874 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8656799991925558, Stdev: 0.003469336333889807 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8643466631571451, Stdev: 0.0007478587239849952 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8669066826502482, Stdev: 0.0005084248605816999 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8618933161099752, Stdev: 0.0024462951376042095 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8622533480326334, Stdev: 0.000994024029015595 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8626933495203654, Stdev: 0.0006130467003674861 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8595733443895975, Stdev: 0.004534463556289164 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huyag1FhAwHV"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u45ZGg_AwHW",
        "outputId": "221bbb79-7371-4da1-8aa4-aca2dc5c2b18"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 50,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r16B5AQ_N6Yp"
      },
      "source": [
        "Grid Search took 4.5 minutes to run.\n",
        "\n",
        "Best model parameters:\n",
        "\n",
        " 'epochs': 3,\n",
        "\n",
        " 'first_layer_nodes': 500,\n",
        "\n",
        " 'last_layer_nodes': 50,\n",
        "\n",
        " 'n_layers': 3\n",
        "\n",
        "Q: what does the standard deviation signify in the cross validation? The spread of accuracy scores was focused, narrow? Is that good?\n",
        "\n",
        "What is the difference between growing vw reducing the number of \n",
        "nodes per epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3HgBCMeAwHX"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2\n",
        "\n",
        "## Benchmark different Optimization Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "- Brute Force Gridsearch\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t1HzyZ_AwHY"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7nlyH4rAwHY"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O5XPRs72AwHZ",
        "outputId": "c8bde216-5d64-4ecf-a119-6d2428e7551f"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TauWE0sAwHZ"
      },
      "source": [
        "------\n",
        "# Run the Gridsearch Algorithms \n",
        "\n",
        "### Random Search\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-60Y8AV4AwHa",
        "outputId": "55e47b91-8d00-4bc8-c9e5-0ee9c951ebf0"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "n_unique_hparam_combos = 16*3*2\n",
        "n_unique_hparam_combos"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9naXhW1AwHa",
        "outputId": "4582b1e0-aa69-40eb-b3ad-5f42f6343c24"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "n_param_combos_to_sample = n_unique_hparam_combos*.25\n",
        "n_param_combos_to_sample"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9YjiT-SAwHb"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-lyWSXYAwHb",
        "outputId": "ace20a45-7acb-4cb6-aba6-e7cea50a068f"
      },
      "source": [
        "# take note of Total elapsed time in print out\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 23 Complete [00h 00m 17s]\n",
            "val_accuracy: 0.8623600006103516\n",
            "\n",
            "Best val_accuracy So Far: 0.872439980506897\n",
            "Total elapsed time: 00h 06m 48s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuMprawqAwHc",
        "outputId": "40c14150-eb8d-47c5-e217-ea5ea45dc857"
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.872439980506897\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8679999709129333\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8676400184631348\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8628000020980835\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8623600006103516\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8607199788093567\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8524399995803833\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8399199843406677\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8309199810028076\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8294399976730347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__wAiXdAwHc"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Yb9RaIKGAwHd"
      },
      "source": [
        "The Random Search took 6 min 48 sec.\n",
        "\n",
        "The best parameters were:\n",
        "\n",
        "Objective(name='val_accuracy', direction='max')\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "units: 384\n",
        "\n",
        "learning_rate: 0.001\n",
        "\n",
        "activation: relu\n",
        "\n",
        "Score: 0.872439980506897"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PERJVQc3AwHd"
      },
      "source": [
        "------\n",
        "### Bayesian Optimization\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxL74A9PAwHe"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
        "# feel free to play with any of these numbers\n",
        "max_trials=15\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxZbMSdRAwHe"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeKF8GgeAwHf",
        "outputId": "97f4a8e4-41bb-40aa-f76a-c1ba311f9e51"
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 15 Complete [00h 00m 16s]\n",
            "val_accuracy: 0.8289200067520142\n",
            "\n",
            "Best val_accuracy So Far: 0.872160017490387\n",
            "Total elapsed time: 00h 04m 12s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYyr9NzSAwHf",
        "outputId": "d9b8774b-92fb-4c38-91a5-6e00540fd3a0"
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.872160017490387\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8702800273895264\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8639600276947021\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8566799759864807\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8372799754142761\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.829200029373169\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8289200067520142\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8268399834632874\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8148400187492371\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8124399781227112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EhK5A63AwHg"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c1d4QdujAwHg"
      },
      "source": [
        "Bayesian Optimization took 4 min 12 sec to run.\n",
        "\n",
        "Best model:\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "units: 352\n",
        "\n",
        "learning_rate: 0.001\n",
        "\n",
        "activation: relu\n",
        "\n",
        "Score: 0.872160017490387"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dwsNv0TAwHh"
      },
      "source": [
        "---------\n",
        "## Brute Force Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ROYPmAAwHi"
      },
      "source": [
        "### Populate a Sklearn compatiable parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YajLMGi1AwHi"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 544, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RwEMlIfAwHk",
        "outputId": "dd435439-a3ce-48cd-f683-1b0171e2e5fc"
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480,\n",
              "  512]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQszZReXAwHl"
      },
      "source": [
        "### Build a Sklearn compatiable model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOcCoi_7AwHm"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUQmR4iwAwHm"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXdeXCRGAwHn",
        "outputId": "e476cfa8-cfc7-42e3-ce21-c2508e033682"
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0145 - accuracy: 0.3000\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9324 - accuracy: 0.2640\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0005 - accuracy: 0.2871\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0456 - accuracy: 0.2300\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2855 - accuracy: 0.1731\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.3080 - accuracy: 0.1036\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.3674 - accuracy: 0.1714\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.2988 - accuracy: 0.1334\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0885 - accuracy: 0.2759\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8968 - accuracy: 0.2695\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2518 - accuracy: 0.2379\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9726 - accuracy: 0.2473\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9885 - accuracy: 0.3510\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9425 - accuracy: 0.2720\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2784 - accuracy: 0.2712\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0532 - accuracy: 0.2316\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1985 - accuracy: 0.3595\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8249 - accuracy: 0.3273\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2680 - accuracy: 0.3109\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8425 - accuracy: 0.2802\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.3906 - accuracy: 0.2830\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9163 - accuracy: 0.2632\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1057 - accuracy: 0.3791\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7867 - accuracy: 0.3227\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2426 - accuracy: 0.3469\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9812 - accuracy: 0.3143\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.3512 - accuracy: 0.3540\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8890 - accuracy: 0.2814\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1691 - accuracy: 0.3466\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.1282 - accuracy: 0.2079\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2790 - accuracy: 0.3823\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8220 - accuracy: 0.3756\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5875 - accuracy: 0.2602\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0873 - accuracy: 0.2438\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5751 - accuracy: 0.3562\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0841 - accuracy: 0.2292\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5746 - accuracy: 0.3458\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8660 - accuracy: 0.2914\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.4724 - accuracy: 0.3559\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8984 - accuracy: 0.3328\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.7659 - accuracy: 0.2624\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0127 - accuracy: 0.2545\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.7983 - accuracy: 0.3353\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8373 - accuracy: 0.3129\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.4452 - accuracy: 0.3402\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8468 - accuracy: 0.3032\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.4487 - accuracy: 0.4143\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9161 - accuracy: 0.2963\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1541 - accuracy: 0.4134\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8755 - accuracy: 0.3120\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.6920 - accuracy: 0.3584\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8462 - accuracy: 0.3388\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1133 - accuracy: 0.3200\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8453 - accuracy: 0.3100\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.9946 - accuracy: 0.3344\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8108 - accuracy: 0.3105\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5890 - accuracy: 0.3589\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9146 - accuracy: 0.3122\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.2147 - accuracy: 0.3137\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8654 - accuracy: 0.3130\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.4724 - accuracy: 0.3517\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7978 - accuracy: 0.3240\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.9127 - accuracy: 0.3495\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8041 - accuracy: 0.3239\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1567 - accuracy: 0.3423\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8958 - accuracy: 0.2903\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1477 - accuracy: 0.3557\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9402 - accuracy: 0.2979\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1713 - accuracy: 0.3631\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8809 - accuracy: 0.3211\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.7447 - accuracy: 0.3905\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7516 - accuracy: 0.3636\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.5070 - accuracy: 0.3161\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8769 - accuracy: 0.3199\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1672 - accuracy: 0.3691\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9477 - accuracy: 0.3328\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.0964 - accuracy: 0.3643\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9612 - accuracy: 0.2754\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.4095 - accuracy: 0.3367\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8553 - accuracy: 0.2962\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.3483 - accuracy: 0.4447\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.1195 - accuracy: 0.2272\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.4333 - accuracy: 0.3588\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9155 - accuracy: 0.2729\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.2388 - accuracy: 0.3326\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9777 - accuracy: 0.2486\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.0146 - accuracy: 0.3978\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0561 - accuracy: 0.3235\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1752 - accuracy: 0.3377\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9652 - accuracy: 0.2555\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.2512 - accuracy: 0.3612\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.1774 - accuracy: 0.2388\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.1277 - accuracy: 0.3683\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8674 - accuracy: 0.3136\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.5709 - accuracy: 0.3735\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9044 - accuracy: 0.3862\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9178 - accuracy: 0.7200\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7788 - accuracy: 0.7695\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9237 - accuracy: 0.7147\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7177 - accuracy: 0.7804\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8989 - accuracy: 0.7222\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6881 - accuracy: 0.7960\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8820 - accuracy: 0.7284\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.7980\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8783 - accuracy: 0.7246\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6588 - accuracy: 0.8042\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8654 - accuracy: 0.7336\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.7944\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8633 - accuracy: 0.7333\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.7957\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8688 - accuracy: 0.7323\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6689 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8697 - accuracy: 0.7323\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8612 - accuracy: 0.7355\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6860 - accuracy: 0.8046\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8468 - accuracy: 0.7358\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8600 - accuracy: 0.7300\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6912 - accuracy: 0.7964\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8631 - accuracy: 0.7361\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6140 - accuracy: 0.8162\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8763 - accuracy: 0.7328\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6861 - accuracy: 0.7927\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8526 - accuracy: 0.7360\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6686 - accuracy: 0.8025\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8796 - accuracy: 0.7285\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6664 - accuracy: 0.7928\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8599 - accuracy: 0.7384\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6511 - accuracy: 0.8071\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8647 - accuracy: 0.7323\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6797 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8758 - accuracy: 0.7298\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6956 - accuracy: 0.7782\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8773 - accuracy: 0.7304\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6518 - accuracy: 0.8101\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8361 - accuracy: 0.7435\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6702 - accuracy: 0.7987\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8526 - accuracy: 0.7392\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6661 - accuracy: 0.8005\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8686 - accuracy: 0.7340\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6675 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8650 - accuracy: 0.7337\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6682 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8686 - accuracy: 0.7323\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6552 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8501 - accuracy: 0.7423\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8582 - accuracy: 0.7432\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6709 - accuracy: 0.8113\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8756 - accuracy: 0.7316\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6491 - accuracy: 0.8096\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8553 - accuracy: 0.7384\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6464 - accuracy: 0.8131\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8523 - accuracy: 0.7362\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6621 - accuracy: 0.8109\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8642 - accuracy: 0.7340\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6857 - accuracy: 0.8006\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8733 - accuracy: 0.7349\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.8022\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8635 - accuracy: 0.7347\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6867 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8791 - accuracy: 0.7341\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6904 - accuracy: 0.7923\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8865 - accuracy: 0.7317\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6706 - accuracy: 0.8006\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8911 - accuracy: 0.7346\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6467 - accuracy: 0.8121\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8696 - accuracy: 0.7315\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6889 - accuracy: 0.7958\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8760 - accuracy: 0.7355\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6828 - accuracy: 0.7946\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8639 - accuracy: 0.7359\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.7943\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8734 - accuracy: 0.7376\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6764 - accuracy: 0.7963\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8809 - accuracy: 0.7326\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6910 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8880 - accuracy: 0.7318\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6710 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8844 - accuracy: 0.7340\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7117 - accuracy: 0.7876\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8793 - accuracy: 0.7324\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6953 - accuracy: 0.8034\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8729 - accuracy: 0.7379\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.8041\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8480 - accuracy: 0.7434\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6731 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8744 - accuracy: 0.7345\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6806 - accuracy: 0.8042\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8734 - accuracy: 0.7356\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6650 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0873 - accuracy: 0.6652\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7152 - accuracy: 0.7922\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0699 - accuracy: 0.6722\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7085 - accuracy: 0.7926\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0666 - accuracy: 0.6730\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7358 - accuracy: 0.7840\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9936 - accuracy: 0.6942\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.8072\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0004 - accuracy: 0.6902\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.8030\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9718 - accuracy: 0.6998\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6517 - accuracy: 0.8098\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9534 - accuracy: 0.7070\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.8218\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9705 - accuracy: 0.7037\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6012 - accuracy: 0.8226\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9454 - accuracy: 0.7119\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6101 - accuracy: 0.8243\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9303 - accuracy: 0.7167\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6163 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9273 - accuracy: 0.7149\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5950 - accuracy: 0.8243\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9352 - accuracy: 0.7162\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5947 - accuracy: 0.8268\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9091 - accuracy: 0.7235\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5875 - accuracy: 0.8270\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9032 - accuracy: 0.7231\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5796 - accuracy: 0.8276\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9099 - accuracy: 0.7226\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5778 - accuracy: 0.8308\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8945 - accuracy: 0.7262\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5656 - accuracy: 0.8332\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9000 - accuracy: 0.7281\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5638 - accuracy: 0.8333\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8935 - accuracy: 0.7279\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5758 - accuracy: 0.8296\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8918 - accuracy: 0.7291\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5564 - accuracy: 0.8372\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8670 - accuracy: 0.7394\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5587 - accuracy: 0.8362\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8804 - accuracy: 0.7315\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5570 - accuracy: 0.8373\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8716 - accuracy: 0.7368\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5798 - accuracy: 0.8225\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8741 - accuracy: 0.7332\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5568 - accuracy: 0.8367\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8696 - accuracy: 0.7344\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5485 - accuracy: 0.8394\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8619 - accuracy: 0.7383\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.8412\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8649 - accuracy: 0.7346\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5478 - accuracy: 0.8381\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8708 - accuracy: 0.7365\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5472 - accuracy: 0.8413\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8607 - accuracy: 0.7373\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5516 - accuracy: 0.8404\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8565 - accuracy: 0.7382\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.8350\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8547 - accuracy: 0.7399\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5497 - accuracy: 0.8390\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8452 - accuracy: 0.7426\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5495 - accuracy: 0.8351\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8444 - accuracy: 0.7407\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5353 - accuracy: 0.8434\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8468 - accuracy: 0.7444\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5464 - accuracy: 0.8402\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8593 - accuracy: 0.7377\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.8410\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8396 - accuracy: 0.7423\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5306 - accuracy: 0.8437\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8319 - accuracy: 0.7450\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5396 - accuracy: 0.8406\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8368 - accuracy: 0.7418\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5304 - accuracy: 0.8404\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8522 - accuracy: 0.7403\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5255 - accuracy: 0.8444\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8374 - accuracy: 0.7450\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5518 - accuracy: 0.8343\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8525 - accuracy: 0.7385\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5508 - accuracy: 0.8375\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8378 - accuracy: 0.7444\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5454 - accuracy: 0.8368\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8356 - accuracy: 0.7450\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5372 - accuracy: 0.8413\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8320 - accuracy: 0.7468\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.8408\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8318 - accuracy: 0.7441\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5246 - accuracy: 0.8429\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8258 - accuracy: 0.7456\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.8382\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8401 - accuracy: 0.7412\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5351 - accuracy: 0.8409\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8214 - accuracy: 0.7471\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5472 - accuracy: 0.8376\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8302 - accuracy: 0.7500\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5219 - accuracy: 0.8463\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2631 - accuracy: 0.5834\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0618 - accuracy: 0.6568\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4573 - accuracy: 0.4856\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1948 - accuracy: 0.6187\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2492 - accuracy: 0.5920\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1835 - accuracy: 0.6106\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3600 - accuracy: 0.5680\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0588 - accuracy: 0.6764\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2056 - accuracy: 0.6184\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0390 - accuracy: 0.6708\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4855 - accuracy: 0.5221\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1996 - accuracy: 0.5946\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3228 - accuracy: 0.5848\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0891 - accuracy: 0.6634\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3637 - accuracy: 0.5744\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1242 - accuracy: 0.6423\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2835 - accuracy: 0.5935\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1508 - accuracy: 0.6294\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3404 - accuracy: 0.5912\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1322 - accuracy: 0.6590\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3496 - accuracy: 0.5853\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0669 - accuracy: 0.6645\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3013 - accuracy: 0.5945\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1529 - accuracy: 0.6210\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4060 - accuracy: 0.5817\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2356 - accuracy: 0.6282\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4696 - accuracy: 0.5701\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1630 - accuracy: 0.6726\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4797 - accuracy: 0.5603\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1862 - accuracy: 0.6559\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4342 - accuracy: 0.5830\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0073 - accuracy: 0.6792\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5016 - accuracy: 0.5699\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2721 - accuracy: 0.6216\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.6314 - accuracy: 0.5450\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1408 - accuracy: 0.6793\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3346 - accuracy: 0.5942\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1470 - accuracy: 0.6521\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7338 - accuracy: 0.5373\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3420 - accuracy: 0.6386\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4731 - accuracy: 0.5785\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1075 - accuracy: 0.6786\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7298 - accuracy: 0.5571\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3248 - accuracy: 0.6326\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7893 - accuracy: 0.5209\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.5388 - accuracy: 0.6078\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8438 - accuracy: 0.5420\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8610 - accuracy: 0.5739\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8717 - accuracy: 0.5492\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2266 - accuracy: 0.6542\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.6107 - accuracy: 0.5484\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1876 - accuracy: 0.6758\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8378 - accuracy: 0.5396\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2455 - accuracy: 0.6377\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8476 - accuracy: 0.5421\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6118 - accuracy: 0.5816\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7331 - accuracy: 0.5446\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1344 - accuracy: 0.6609\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5946 - accuracy: 0.5750\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1836 - accuracy: 0.6336\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5138 - accuracy: 0.5835\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0890 - accuracy: 0.6870\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.3672 - accuracy: 0.5096\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6295 - accuracy: 0.5950\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1178 - accuracy: 0.5327\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3604 - accuracy: 0.6480\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5861 - accuracy: 0.5819\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1962 - accuracy: 0.6611\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5798 - accuracy: 0.5209\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7995 - accuracy: 0.6038\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.3214 - accuracy: 0.5155\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6894 - accuracy: 0.5445\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2965 - accuracy: 0.5105\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7494 - accuracy: 0.5404\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0977 - accuracy: 0.5355\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6209 - accuracy: 0.6002\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5767 - accuracy: 0.5611\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4274 - accuracy: 0.5900\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8520 - accuracy: 0.5444\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1230 - accuracy: 0.6552\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5626 - accuracy: 0.5140\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9902 - accuracy: 0.5862\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0786 - accuracy: 0.5177\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4790 - accuracy: 0.6310\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 3.0524 - accuracy: 0.4879\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.5569 - accuracy: 0.4829\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8075 - accuracy: 0.5584\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2952 - accuracy: 0.6326\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7710 - accuracy: 0.5644\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4976 - accuracy: 0.6176\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7812 - accuracy: 0.5751\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2106 - accuracy: 0.6454\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.8887 - accuracy: 0.5082\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5448 - accuracy: 0.6429\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.5263 - accuracy: 0.5401\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.2129 - accuracy: 0.5858\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9439 - accuracy: 0.7093\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6996 - accuracy: 0.7898\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9590 - accuracy: 0.7018\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6919 - accuracy: 0.7902\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9408 - accuracy: 0.7055\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7003 - accuracy: 0.7879\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8718 - accuracy: 0.7279\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6399 - accuracy: 0.8088\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8682 - accuracy: 0.7296\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8610 - accuracy: 0.7362\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6461 - accuracy: 0.8062\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8413 - accuracy: 0.7348\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6168 - accuracy: 0.8131\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8452 - accuracy: 0.7380\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6117 - accuracy: 0.8153\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8451 - accuracy: 0.7324\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6244 - accuracy: 0.8105\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8313 - accuracy: 0.7401\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6046 - accuracy: 0.8163\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8493 - accuracy: 0.7345\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6044 - accuracy: 0.8168\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8354 - accuracy: 0.7392\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.8196\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8344 - accuracy: 0.7390\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.8186\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8223 - accuracy: 0.7441\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.8121\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8259 - accuracy: 0.7422\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.8172\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8212 - accuracy: 0.7451\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.8175\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8262 - accuracy: 0.7415\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6141 - accuracy: 0.8115\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8254 - accuracy: 0.7431\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5824 - accuracy: 0.8286\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8247 - accuracy: 0.7449\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5851 - accuracy: 0.8230\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8243 - accuracy: 0.7428\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5851 - accuracy: 0.8230\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8352 - accuracy: 0.7390\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.8158\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8267 - accuracy: 0.7409\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6036 - accuracy: 0.8145\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8375 - accuracy: 0.7401\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5890 - accuracy: 0.8230\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8341 - accuracy: 0.7396\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5865 - accuracy: 0.8225\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8322 - accuracy: 0.7385\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6158 - accuracy: 0.8131\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8350 - accuracy: 0.7409\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6143 - accuracy: 0.8134\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8457 - accuracy: 0.7403\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5850 - accuracy: 0.8259\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8420 - accuracy: 0.7390\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5935 - accuracy: 0.8196\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8367 - accuracy: 0.7404\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8599 - accuracy: 0.7385\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5915 - accuracy: 0.8175\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8361 - accuracy: 0.7433\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6028 - accuracy: 0.8172\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8572 - accuracy: 0.7375\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5943 - accuracy: 0.8192\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8507 - accuracy: 0.7362\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.8195\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8253 - accuracy: 0.7433\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.8097\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8501 - accuracy: 0.7408\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6043 - accuracy: 0.8155\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8461 - accuracy: 0.7389\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5878 - accuracy: 0.8232\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8382 - accuracy: 0.7409\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.8096\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8385 - accuracy: 0.7421\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6314 - accuracy: 0.8096\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8437 - accuracy: 0.7411\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5961 - accuracy: 0.8194\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8774 - accuracy: 0.7347\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5830 - accuracy: 0.8245\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8329 - accuracy: 0.7444\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6086 - accuracy: 0.8198\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8355 - accuracy: 0.7452\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6092 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8414 - accuracy: 0.7424\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6148 - accuracy: 0.8134\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8697 - accuracy: 0.7357\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6007 - accuracy: 0.8206\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8570 - accuracy: 0.7358\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.8216\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8849 - accuracy: 0.7367\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6329 - accuracy: 0.8094\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8687 - accuracy: 0.7394\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6092 - accuracy: 0.8138\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8506 - accuracy: 0.7393\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5928 - accuracy: 0.8219\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3083 - accuracy: 0.6284\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.8080 - accuracy: 0.7652\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3457 - accuracy: 0.6188\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.8081 - accuracy: 0.7617\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3278 - accuracy: 0.6296\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.8194 - accuracy: 0.7550\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1739 - accuracy: 0.6636\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7545 - accuracy: 0.7754\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1691 - accuracy: 0.6650\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7434 - accuracy: 0.7804\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1894 - accuracy: 0.6468\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7613 - accuracy: 0.7726\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1251 - accuracy: 0.6752\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7341 - accuracy: 0.7871\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1089 - accuracy: 0.6798\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7267 - accuracy: 0.7834\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1214 - accuracy: 0.6728\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7307 - accuracy: 0.7852\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0926 - accuracy: 0.6809\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7113 - accuracy: 0.7910\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0896 - accuracy: 0.6779\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7183 - accuracy: 0.7859\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1077 - accuracy: 0.6715\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7243 - accuracy: 0.7850\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0621 - accuracy: 0.6903\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7036 - accuracy: 0.7913\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0667 - accuracy: 0.6822\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.7962\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0528 - accuracy: 0.6857\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7002 - accuracy: 0.7936\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0383 - accuracy: 0.6905\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6950 - accuracy: 0.7978\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0351 - accuracy: 0.6951\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6840 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0619 - accuracy: 0.6856\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6961 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0305 - accuracy: 0.6900\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0242 - accuracy: 0.6980\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.7958\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0306 - accuracy: 0.6881\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0330 - accuracy: 0.6861\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6862 - accuracy: 0.7985\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0129 - accuracy: 0.7013\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6930 - accuracy: 0.7921\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0109 - accuracy: 0.6974\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6879 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0221 - accuracy: 0.6936\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6774 - accuracy: 0.7995\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0180 - accuracy: 0.6929\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6780 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0238 - accuracy: 0.6920\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6829 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0112 - accuracy: 0.6921\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6732 - accuracy: 0.8049\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0047 - accuracy: 0.7004\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.7925\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0152 - accuracy: 0.6931\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6838 - accuracy: 0.7985\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9959 - accuracy: 0.6971\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6721 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0091 - accuracy: 0.7002\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6768 - accuracy: 0.7966\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0111 - accuracy: 0.6950\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6764 - accuracy: 0.7952\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9951 - accuracy: 0.7005\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6746 - accuracy: 0.8027\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9928 - accuracy: 0.6986\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6974 - accuracy: 0.7913\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9962 - accuracy: 0.6951\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6780 - accuracy: 0.7996\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9880 - accuracy: 0.6983\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6816 - accuracy: 0.7979\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9999 - accuracy: 0.6993\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6620 - accuracy: 0.8034\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9932 - accuracy: 0.6989\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6793 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9853 - accuracy: 0.6999\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6703 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9864 - accuracy: 0.7039\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6657 - accuracy: 0.8031\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9908 - accuracy: 0.6983\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6747 - accuracy: 0.8012\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9949 - accuracy: 0.6973\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6680 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9931 - accuracy: 0.6972\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6670 - accuracy: 0.8030\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9838 - accuracy: 0.7023\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6845 - accuracy: 0.8021\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9833 - accuracy: 0.7028\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6885 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9851 - accuracy: 0.7009\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6752 - accuracy: 0.7996\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9744 - accuracy: 0.7049\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6660 - accuracy: 0.8027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done 288 out of 288 | elapsed: 23.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.7795 - accuracy: 0.7636\n",
            "Best: 0.8417600194613138 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.19918666779994965, Stdev: 0.06902166947507711 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.2167333314816157, Stdev: 0.05959051279704337 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.27693333725134534, Stdev: 0.039235448417879855 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.2886799971262614, Stdev: 0.025050496521315434 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.26786666611830395, Stdev: 0.04449517772227378 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.2828666716814041, Stdev: 0.06584271158598609 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.29290666182835895, Stdev: 0.03197543044886708 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.3041599988937378, Stdev: 0.006808078675750427 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.3202666739622752, Stdev: 0.013157683586241522 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.31191999713579815, Stdev: 0.0010374339731932377 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.3127333422501882, Stdev: 0.01584862834575221 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.3275333344936371, Stdev: 0.027231465780340403 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.30937333901723224, Stdev: 0.024591512872430773 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.2654533286889394, Stdev: 0.02865445454959554 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.27586666742960614, Stdev: 0.03378603583678221 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.312866672873497, Stdev: 0.06016147059167304 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.78193332751592, Stdev: 0.010867517700669972 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.7988666494687399, Stdev: 0.004047503115350007 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8004533251126608, Stdev: 0.0037169626477551636 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8028133312861124, Stdev: 0.004657469289693666 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8038133382797241, Stdev: 0.009648174063380548 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8008400201797485, Stdev: 0.0059990264811497504 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.7956533233324686, Stdev: 0.013210481739837411 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8021600047747294, Stdev: 0.0027223444097212424 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8040666778882345, Stdev: 0.005209872784324655 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8112133344014486, Stdev: 0.0014196178865682378 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8015733361244202, Stdev: 0.0007195111140263301 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8016800085703532, Stdev: 0.008119314383899349 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.794920007387797, Stdev: 0.0006669464762699005 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.7991733352343241, Stdev: 0.0027013680074270377 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.7983733216921488, Stdev: 0.007594697693546337 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7996933261553446, Stdev: 0.003740984502160687 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7896266579627991, Stdev: 0.003953114209893531 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8066533406575521, Stdev: 0.00283152461358018 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8228933413823446, Stdev: 0.001023645220624093 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8233199914296468, Stdev: 0.003357626565330905 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.8284666736920675, Stdev: 0.0016376634764613154 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.832039992014567, Stdev: 0.0017260473659528313 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8369200030962626, Stdev: 0.00048333034964322347 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8328533371289571, Stdev: 0.007418617738534981 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8401733239491781, Stdev: 0.00148102162777122 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8381733099619547, Stdev: 0.0022881353249959826 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8395866751670837, Stdev: 0.003444066942473436 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8417600194613138, Stdev: 0.0013937002099415879 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.839679996172587, Stdev: 0.004146507108489143 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8385066588719686, Stdev: 0.001982944928465562 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8405999938646952, Stdev: 0.001930263492253531 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8415866494178772, Stdev: 0.00361098620817825 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
            "Means: 0.6287199854850769, Stdev: 0.020127690002130716 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6472533345222473, Stdev: 0.037329868131540746 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6450133323669434, Stdev: 0.014030522963566014 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6481333176294962, Stdev: 0.019346137772810405 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.652239998181661, Stdev: 0.01827972076537981 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6600533326466879, Stdev: 0.027162353382227885 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6564666628837585, Stdev: 0.016616669860438887 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.6047733426094055, Stdev: 0.02405141799555726 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6559066772460938, Stdev: 0.015611369770039523 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6253600120544434, Stdev: 0.03291834073912505 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6433466672897339, Stdev: 0.03768641130349995 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6031466722488403, Stdev: 0.04760398293355269 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.5768800179163615, Stdev: 0.026096083954691405 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6241333285967509, Stdev: 0.028616715780150532 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.5777066747347513, Stdev: 0.06730051079476942 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.6247066656748453, Stdev: 0.027531016279300966 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.789306660493215, Stdev: 0.0009999077924873867 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8050933281580607, Stdev: 0.003555637124027248 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8129733403523763, Stdev: 0.0019786481994743927 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8175599972407023, Stdev: 0.001453631352737511 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.815933346748352, Stdev: 0.00278400888532679 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8191999991734823, Stdev: 0.007078154335000932 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8205866813659668, Stdev: 0.0034131237206102617 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8200133442878723, Stdev: 0.0038911016389664875 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.817466656366984, Stdev: 0.005978833595812484 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8143333395322164, Stdev: 0.005981425634582518 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8185999989509583, Stdev: 0.0010265729974298491 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8161333203315735, Stdev: 0.005555088169847459 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8128799994786581, Stdev: 0.004638740896092604 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8186400135358175, Stdev: 0.005337950112891901 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8185333410898844, Stdev: 0.0036227823272248906 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8150533239046732, Stdev: 0.005150557049088007 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.76064000527064, Stdev: 0.004212501209128441 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7761333187421163, Stdev: 0.0032262744104046164 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7852399945259094, Stdev: 0.0015189410094569847 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7873066862424215, Stdev: 0.0026384428958907895 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7937066555023193, Stdev: 0.002026315075568243 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7982666691144308, Stdev: 0.0008028617802369558 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7980533242225647, Stdev: 0.002413382566603225 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7960666616757711, Stdev: 0.0028397852941814024 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8002933462460836, Stdev: 0.0008108099631840385 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.7986266613006592, Stdev: 0.005047029661102869 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.7975333333015442, Stdev: 0.002403196902067193 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.7978533307711283, Stdev: 0.004823773616686171 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8007733225822449, Stdev: 0.002228860129548261 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8018933534622192, Stdev: 0.0008405301142224801 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8025599916776022, Stdev: 0.0003766528078091649 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.7993066708246866, Stdev: 0.002895951343982765 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTPZ2UAHAwHo",
        "outputId": "b83a0b68-2ea2-4c60-d1f5-a0a9b751741f"
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.622300978501638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlnkDgSxAwHo",
        "outputId": "90761abe-26b5-4718-e2a3-b56ca3a597d5"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 384}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4NPII8-AwHp",
        "outputId": "370e73fd-435e-4c52-d928-96983079cecb"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4975 - accuracy: 0.8526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsLt8-1YAwHp",
        "outputId": "e2000cdc-ea93-47bf-fa3b-c1e5deb05b16"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8525999784469604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJLCcLWTAwHq"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "DnTzto6kAwHq"
      },
      "source": [
        "Total run time: over 23 minutes.\n",
        "\n",
        "Best model score: 0.8417600194613138 \n",
        "\n",
        "using {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usnUM4SNAwHr"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABSmoOv2AwHr"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridserach experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZSxzzITAwHs"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}