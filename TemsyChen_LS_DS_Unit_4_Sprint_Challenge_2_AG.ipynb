{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "TemsyChen_LS_DS_Unit_4_Sprint_Challenge_2_AG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9m1wq1R_l0a"
      },
      "source": [
        "\n",
        "## Autograded Notebook (Canvas & CodeGrade)\n",
        "\n",
        "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
        "Instructions\n",
        "\n",
        "- **Download** this notebook as you would any other ipynb file \n",
        "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
        "- **Delete** `raise NotImplementedError()`\n",
        "\n",
        "- **Write** your code in the `# YOUR CODE HERE` space\n",
        "\n",
        "\n",
        "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
        "\n",
        "- **Save** your notebook when you are finished\n",
        "- **Download** as a ipynb file (if working in Colab)\n",
        "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_8hv860_l0j"
      },
      "source": [
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Simple Perceptron](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron (i.e. Neural Network)\n",
        "    - Analyze and Compare\n",
        "4. [Keras MMP](#Q3)\n",
        "\n",
        "\n",
        "____\n",
        "\n",
        "# Before you submit your notebook you must first\n",
        "\n",
        "1) Restart your notebook's Kernel\n",
        "\n",
        "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
        "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a2d017ba3200be3890c0b67eda283c48",
          "grade": false,
          "grade_id": "cell-621a8b86bacf295a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "DT_M2voD_l0m"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Defining Neural Networks \n",
        "\n",
        "Write *your own* definitions for the following terms:\n",
        "\n",
        "- **Neuron:** A neuron is a unit that accepts input/features/data, and does calculations on it to create an output.\n",
        "\n",
        "- **Input Layer:** A group of neurons that take in the initial data to be processed by the neural network.\n",
        "\n",
        "- **Hidden Layer:** A group of neurons that take the outputs from the input layer, or another hidden layer, and does more calculating and updating to the data, and passes it on to the next neuron layer.\n",
        "\n",
        "- **Output Layer:** The final group of neurons that take the processed inputs that have been through the neural network and outputs the prediction.\n",
        "\n",
        "- **Activation:** A modeling function in each neuron layer that calculates an updated weight to pass on to the next neuron. You choose what kind of activation function to use in each layer, such as a sigmoid function for binary classification, softmax for multiclass classification, relu for a continuous differentiable, etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "10aa095d3db59bfe47f9823cfd62f1ef",
          "grade": false,
          "grade_id": "cell-d64f1de9e9458dc7",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "I8y4xcs5_l0s"
      },
      "source": [
        "- `Explain` how Back-propagation works \n",
        "- `Explain` how Gradient Descent works (mention the learning rate)\n",
        "- `Explain` how Back-propagation and Gradient Descent are related   \n",
        "\n",
        "Use your own words, but feel free to reference external materials for this question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ceb3f64a4b1b18346decf75c8f5567d2",
          "grade": true,
          "grade_id": "cell-cef20b23d4e0b056",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "r-bK1Cw5_l0u"
      },
      "source": [
        "**Back-propagation:** \n",
        "An important part of neural network training, back-propagation improves the individual weights between neurons as the model trains. Data enters the neural network in a feed forward direction, through the input layer first, hidden layers, and output layers last. When training with gradient descent, new weights are calculated by the loss function and sent back to update the weight via back-propagation, and tested and improved on again in the next batch or epoch. \n",
        "\n",
        "**Gradient Descent:** the process of finding the minimal error rate for the neural network. The function that finds this (J-theta) compares the predicted value against the actual value. It adjusts the weights by taking this error and using it to push the predictions closer to the true values. The increment the weights are updated in each batch is the **learning rate**, which is usually between 0-1.\n",
        "\n",
        "**Back-propagation and Gradient Descent** work together like this: one batch of data goes through forward propagation in the neural network, where a gradient descent algorithm calculates the error between predicted and real values. The learning rate determines how much of that error to update the weights with, and through back-propagation, updates the weights for the next batch of data to be calculated on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e040f3ddce6eb34b017f0eb685b202e6",
          "grade": false,
          "grade_id": "cell-e013d19857352d79",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "Sd8OLelR_l0x"
      },
      "source": [
        "Remember our Simple Perceptron Class from Monday. \n",
        "\n",
        "- Describe the process of making a prediction, i.e. how do you go from inputs to outputs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d746de6391012340f8548821850a621c",
          "grade": true,
          "grade_id": "cell-53c7cc36db9d7983",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "XBOgND1j_l0z"
      },
      "source": [
        "**How a Neural Network makes a prediction:**\n",
        "The Neural Network has three types of neuron layers: input, hidden, output. The neurons in each layers have parameters with assigned values, such as input dimensions, and activation function. The model is also compiled, where you can assign other hyperparameters such as optimization function, and loss function. As the data enters a neuron, the neural network assigns a weight and a bias to the connection from one neuron to the next neuron in the next layer. The inputs are multiplied with the weights, added to the bias, and go through the activation function. The output is sent to the next neuron in the chain. The weights are initialized randomly, but updated and improved as the data trains in batches and epochs. The output layer makes the prediction, and will have benefited from the training process and achieved its highest accuracy and lowest loss error.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L9H51pd_l04"
      },
      "source": [
        "<a id=\"Q2\"></a>\n",
        "## 2. Simple Perceptron\n",
        "\n",
        "In this question, you will build two neural networks using `Keras`. After you build these two models, compare the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lhL96hk_l07"
      },
      "source": [
        "\"\"\"\n",
        "Our Dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\"Use this X & y in the following 2 models\"\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP3fuyf7_l08",
        "outputId": "c75a9135-3a7e-499a-8c4e-ef3ae91273cb"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZpdLtK_l08",
        "outputId": "c2055019-2a5a-4e63-cc07-b95176335205"
      },
      "source": [
        "y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27rKJdgX_l1C"
      },
      "source": [
        "### Simple Perceptron\n",
        "Construct a simple perceptron using Keras. \n",
        "\n",
        "Make sure to include the following in your model:\n",
        "- Add `1 dense layer` with a `single neuron` \n",
        "- Use a `sigmoid activation function`\n",
        "- Set `epochs` to 10 \n",
        "- Use the version of `crossentropy loss` that is appropriate for this data.\n",
        "\n",
        "Your model should be called `model1` and make sure to save the results of your fit statement to a variable called `h1`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67e9f7297eb22a79437494d713d74b71",
          "grade": false,
          "grade_id": "cell-427690628f9c900b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRSIfzE4_l1C",
        "outputId": "23335230-37f3-4802-d29b-235898f0f0cb"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# build and fit model\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model1.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy')\n",
        "\n",
        "h1 = model1.fit(X, y,\n",
        "                epochs=10,\n",
        "                batch_size=32,\n",
        "                validation_split=.2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 52ms/step - loss: 0.7113 - accuracy: 0.5387 - val_loss: 0.8050 - val_accuracy: 0.4167\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.5649 - val_loss: 0.8034 - val_accuracy: 0.4167\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7063 - accuracy: 0.5290 - val_loss: 0.8016 - val_accuracy: 0.4167\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.7153 - accuracy: 0.5452 - val_loss: 0.8000 - val_accuracy: 0.4333\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7193 - accuracy: 0.5331 - val_loss: 0.7983 - val_accuracy: 0.4333\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6960 - accuracy: 0.5565 - val_loss: 0.7967 - val_accuracy: 0.4333\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.7000 - accuracy: 0.5455 - val_loss: 0.7950 - val_accuracy: 0.4333\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6791 - accuracy: 0.5853 - val_loss: 0.7936 - val_accuracy: 0.4333\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6879 - accuracy: 0.5749 - val_loss: 0.7921 - val_accuracy: 0.4333\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6951 - accuracy: 0.5393 - val_loss: 0.7907 - val_accuracy: 0.4333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36f7f830036d0443ca8e8ba0f17b2a4e",
          "grade": true,
          "grade_id": "cell-bf2ae566afacde8c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZXhDR6QK_l1E"
      },
      "source": [
        "# Visible test\n",
        "assert len(model1.get_config()[\"layers\"]) == 2, \"Make sure you only create 1 Dense layer.\"\n",
        "assert len(h1.epoch) <=10, \"Did you make sure to set epochs to 10 or less?\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvTaT6nK_l1I",
        "outputId": "fde1886c-5fe2-42f0-e3a4-74899c6f69c5"
      },
      "source": [
        "model1.get_config()[\"layers\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'class_name': 'InputLayer',\n",
              "  'config': {'batch_input_shape': (None, 2),\n",
              "   'dtype': 'float32',\n",
              "   'name': 'dense_input',\n",
              "   'ragged': False,\n",
              "   'sparse': False}},\n",
              " {'class_name': 'Dense',\n",
              "  'config': {'activation': 'sigmoid',\n",
              "   'activity_regularizer': None,\n",
              "   'bias_constraint': None,\n",
              "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "   'bias_regularizer': None,\n",
              "   'dtype': 'float32',\n",
              "   'kernel_constraint': None,\n",
              "   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "    'config': {'seed': None}},\n",
              "   'kernel_regularizer': None,\n",
              "   'name': 'dense',\n",
              "   'trainable': True,\n",
              "   'units': 1,\n",
              "   'use_bias': True}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "95d3ee2935a0de64f2a5a22460520e69",
          "grade": true,
          "grade_id": "cell-a957e14380b2f508",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SP9vYoo6_l1J"
      },
      "source": [
        "# Hidden tests - you will see the results when you submit to Canvas"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Pt_E3d_l1J"
      },
      "source": [
        "### Multi-Layer Perceptron\n",
        "Now construct a multi-layer perceptron model (also known as a neural network). \n",
        "\n",
        "Your neural network `must` have: \n",
        "- `2` Hidden Layers\n",
        "- Select any number between `5-32` for the number of neurons in each hidden layers\n",
        "- Your pick of activation function and optimizer\n",
        "- Incorporate the `Callback function` below into your model\n",
        "- Set epochs to `100`\n",
        "- Your model should be called `model2` \n",
        "- Save the results of your fit statement to a variable called `h2`. \n",
        "- Use the version of `crossentropy loss` that is appropriate for this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFukdphR_l1K"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        # if model reaches 99% accuracy, training is terminated \n",
        "        acc_threshold = 0.99\n",
        "        if(logs.get('accuracy') > acc_threshold):   \n",
        "            self.model.stop_training = True\n",
        "            self.model.callback_used = True"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "314337f29c8cd7f38224a31687a86b12",
          "grade": false,
          "grade_id": "cell-77523c4c64743f16",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSVETcgP_l1L",
        "outputId": "631f8d83-8301-434d-8e74-3781c90f92f9"
      },
      "source": [
        "# build and fit model\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(units=32, input_dim=2, activation='relu'))\n",
        "model2.add(Dense(units=32, activation='relu'))\n",
        "model2.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy')\n",
        "\n",
        "h2 = model1.fit(X, y,\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                validation_split=.2,\n",
        "                callbacks=[myCallback()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7004 - accuracy: 0.5458 - val_loss: 0.7895 - val_accuracy: 0.4333\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6999 - accuracy: 0.5458 - val_loss: 0.7882 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.5500 - val_loss: 0.7869 - val_accuracy: 0.4333\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6989 - accuracy: 0.5542 - val_loss: 0.7856 - val_accuracy: 0.4333\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6984 - accuracy: 0.5542 - val_loss: 0.7842 - val_accuracy: 0.4333\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6979 - accuracy: 0.5542 - val_loss: 0.7827 - val_accuracy: 0.4333\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6973 - accuracy: 0.5583 - val_loss: 0.7815 - val_accuracy: 0.4333\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6969 - accuracy: 0.5667 - val_loss: 0.7803 - val_accuracy: 0.4333\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6964 - accuracy: 0.5667 - val_loss: 0.7790 - val_accuracy: 0.4333\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6960 - accuracy: 0.5667 - val_loss: 0.7777 - val_accuracy: 0.4333\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.5750 - val_loss: 0.7765 - val_accuracy: 0.4333\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6951 - accuracy: 0.5750 - val_loss: 0.7755 - val_accuracy: 0.4333\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6947 - accuracy: 0.5750 - val_loss: 0.7743 - val_accuracy: 0.4333\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6944 - accuracy: 0.5750 - val_loss: 0.7731 - val_accuracy: 0.4333\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.5750 - val_loss: 0.7721 - val_accuracy: 0.4333\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5750 - val_loss: 0.7711 - val_accuracy: 0.4333\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5750 - val_loss: 0.7699 - val_accuracy: 0.4333\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5750 - val_loss: 0.7688 - val_accuracy: 0.4333\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5750 - val_loss: 0.7677 - val_accuracy: 0.4333\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5750 - val_loss: 0.7668 - val_accuracy: 0.4333\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5750 - val_loss: 0.7660 - val_accuracy: 0.4333\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5792 - val_loss: 0.7648 - val_accuracy: 0.4333\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5792 - val_loss: 0.7637 - val_accuracy: 0.4333\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6908 - accuracy: 0.5792 - val_loss: 0.7628 - val_accuracy: 0.4333\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6905 - accuracy: 0.5792 - val_loss: 0.7620 - val_accuracy: 0.4333\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6902 - accuracy: 0.5792 - val_loss: 0.7612 - val_accuracy: 0.4333\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5792 - val_loss: 0.7603 - val_accuracy: 0.4333\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6897 - accuracy: 0.5792 - val_loss: 0.7595 - val_accuracy: 0.4333\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5792 - val_loss: 0.7586 - val_accuracy: 0.4333\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6892 - accuracy: 0.5792 - val_loss: 0.7578 - val_accuracy: 0.4333\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6890 - accuracy: 0.5792 - val_loss: 0.7569 - val_accuracy: 0.4333\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6887 - accuracy: 0.5833 - val_loss: 0.7563 - val_accuracy: 0.4333\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6885 - accuracy: 0.5833 - val_loss: 0.7555 - val_accuracy: 0.4333\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6882 - accuracy: 0.5875 - val_loss: 0.7548 - val_accuracy: 0.4500\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6880 - accuracy: 0.5917 - val_loss: 0.7542 - val_accuracy: 0.4500\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6878 - accuracy: 0.5917 - val_loss: 0.7534 - val_accuracy: 0.4500\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6876 - accuracy: 0.5958 - val_loss: 0.7527 - val_accuracy: 0.4500\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6873 - accuracy: 0.5958 - val_loss: 0.7519 - val_accuracy: 0.4500\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6872 - accuracy: 0.5958 - val_loss: 0.7511 - val_accuracy: 0.4500\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.6000 - val_loss: 0.7503 - val_accuracy: 0.4500\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6868 - accuracy: 0.6042 - val_loss: 0.7497 - val_accuracy: 0.4500\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6866 - accuracy: 0.6083 - val_loss: 0.7489 - val_accuracy: 0.4500\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6864 - accuracy: 0.6083 - val_loss: 0.7482 - val_accuracy: 0.4500\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6863 - accuracy: 0.6083 - val_loss: 0.7476 - val_accuracy: 0.4500\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6860 - accuracy: 0.6125 - val_loss: 0.7470 - val_accuracy: 0.4500\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6859 - accuracy: 0.6167 - val_loss: 0.7463 - val_accuracy: 0.4500\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6857 - accuracy: 0.6167 - val_loss: 0.7456 - val_accuracy: 0.4333\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.6167 - val_loss: 0.7450 - val_accuracy: 0.4500\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6854 - accuracy: 0.6167 - val_loss: 0.7443 - val_accuracy: 0.4500\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.6167 - val_loss: 0.7436 - val_accuracy: 0.4500\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6850 - accuracy: 0.6167 - val_loss: 0.7431 - val_accuracy: 0.4500\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6849 - accuracy: 0.6167 - val_loss: 0.7424 - val_accuracy: 0.4500\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6847 - accuracy: 0.6167 - val_loss: 0.7418 - val_accuracy: 0.4667\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.6167 - val_loss: 0.7413 - val_accuracy: 0.4667\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6845 - accuracy: 0.6167 - val_loss: 0.7407 - val_accuracy: 0.4667\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6843 - accuracy: 0.6125 - val_loss: 0.7403 - val_accuracy: 0.4667\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6842 - accuracy: 0.6125 - val_loss: 0.7399 - val_accuracy: 0.4667\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6841 - accuracy: 0.6125 - val_loss: 0.7393 - val_accuracy: 0.4667\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6840 - accuracy: 0.6125 - val_loss: 0.7387 - val_accuracy: 0.4833\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6839 - accuracy: 0.6125 - val_loss: 0.7380 - val_accuracy: 0.4833\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6838 - accuracy: 0.6167 - val_loss: 0.7375 - val_accuracy: 0.4833\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6837 - accuracy: 0.6167 - val_loss: 0.7370 - val_accuracy: 0.4833\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6836 - accuracy: 0.6167 - val_loss: 0.7366 - val_accuracy: 0.4833\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.6125 - val_loss: 0.7362 - val_accuracy: 0.4833\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6834 - accuracy: 0.6208 - val_loss: 0.7357 - val_accuracy: 0.4833\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6832 - accuracy: 0.6208 - val_loss: 0.7352 - val_accuracy: 0.5000\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6831 - accuracy: 0.6167 - val_loss: 0.7349 - val_accuracy: 0.5000\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6831 - accuracy: 0.6167 - val_loss: 0.7344 - val_accuracy: 0.5000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6830 - accuracy: 0.6167 - val_loss: 0.7340 - val_accuracy: 0.5000\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6829 - accuracy: 0.6167 - val_loss: 0.7337 - val_accuracy: 0.5167\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6829 - accuracy: 0.6167 - val_loss: 0.7332 - val_accuracy: 0.5167\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6828 - accuracy: 0.6167 - val_loss: 0.7327 - val_accuracy: 0.5167\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6827 - accuracy: 0.6167 - val_loss: 0.7324 - val_accuracy: 0.5167\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6826 - accuracy: 0.6208 - val_loss: 0.7321 - val_accuracy: 0.5167\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6826 - accuracy: 0.6208 - val_loss: 0.7318 - val_accuracy: 0.5167\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.6208 - val_loss: 0.7316 - val_accuracy: 0.5167\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6825 - accuracy: 0.6208 - val_loss: 0.7312 - val_accuracy: 0.5167\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6824 - accuracy: 0.6208 - val_loss: 0.7308 - val_accuracy: 0.5167\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6824 - accuracy: 0.6250 - val_loss: 0.7305 - val_accuracy: 0.5167\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6823 - accuracy: 0.6250 - val_loss: 0.7301 - val_accuracy: 0.5167\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6822 - accuracy: 0.6250 - val_loss: 0.7299 - val_accuracy: 0.5167\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6822 - accuracy: 0.6250 - val_loss: 0.7297 - val_accuracy: 0.5167\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6822 - accuracy: 0.6250 - val_loss: 0.7294 - val_accuracy: 0.5167\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6821 - accuracy: 0.6333 - val_loss: 0.7292 - val_accuracy: 0.5167\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6821 - accuracy: 0.6333 - val_loss: 0.7289 - val_accuracy: 0.5167\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6821 - accuracy: 0.6333 - val_loss: 0.7286 - val_accuracy: 0.5167\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6820 - accuracy: 0.6375 - val_loss: 0.7285 - val_accuracy: 0.5167\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6820 - accuracy: 0.6375 - val_loss: 0.7282 - val_accuracy: 0.5167\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6820 - accuracy: 0.6375 - val_loss: 0.7278 - val_accuracy: 0.5167\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6819 - accuracy: 0.6375 - val_loss: 0.7276 - val_accuracy: 0.5167\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6819 - accuracy: 0.6375 - val_loss: 0.7275 - val_accuracy: 0.5167\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6819 - accuracy: 0.6375 - val_loss: 0.7272 - val_accuracy: 0.5167\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6819 - accuracy: 0.6375 - val_loss: 0.7269 - val_accuracy: 0.5167\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7269 - val_accuracy: 0.5167\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7267 - val_accuracy: 0.5333\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7265 - val_accuracy: 0.5333\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6818 - accuracy: 0.6375 - val_loss: 0.7263 - val_accuracy: 0.5333\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6817 - accuracy: 0.6375 - val_loss: 0.7261 - val_accuracy: 0.5333\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6817 - accuracy: 0.6375 - val_loss: 0.7258 - val_accuracy: 0.5333\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.6375 - val_loss: 0.7255 - val_accuracy: 0.5333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a5f575f46f151f97f1cebc19a484bae",
          "grade": true,
          "grade_id": "cell-770612ca24334d8a",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Y0San3O1_l1L"
      },
      "source": [
        "# Visible test\n",
        "assert len(model2.get_config()[\"layers\"]) == 4, \"You should have 4 layers: Input, hidden 1, hidden 2, output.\"\n",
        "assert 5 <= model2.get_config()[\"layers\"][1][\"config\"][\"units\"] <= 32, \"You should have 5 - 32 units in hidden layer 1, but don't.\"\n",
        "assert 5 <= model2.get_config()[\"layers\"][2][\"config\"][\"units\"] <= 32, \"You should have 5 - 32 units in hidden layer 2, but don't.\"\n",
        "assert h2.params[\"epochs\"] == 100, \"You didn't set epochs to 100.\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ca73d4d3d17897a570e19a8a97c050f",
          "grade": true,
          "grade_id": "cell-49b1bf7cce22b5b9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "G-Q4ydUr_l1M"
      },
      "source": [
        "# Hidden tests - you will see the results when you submit to Canvas"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qU-vauK_l1N"
      },
      "source": [
        "### Analyze and Compare\n",
        "\n",
        "**Before you Start**: You will need to install an additional library for this next segment. \n",
        "\n",
        "Install the package `mlxtend` into the environment you are using for the sprint challenge.\n",
        "\n",
        "You can install this package using the following statement in the terminal\n",
        "\n",
        "```python\n",
        "pip install mlxtend\n",
        "```\n",
        "\n",
        "Or you can install this package using the following statement in your notebook\n",
        "\n",
        "```python\n",
        "!pip install mlxtend\n",
        "```\n",
        "\n",
        "If you choose to install this package from within your notebook, be sure to delete the install statement afterwards so that CodeGrade doesn't try to install it and potentially crash. \n",
        "\n",
        "\n",
        "The cells below generate decision boundary plots of your models (`model1` & `model2`). Review the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQf0kFExqKfa"
      },
      "source": [
        "# !pip install mlxtend"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YXGzb-h_l1O",
        "outputId": "60105e45-7df2-45f1-b34e-87b74dc8e550"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300, 2), (300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ko0gz9bG_l1P",
        "outputId": "2a0070e5-64ac-4ebf-9bcf-6daf8b3043f7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
        "\n",
        "    ax = plt.subplot(1,2, grd)\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAF1CAYAAAAJAjeKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5xU5dXHv8/MbN9lYVmK9CCYKPYaY1QisWBQYxcLolGJkSSWVxJrzGsSjb62CBJ7Q4rYotiNUqKJoBJBRERBOizD1tk+M8/7x72z3J2dPnfa7vl+PvPZnVue57l35v7umXPPc47SWiMIgiAIgiAIPQlHpgcgCIIgCIIgCOlGjGBBEARBEAShxyFGsCAIgiAIgtDjECNYEARBEARB6HGIESwIgiAIgiD0OMQIFgRBEARBEHocYgQLWY9S6gKl1DsR1o9VSm1O55gEQchelFJaKTUqwvpVSqmxaRySkCGUUsOUUh6llDPCNhG/L0L3RYzgNKKU+k4p1WxekDuUUk8ppUozPa4ASqnblFKzMj2OYLTWz2mtTwi8T1awlFIFSqknlFL1SqntSqlrI2w7WSnlMz+zwGusZb31M/VEMtYt+9xmHsMRiR6DIHRHzOupTSlVGbR8uXnNjEigzaeUUn+yLtNaj9FaLwyz/QizL1e8faUS8zjaTJ2pVkq9q5T6QabHFSBbnRFa641a61KttQ9AKbVQKXVZMm0qpa4x7x315r2kIMK2xUqph5RSbqVUnVJqcYht8pVSq2M5f0qp7yml/Eqpmckcg2AgRnD6OUVrXQocDBwK3BzPzsogI59bJvu2mduA0cBw4CfANKXUSRG2/7cpooHXwqD1p1jWnRCqgQBKKQVMAqrNv2kj227qghCG9cDEwBul1H5AceaGk34iXKt3mfePIUAV8JSNbaec7qBBSqkTgd8D4zDuISOBP0bY5RGgAtjb/HtNiG2uB3bGOIRJQA1wbiTjOxVE8qbnLFpreaXpBXwH/NTy/m5ggfn/D4GPgFrgc2CsZbuFwJ+BD4FmYBQwBngXw5jaAdxobuvAuEC/BXYBzwMV5roRgAauALYC24D/MdedBLQB7YAH+DxC3z8ClgF15t8fBY31dnP7BuAdoDLM+VgEnGn+f5Q5tp+Z78cB/zX/nwz8y/x/sbldoznOc4GxwGbgOowbwzbgkgifw1bgBMv724G5Ybbt6DuWzzSG78Ax5nm8wPx88i3rioB7gA3muf0XUGSu+7Hl+7EJmGw535eFG695rq4C1gLrzWUPmG3UA58CR1u2dwI3mt+fBnP9UGAGcE/QsbwKXJPp60pe3edlXk83A8ssy/4PuMn8Lo8wl8XyvR+FoXXtGNrmAV6z9BPyumW3TrpCrDsc+Ld5HW4Dpgeu4WjXCDAIeBHD2FkP/May3W3AC8As87q8LETfTwF/srz/GeBJpG0MY+xJDC2sAV6xbD8B+K95jB8B+wd9PjcAX5r7PQkUAiUYuuY3z7PHHFOovgeZ56Ua+Aa4PGiszwPPYOjPKuDQMJ/TH4EHzf/zMO4Jd5vvi4AW8zg7Pk+Me5nPXOcBplu+L7/E0Mla87NUYfqdDfzF8n4csD3Mtj8wj7tXhO/894DVwHhgc5TrQ2Fo85UY9/2zgtafZn529eZ2J5nLQ37ehLi/medilOU7NxN4wzy/P8X43i03+9gE3Ba0f5d7FXCYOV6nZbszMO2MjGpOpgfQk15YhBfDsFiFYYANxjCITsYwYo833/czt10IbMQwfF1AGYYAX4chQGXAEea2vwX+g+EpKAAeBuaY6wJiMAdDtPbDEM3AmG4DZgWNObjvAeZFdJH5fqL5vq9l+2+BvTCEaCFwZ5jz8b/sFrGA4fVXy7oHzP87XajWi9R8PxbwmvvkmeexCegTos8+5v4DLMvOAlaGGeNk8+J3A18Dt2C5OZqf6Q7zPL4DHBDlO/A4hsjnmZ/xmZZ1M8zzNRjDGP2R+RkOx7ghTDT36wscaDnf0YyBdzFEMGBQX2i24cL4Dm0HCs111wMrge9jCO4B5raHYwiow9yu0jzHAyIdr7zkFc/LvJ5+CqzB8Jw5MX7gDicBI9j8/yksxqO1nzBjGEF4I/gQDIeFy9xuNXC1uS7sNYKh658CtwL5GN7DdcCJ5ra3YRjrPze3LQrRd8dxAKUYxtiSRNoGXgfmYehhHnCsue1BGI6EI8xzf7F5rgos5+0LjPtXBYazIzCmsQQZcWH6Xgw8hHHvOhBDO4+zbN+CoeFO4A7gP2E+p+MwdRtDK78FPras+zzU50nQd8fyfVkA9AaGmWM6KUy/nwPnWt5Xmvv3DbHtJAw9vQ/jHrISi+ab2ywATg91/kK0dzTQan5uD2L+qLN8/+ow7AcHxn3kB+a6cJ/3ZKIbwXUYTiqH+ZmNxbAdHMD+GPe/n5vbR7pXfQmMt/TzMnBdxjUn0wPoSS8MAfFg/ELaYApBEfA74Nmgbd8GLjb/Xwj8r2XdRGB5mD5WA+Ms7/fAEKGAaOvAhWGuvwt43Pz/NkIbwda+LwKWBm3zbzp7Jm+2rPsV8FaYsY4DVpj/v4XhJfiP+X4RcIb5f6cLldBGcDOdjdMq4Ich+hxq7l9oWXY88F2YMY7E+KXuMC/8L4EbLOuPMj/DYgwPyXagd5i2ijF+PQcE42HgH+b/DvMYuhjRZrsvh2lzIdGNgeOifC9rAv1iGB+nRfhuHW/+PxV4I9PXlLy614vdRvDNGAbQSRg/4lxkgREcYturrddmuGsEw6jcGLTvDcCT5v+3AYuj9PUUhoFYa+rMq8Ce8baNcU/wE9pJMBO4PWjZGnYbTd8Bv7SsOxn41vx/LKGNYGvfQzE8sWWWZXcAT1m2f8+ybh+gOcz5CHh7+2I8/bwR4wdTKYaX+G+hPs/g747l+/Jjy/vngd+H6bfDw2q+z8Py3Qza9kZz3W0YP1COxbAB9jbXnw68Ge78hWjvMXZ7cY/EuLf3N98/DNwXYp9In/dkohvBz0QZ0/2Bfol8r/od8Jz5fwXGD8Q9ol1jqX51h/jOXOPnWuveWuvhWutfaa2bMX49na2Uqg28MB4p7GHZb5Pl/6EYF2IohgMvW9pZjSE6A8K0tQHj8VQkrNsPMvexsgHjV2eA7Zb/mzBEKRT/BvZSSg3A8Ag8Aww1J8UcjuExiJVdWmtvDP16zL+9LMt6Yfx67YLWep3Wer3W2q+1XonhbT7Lsv5DrXWz1rpJa30Hxg3q6DBjPB3DY/2G+f45YLxSqh+GN6GQ0J9rpM87FqyfH0qp/zEnYdSZ35Fys/9ofT2N4UXG/PtsEmMShEg8C5yPcZN+JpUdBU16HRZl272UUgsCk6KAv7D72oHw18hwYFCQxt9IeF0Ox/+Z94+BWutTtdbfJtD2UKBaa10Tov3hwHVBbQ2l8z0i2ftHtdbaqrfR7h+FoWKJzXvnJxiG5TEYjpOPMBwTx5rv4yHW+5aHrvcPCH0PacYwVP+ktW7TWi8CPgBOUEqVYDihfhPL4JRSRcDZGPcNtNb/xnhKe765STjtjvR5x0Lw/eMIpdQHSqmdSqk6jDCSWO4fs4BTzOM+B1iitd6W4JhsQ4zg7GAThie4t+VVorW+07KNDtp+ZIS2xge1Vai13mLZZqjl/2EYj/CC+7BiXb4VQyitDAO2ECda6yaMx3i/Bb7QWrdhiNi1GN4Fd7xtxtBnDUYoyQGWxQdghKbE1ARGmEAi6y/GENaNSqntwHwML8L5GI/KWjA8O8FsCrMcjFAN66ShgWHGBIBS6mhgGoYI9dFa98Z43BUYc6S+ZgGnKaUOwHhU/UqY7QQhKbTWGzBiW08GXgqxSSzf+47movRlnfS6McrQZgJfAaO11r0wjE3r9R7uGtmEEZNv1eUyrfXJsY4zAvG2vQmoUEr1DtPWn4PaKtZaz7Fsk+z9o0IpVRbURtz3D5NFGKEPB2HMT1kEnEhkJ0qi5znAKrreP3ZorXeF2HZFhP5HY3ipl5j3g5eAPcwfWCNC7Hc6hsH9kLnNdowfDxeb68Npd6TPu9N1pJSKeP8wmY3xFGKo1roc+Dsx3D9MG+TfGLHAF5ElThQxgrODwC+kE5VSTqVUoZluZkiY7RdgXCxXKyPdV5kl3dbfgT8rpYYDKKX6KaVOC9r/FjNtyxjgEoxYITBie0ZEyQDxBob39nyllEspdS7GI6sF8R82YIjWVHb/al8Y9D4UOwj/IyAWngFuVkr1MVMMXU6YWdZKqfGmpxpz21uAf5jvhymljjLT2xQqpa7H+EX8YYh2BmOEf0zA8HofiCGefwUmaa39wBPAvUqpQeb34Ehz9u9zwE+VUueY57yvUupAs+n/AmeYn+co4BdRjr0Mwxu9E3AppW6ls1fjMeB2pdRoMxvI/kqpvgBa680YN5pngRdNT4wgpIpfYITyNIZYF8/3PlG9KDCv68DLgXH91AMeUw+utO4Q4RpZCjQopX6nlCoyr+99lVKHJTCuYOJq2/S+vYlhTPVRSuUppY4xVz8K/NL09imlVIlS6mdBRutVSqkhSqkKjAmL1vtHX6VUebiBaq03YTg67jDP6f4Yn12iqTkXYcTdfmk6URZihNWt11qHy7Zgx/3jF0qpfUzD8mbCZ+lYjOGtvcHU7qMwMhK9ze7Y6sD94DJzbAcS+qnAxRj3iP0s+xwFHKCMDCqPA5copcYppRxKqcFKqR9E+bw/B8YopQ5UShVihG1EowzDs9yilDqc3Z5oiHyvCpy7aeYxhPpxm3bECM4CTGE4DcOrsBPjArieMJ+P+SjpeOAUjEc4azEuLDBm/r8KvKOUasCYJBecj3YRxqzcf2I8Xgvktp1v/t2llPosTN+7MAy56zAmdk0DJiThtV2EcVEtDvM+FLcBT5uP685JoM8/YDyy2WD2d7fW+i3olFg98Fh0HLBCKdWI8QPgJYxHoJjjnIkRU7sFI35xfBiPwEUY2S7e0VpvD7yAvwH7K6X2Bf4HY+LEMoyZ03/FmGSzEcMjdp25/L/s9kTchzHzfQfGo9jnohz72xjx11+bx99CZ8G9FyMe7h2Mm/3jGLF3AZ7GELCs+BUvdF+01t9qrT8Jszqe7/3jwD6mXsTz9MKD8Tg78DoO4xo9H+PR96PsNgCtdLlGtJGjNvADeD3Gk5/HMEKRkiLBti/CeEz/Fcb8iavNtj7BcApMx9C1bzBCUqzMxtCHdRg6+idz368wJl2vM891uDCJiRge0K0Yk6P+oLV+L9bjDeIjdk+2A2PORguR7x8PAGcppWqUUn+Lt0PzXnEXRljDRgwd/UNgvTIKsVxgbtuOcW8/GeOJ26MYTo+vtNbeoHtBNeA33/usfVqcKPdb99Faf4qh5xdrrZdiOLXuM/taxO6ntuE+768xQvzew7Aj/hXDKfgV8L+mfXErxv0icG4i3avA+LyHY8QNN8XQV8pRWif7ZEDIFcxHLOuBvKD4WUGICdODMAsYrkU8BKEL3fkaUUp9hzGpLFGjVejhKKW+BaZky3dIPMGCIMSEUioPI377se52cxcEO5BrRBDCo5Q6EyPG+P1MjyWAGMGCIERFKbU3RuaLPTBS4giCYEGuEUEIj1JqIUb44FXmHJisQMIhBEEQBEEQhB6HeIIFQRAEQRCEHocYwYIgCIIgCEKPo0sVlrTw0YMSgyEIOUhVTQPXzV/LMZfekumhZIzLjxkZqVhKt+TFTzfr6sa2TA9DEAQhbvYdXM6Re/YNqdviCRYEIWZunb2Uw869OtPDEARBEISkESNYEISYmPPBF5QfcgpFJWXRNxYEQRCELEeMYEEQolJV08Aba9sYdehPom8sCIIgCDmAGMGCIERFwiAEQRCE7kZmJsaFwI+i0VmBz1UIZOO8E43T20KJrxoHMq9P6DlIGIQQCoWmPM9PoROUykbNBq01LT6oa3egs/K+IghCJskaI7jRWUFeaW9KlY9s1FOtoVUX0uiBMt+uTA9HENJCIAzimEslDELoTHmen94lhfiVi6wUbQCtKdReaGyhtt2Z6dEIgpBlZE04hM9VSEGWGsBgaHyB8pmeakHoGdwy+2MJgxBCUugkuw1gAKXwKxeFYv8KghCCrDGCQWW1lkJA67N8kIJgE3M++ILeh5wqYRBCSJRS2W0AB1Aqa8M1BEHILFlkBGcHby35lO+ffCWjTryCOx99IdPDEYSMINkghFzhk3+9zy9O+TGXnHwk8x57MNPDEQQhhxAj2ILP5+OqPz3Mmw//gS9fm8GcNxbz5TcbMz0sQUg7kg1CyAV8Ph8z/nwjf3roOR75xyIWvvkKG75dk+lhCYKQI2TNxLh4OPzCm3DXNXdZXllexNJZf0643aUr1zJq2B6MHDoQgPPGH80/3v+YfUYNS7hNQcg1JBuEYDe/nXQ6dfX1XZaX9+rFA8+8nHC7a1YuZ49hI9hj6HAAjh1/Gv/+4G2G7/n9hNsUBKHnkJNGsLuumTFT7uuyfNXD1yTV7pYduxg6sLLj/ZCBlXy8QrwKQs9BskEIqaCuvp7RV0zvsnztI1OTandX1Xb6DRzc8b5ywB6sWbE8qTYFQeg5SDiEIAgdSBiEIAiC0FNI2ghWShUqpZYqpT5XSq1SSv3RjoFlgsED+rJpu7vj/ebtbgb375vBEQlC+pAwiJ5Dd9Htvv0HsnP7lo737h3b6DtgYAZHJAhCLmGHJ7gVOE5rfQBwIHCSUuqHNrSbdg7bdzRrN2xl/ebttLW1M/fNJZz6kyMyPSxBSDmSDaLH0S10+/v7HsjWDevZvnkj7e1tLHrzH/xw7ImZHpYgCDlC0jHBWmsNeMy3eeYrJ+sKu1xOpt80hRMvvw2f38+lp/+UMaNlUpzQ/bl1zlIOuyAnnYFCAnQX3Xa6XPzqxr9w0y8n4vf5OOH08xgxSibFCYIQG7ZMjFNKOYFPgVHADK31x3a0G47K8qKQk+Aqy4uSbvvkYw/l5GMPTbodQcgV5nzwBeUHSxhETyOdul3eq1fISXDlvXol3fbhx4zj8GPGJd2OIAg9D1uMYK21DzhQKdUbeFkpta/W+gvrNkqpK4ArAB6edi5XnHZUwv0lkwZNEITdVNU08PraVo6VbBA9jmi6bdXsKTfeySEnnp1wX8mkQRMEQUgVtqZI01rXKqU+AE4Cvgha9wjwCAAfPZhzj90EoTty6+ylHH6hhEH0ZMLptlWzX/x0s65ubMvQCAVBEFKDHdkh+pmeBJRSRcDxwFfJtisIQmqRbBA9F9FtQRAEezzBewBPm/FlDuB5rfUCG9oVBCFFSFGMHo/otiAIPR47skOsAA6yYSyCIKSJW2cv5TAJg+ixiG4LgiBIxThB6HFIGIQgCIIgiBHciUtveoD+P76IfU9Nrp69IGQrO81sEFIUQ+gO3HvLNZx77L5MOX1spociCEIOIkawhcmnj+OtR27L9DAEIWXcMnsph53TNce2IOQix592Dn+aOTvTwxAEIUfJaSPYXVPPmVP/l1219ba0d8yh+1JRXmpLW4KQbcxduIryQ06huFTCIITMUFeziz//5kLqa6ttaW+/Q4+krLyPLW0JgtDzyGkj+JmX3qZmyzc8/eLbmR6KIGQ1VTUNvP61hEEImeX9V57Dv/Vz/vnyrEwPRRAEIXeNYHdNPQve/YCZZwxgwbsf2OYNFoTuyK1zlnLoOVdnehhCD6auZhfL332B+88YwvJ3X7DNGywIgpAoOWsEP/PS20zYU/H9AYVM2FOJN1gQwjDngy8oP1jCIITM8v4rz3HKKBg9oIhTRiHeYEEQMk5OGsEBL/CkQ3oBMOmQXuINFoQQ7DSLYkgYhJBJAl7g8w8pB+D8Q8rFGywIQsbJSSM44AWuLDVqfVSWumzxBk/8n7s5cuI01ny3hSE/uYTHX3zHjuEKQsa4ZfZSDjtXwiCEzBLwAvctzQOMv3Z4g++YdiXXXDiBzd99y4XjDuatlyRThCAIsWNH2eS0s3Dp52zd1srslds6LR/k/pxrf3F2wu3O+b/rkx2aIGQNcxeuotfBE6QohpBxVi5dwpJtLcxZsbnT8t47l3D6Jb9JuN0b7pqZ7NAEQejB5KQR/OrDf8r0EAQhq9lZ08CCr1s49tLjMj0UQeDWmfMzPQRBEIQu5GQ4hCAIkblljhTFEARBEIRIiBEsCN2MuQtX0eugCZINQhAEQRAikEVGsEbrTI8hMsb4snyQQo+mygyDGH2YhEEIqUVrTdaLNoDWxlgFQRCCyBoj2OltoVU7s1ZTtYZW7cTpbcn0UAQhLLfOljAIIT20+MChvdltCGuNQ3tp8WV6IIIgZCNZMzGuxFdNowdaXIWAyvRwQqBxehso8UleSyE7mbtwFeWHSFEMIT3UtTugsYVCJyiVjZpteKtbfOZYBUEQgsgaI9iBpsy3C+QXuyDEze5sEFIUQ0gPGkVtuxPaMz0SQRCExJCfx0LGcNd6OPP3f2dXXWOmh5LzSDYIQRBSTUNtNY/e9As8dTWZHoog2IIYwULGeOb1j6jZvomnF3yY6aHkNJINQhCEdLDszXm4dqxk6RtzMz0UQbAFMYKFjOCu9bBg0TJmnlHJgkXLxBucIDslG4QgCGmgobaaNYtf5p7TB7Nm8cviDRa6BWIECxnhmdc/YsIoB9/vX8CEUQ7xBieIhEEIgpAOlr05j1NGw6j+RZwyGvEGC90CMYKFtBPwAk86uASASQeXiDc4ASQMQhCEdBDwAk88uByAiQeXizdY6BaIESyknYAXuLLUSE5SWeoSb3Cc7Kxp4PWvWyUMQhCElBPwAvctyQOMv+INFroDWZMiTeg5LPzsa7ZWtTJ7ZVWn5YN2fM21F5yQoVHlFrfMWcqh5/8x08MQBKEHsHb5hyyvamHeis2dlpdu/5DjJl6ZoVEJQvKIESyknVfvmZrU/u5aD1PunMUjN1xE3/ISm0aVO0gYhCAI6WTKXbOS2r+htpq5d1/PxGn/R2l5H5tGJQjJI0awkHNYU6vliuf48Ctn4G5o7bK8sqyApTOvirmd3UUxJAxCEITcwJpaLVc8x3dMnYjH09BleWlpGTdMn5OBEQmpQIxgIaewpla7csEyLp5wVE54g90NrYy5/J4uy1c9el1c7dwyZymHSRiEIAg5QmBS3YzTB3PVgpc5/OTzcsIb7PE0MPKyB7ssX/fYrzMwGiFVyMQ4IafoyanVJAxCEIRcQ1KrCdmMGMFCztCTU6tJUQxBEHINSa0mZDtiBAs5Q09OrSZFMQRByDUktZqQ7UhMsJAz9NTUahIGIQhCLiKp1YRsR4xgIWdINrVaJqksKwg5Ca6yrCDifpINQhCEXCXZ1GqZpLS0LOQkuFJxRnQrxAgWhDQQTxo0K5INQhAEIf1IGrSegcQEZxB3rYczf//3HjGxS4ifeYskDEIQso2G2moevekXMrlLELoB4gnOILlY9KE7YVcBi1Sws6aB19ZIGIQgZBu5WPihOyFFLAQ7SdoIVkoNBZ4BBgAaeERr/UCy7XZ3crXoQ3fCrgIWqUDCIIRUIrqdGLla+KE7IUUsBDuxIxzCC1yntd4H+CFwlVJqHxva7db05KIPQmQkG4SQBkS3E0AKPwhC9yJpI1hrvU1r/Zn5fwOwGhicbLvdmZ5c9EGIzM6aBl7/ulWKYggpRXQ7fqTwgyB0P2ydGKeUGgEcBHwcYt0VSqlPlFKfPPKPnu31TLboQ6om1KV7op5MDOzKLXOWcug5V2d6GEIPIpxuWzX73Zeey8TQsopkCj+kajJduifpyaRAobthmxGslCoFXgSu1lrXB6/XWj+itT5Ua33oFacdZVe3OcnCz75m9spWDp1R1fGavbKVhZ99HdP+1gl1dpJIu8kYsqk6jlxFwiCEdBNJt62affwZF2RmgFnE2uUfMm9FC0fP2NzxmreihbXLo+uXdTKdnSTSbjKGbKqOQxAyhS3ZIZRSeRhC+pzW+iU72uzOJFP0IVUT6hJtN9EMF9kwMTDRAhapIJGiGDJLWkgG0e34SLTwQ6om0yXabqLZLbJlUmAuF7EQzc4+7MgOoYDHgdVa63uTH5IQic4T6lpsS6+WSLvJGLKpOo54yHQaNCuJZIOQWdJCoohup4/Ok+kabUutlki7yRiyqTqOeMllY1E0O/uwIxziKOAi4Dil1H/N18k2tCsEkaoJdYm2m2iGi+D+Jh5QzMMvvMvaTVVJHUeuIkUxhAwgup0GUjWZLtF2E81uEaq/1QtfYOa0SRIfLOQ0dmSH+JfWWmmt99daH2i+3rBjcEJnkp1QZ2e7yRjkwf0pbzMT9oRpD85P6jhykUBRDMkGIaQT0e30kMxkOrvbTcYgD9XfCYMb8az/TOKDhZxGKsblEAs/+5qtVa3MXtnZYzpox9dJhRIk0m4kwznaWKz9+f2anTX1VBQ5qG5Zz666xh5VNESKYghC92Xt8g9ZXtXCvBWbOy0v3f5hUqEEibQbyXCONpbg/vx+P421tYzqV8CaxVI0RMhdxAjOIZKZUGd3u8kY5Nb+7n3uHdjyKdceU869i+sSjg3O5hLI4ZBsEILQvUl0Ml0q2k3GIA/u7/05M9lr28tMPbqS6UvcCcUHyyQxIRsQI1hICDsM8kBIxfPnGEbgpINLOOf5xDJFZHMJ5FAkkg0imFyeJS0IQnqxyyAPhFX84dzdYRXnz4vfG9wTJ4mJZmcfYgQLGSOZkIpkyAavsR1hEOItEQQh3SQTVpEo3cVrnEtj7SmIESxkDLtinN21HmrdO2hraiC/OPov6kx7je3MBtFdbg6CIOQGdsQ5N9RW0+reRHtTHXnF5VG3705eY9Hs7EKMYCFj2BXj/MzrHzG8tJ0dn77D0KPPtKXNVLGzpoEFa1o4JokwCCvd6eYgCEL2Y0dYxbI35/G90lbqPnuDyh9PtGFUuYNodnZhW9lkIXdIptRxtvUTiCv+40+KafpqMW1NXX9hZxO3zl3Koedck+lhCIKQYyRT7jjb+liz+GVu+0kJ+qt3aW+qS1lfghAN8QT3QBItdZyN/QTiikdX5jF+4C6e//s1FJXtfryWiRLI4Zi3aBVlB0o2CEEQ4ifRcsfZ2Mcpo2HPynxOHriL5x7+Ja6yyo71MklMSCdiBPcwkil1nG39WLNLVJaWc0tfLyvrG5h/95SsyzVsdxiEIPrN0IMAACAASURBVAg9h2TKHWdjH384t5y+JZVc1bedD+vquOjOZyXPsJARxAjuYXQuddySMi9tOvpJNLtEZVlByElwqfQa3zJnKYdKUQxBEBKgc7njxpR4atPZR7yZJSS1mJAqxAjuQdiZlzcb+kk0u0S6i2fYmQ0iGLk5CEL3xq68vJnuAxLPLNGdsiaIZmcXYgT3INKVl3fmiws5pI+H3kXlKe0nVRX07CTVYRDd6eYgCEJX0pGXd8nLT3JsxU76FPZJWR+Qugp6uYRodnYhRnAPwq68vNF48YPP2LWrmVfWbKLN66NveQkOh0q4H3ethyl3zuKRGy7KuljfaNw6dymHTpQwCEEQEsOOvLzRWLFwAcuqG3lpzRq83nZKevXB4XAk1UdDbTVz776eidP+T+J9haxFjOAeRDo8p+5aDxXFTuadM5yfP7uTIeVOTjnhqKSM7HRls7AbyQYhCEKypNp72lBbTXlxHjPOGcP5z25mWLmLET+9IGkDOx2ZJgQhWcQIFmwlEHLRt9hJgW7l9nF9uHVR4vHA6cpmYTe5lA1CKhgJQs8lEG7Rp8RFGR5uH9efaYuTiwdOR6aJnoxotn2IESzYhnVC3DOf1HHBfnn0y29l/MiihL24yWSZOPzKGbgbWrssrywrSPnkuHSGQSQriFLBSBB6JtYJcfOXVXH+fvkMym9iwsi8pDy4yWSa6AkGnmh29iBGsGAbAYMVYMGqep4/qxiv1py8p+bX78bvxU02y4S7oZUxl9/TZXmo9Gh2ku4wCBFEQRASIWCsArz35S7mnlWCX/s5dZSfK95JzIObbKaJnqBnPeEYcwUxggXbCEy8m/5RLaeNgqomHwD5rnYmjCqI2xucrmwWdhIcBtETvBqCIOQmgUl3j3+4k7NGw65GLwDFec2cMrosIW9wOrJZpBLR7J6FGMGCbQQm3p163XSW7HCz5A3r2ta4s0OkK5uFnQQXxZBf/IIgZCuBSXcPT7uQt7Zv5K1Omt2SUHaIdGSzSCWi2T0LMYIF27ErC0Uu5AG2Yi2KEfAm1Lir2PLd2o5tnE4nA4eOzOAoBUEQOmNnBopczgV8x9SJXTQbDN0WuidiBAuCDQSHQQS8CSumX0lB5bCO7VrdGzM1xLDEWsFIHhMKgtCd8XgayCut6KTZkH26LZptH2IEC92WyrKCkJPgKssKbM8ckcmiGMmW4YxVDOUxoSAIqSacnjVU7+SmyRNCbp9rBp1odvYgRrDQgd2V2TJd6S2SMTvywnuTyhxhNaI9jU20qXzyP5wYlyDb9Ss9124AgiDYQyqqsmW60ls4Pbtp8oSkDDo79FY0u/shRrDQgd2V2XK10lssBNKvtXrqWPPR24w8/nKgqyA7C4vZ+tTVHe/bPdW0VvantLRMfqULgpAUqajK1l0rvcWqt8GaDYZuDx2xp2h2N0SMYAGwvzJbrlZ6i5e1S16j39iLw64fc1lnb/O6x37Nn59aABDy0Z4gCEIspKIqm1R666rZYOj2DdPniGZ3Q8QIFoDkKrPZ1V6mwyfiZeuq/1C452G4Cku7rEs25ksQBCESyVRls6vNTIdO2I3ods9DjGAhYmU2rXXchmmild5yKXzC5/Ph3rKRPY4/MeT6RGO+tm9aR427qovHIRsmf8gNQhCyg0hV2bTWCRmmiVR6626hE4lqbN0ud1ZO2hPNjo4YwULEymxA3IZpIpXe0h0+ESlzRCzU1NWz19ET7R4WPp+PvNKKLnFn2RBzlmkjXBAEg0hV2YCEDNN4K72lO3Qimw06v/ZnZaywaHZ0xAgWwlZm67dlNa3NnrgN00QqvdkdjhGNRNKgBZi3aBUlFQPYOOuGLuviEeRQol7jrqKwckjCYxMEofsTripb4eaFOJprEjJM4630lopwjEgka9DZYUSHa0Npf1JjEzKHGMFC2Mps9z73Dmz5NG7DNN5Kb4mET6zZsIOTfvsA7zx4NaOH9o+rv2QIFMX436feDLtNrGl0Qom6kQao68QMQRCEAOGqsr0/ZyZ7bXs5IcM0nkpviYRObNu4joeuPZep9z3PgKHfi7kvu4hkRCej2SCTnHMZR6YHIGQnAcN00sGGETrp4BIWLFrGrrrGjvVn/v7vHe+TIVo4Rih+P+MFKlzNTHtwftL9x8Otc5dy6DnXRNwmkEYn+BVKZAVBEOwgYJhOPHi3Ybpm8ct46mo61j960y863idDtHCMUCyY+UcGu+p49aHbku7fbkSzey5iBAshiWaYWiexJcvCz75m9spWDp1R1fGavbKVhZ99HXL7NRt2sPKrb3ny5yWs/Opb1m6qCrmd3cxbtIqyAydQnAUxaIIgCFaiGabWSWzJsnb5h8xb0cLRMzZ3vOataGHt8tD3g20b1+Fes5RHf16Ge81Sdmxan/QYBMEOJBxCCEmkuN5JP/uRrZPY4g2f+P2MFzhvjIviPM15Y1xMe3A+L9+VeIxvLATCII659Djb27Y+iqvb5ebTO88FjDiz3v0GAtkx+UMQhOwlUkzvYePPtXUSWzyhE2B4gc8b46Qsz895Y5y8+tBtXH7H0wn3n2mCwycCum3VbBDdzgXECBZC8uo9U8Pm7b33uXfSOonNSsALfPs5hfh8hhH88+cNb7CdscHWssgA7uoa8nsP4MPPJto+4zZSFaJAYQ1BEIRITLlrVti8ve/PmZnWSWxWAl7g884pxO/zc96YPOY+b3iD7YwNtqukcSyIZncfxAgWwhIqb2+iOYDtIuAFznPCsHIHG2p9KfEGB8oiA2xd9TEFeb0pH314yJnBweJb465ixfQrcRYWh6w+JAiCkApC5e1NZBKbnQS8wPlOv6nZqfEGx1vS2KrbAc0GRLd7GLYYwUqpJ4AJQJXWel872hQyS7i8vYnkALaT5Ws28Z+WNuasbKUkT+Fp0zS1Q2HRppT01+qpw71lA3scH/7YgsV3+6Z1+Hw+ts+9uZMAZ+rRWDo9JEJuIJrd/QiXtzfe/L92s2nNSp5obWXeSijJo0OzVeHKlPcdCatuBzQb6KTbmQxnEN1OD3Z5gp8CpgPP2NSekAYilSkOl7c3kRzAdvLJ0zdzzrQHeO7MUupr3JTkw9inGnlz+rUR9wsObwhQWVYQMWfw2iWv0m/s5LjGOHDoSABaK/tnxaOxeD0kQo/gKUSzc45IZYrD5e2NN/+v3Vz/xHvMuuE8nj2rFFWzieJ8OO4pD5c+8GLE/dJpBAY0G0S3exq2GMFa68VKqRF2tCWkj3BliiOFPMQ7ic1uAsa58jZTXqgYWOrk/H2jh0NYwxushKoaF2Drqo8p3PNwXIWltoxdELIF0ezcJFyZ4kghD/FOYrObgHFe5G2goBD6l7qYuK8rajiEGIFCOkhbTLBS6grgCoCHp53LFacdla6uhRBEKlNsR8hDJC9zMiz87Gs2b2/hvoX19Ct24HCA3w87m9ezq67Rtr58Ph/uzd+xxwlX2NJeJLK5HGi8yCO87oNVs6fceCeHnHh2hkfUs4lUptiOkIdIXuZkWLv8Qz7d3sRjC91UFjtwOsDnh6rmT/HU1aQlLtluRLO7D2kzgrXWjwCPAPDRgzpd/QqhCRXuMOlnP2LKnbNoam5lZ3VyIQ/hvMzJ8uo9Uzsq2V17THnH8nsX19naV3NzM/7v/ttJ6Op2udF+b5fqQHW73En1lSmhqdvlDlnpKBnxE+9N98Gq2S9+ullXN7ZleEQ9m1DhDoeNP5e5d19Pe3Mjy6uTC3kI52VOlil3zeqoZDf16MqO5dOXuG3tK5RhGk6zkzVWM6XZ2zeto8ZdFfJ4RLMTQ7JD9EDChTs0trRRs30TE44/NiljMpKXOdlxT7lzFo1NrbhrUheX/PziL7l42l/Y6/BxnZYbJY27isVnd5ydk14Bv/b3aPEThFwhXLhDa0szrh0r2XPcJUkZk5G8zMmOe+7d19PW5GF5TWrjksOXoQ+tcbnozfX5fOSVVnQ5JtHsxBEj2AZS9eg/VYQKdxg/Ep546yNeuahf0oZruEl1dozbDiM9Eu5aD6991cwxl46LvrFJ734Ds2IiRTjCib3S/gyMRhCyg1Q9/k8FocIdJoz08+zbc5h90ZCkDddwk+rsGLcdRnoqyPZH/aF0u8ZdRWHlkAyNqHtiV4q0OcBYoFIptRn4g9b6cTvazgVS9eg/VQRnePD6/Gze2cDAXskbrqnKI5ysd7myrCDkJLjKsoJO72+Z8zGHTvxjwuNMF/HEcYUT+1ChEELPoKdrNqTu8X8qCM7w4PX5qHa7GdDLlbThmqo8wsl6l3PRUxuJeGNvw3u2JYexndiVHWKiHe3kIql69J9KgjM83P74Al55831+vl9Xw1VrHZeXO1V5hJP1LkdKgxZg3qJVlB04geI4RbZ253bbY2uj0dPjuITk6MmaDal7/J8qgjM8vPHkPWx4+1FO3K8C6Gy4aq3j8nCnKo9wst7lVHtq0z0hTDQ7O5FwiCRJ1aN/u4gWquGu9fDiu/9m+slF3PpBI7/6sa+T4QrE5eW2O4+wu9bD5NufoqGulhfP6wWkpkpdIAzi2F8cF/e+WjlE3Ey6m/dG6J6k6vG/HUQL02iorWb1P+fxyMnF3PJBDZOPGtjJcAXi8nCnIo/wto3rWPrSTG755Sgg/VXqYkGMUoOertliBCdBpksIx0K0UI2ZLy7k2EFtVBQWcMAAOOC+jTQ0tTG0XxmDN6+mvcUTl5fb7jzCz7z+Ed+u28DZ+xVF9S4nWhADjDCIw6KEQYQTC4dyxHIoWUcqxC/b4+wEIdNlhKMRLUxjyctPcsIgD30KizhoABxz3xqam5uprKyk14CFOFpq4vJwpyKP8IKZf+S0PX0UehuAwrDe5VR7YyNpXKh+sx3RbPsRIzgJMl1COBrRQjUCXuC/n+CioryUm07qz/yvNjKqwsGw4Xtw9AGjYcunGfNyB8Y/uJeDJz9pYMG3CodDdawP9i4nUhADjGwQpQf8LGoYRHeLre3p4if0TDJdRjgS0cI0Al7g60/Ip6S8gl+e1Ju5q1czukLhHL4Xe+7/Q/ba9nJGPdwNtdVs+vJT3i30M+/LTZT0bsbhMBwFwd7lVHtjI2lcLuq2aLb9iBGcBJkuIRyNaKEaM19cyLih7Rw4qIgNtY142/PJ114ePbWYc174lh1Vu3j1wt5AZrzcgfFfe8xw7l1cB4MPsf287qxp4NXVTRz7i9izQQiCkLtkuoxwJKKFaSx5+UnGD21m/0HFbKytpaatiCJaeeTUEs6av5TmnRv4w4X9gMx5uJe9OY8rjh3E1KMrmb7Ezdd7nJ7x8yoI4RAjOAkyXUI4ErGEarz4wWc01XtZtMFDfYumpqWByw9ysU+lg9O/7+QLt4fKUiO5ebq93OkKNRlzyX34iyp4Z0l8ycetj/Fq3FWsmG6IvLOwmDFpmL3b0+O4BCFRMl1GOByxhGmsWLiAlQ3tLNrQQH2Ln5rmOqYcbGj2mT9QfO6upm/JICAzHu50hJokGkIRvF9At0WzezZiBHdTooVquGs9VBQ7eW/yCCpLXfxnfRPnPbuJXx9ZTGG+kzP39vH8C03sf/82XE5HR0niIQl4uRPJo5yOUJPnF39Jm6OQH0x5CIBVj12Hr6UJgBr3tx2Py0KJq/Ux3vZN6/D5fMb/c2/uELpUils8j8V6ellMQcgFooVpNNRWU16cx+xL9qNvSR6ffNfAlc9+xZU/LMWV7+Ksvb3Mf6GZH97/HU6ng8b6Gkp69aFXAh7uRHMopyPUxKq9iWo27NZtq2YH9k0FotnZiRjBMZJrBTGihWoEG5l//WAXF+yfR2Whsd0PhxVy8YE+VnoHcvQBo1nw7iImHH9UQgZoInmUUx1qEsgGkV+8W/B8LU0Mmnw/AK3ujQweMRqIHp82cOjIjv9bK/vHXDgjXUIns6CFnkguFcOA6GEawQbmjA82c8H+efQvMrY7ZFgJFx6oebd9NHvu/0M2/PNJho+7ICHjM9EcyukONUlUs2G3botm92zECI6RXCuIES1UI9jIXLetiX9/B08sr+uYxADgyttIXW1twnmQE82jnEioSaiCGNuqG8DnZeSF93Za7vF4uOmp93h7cebSpYrQCULqyKViGBA9TCPYwNyxzcMn32keX96Gw+Hs2M7v+hxv7baEcyAnk0M53lCTUCECdbvcaL+3y8S1bPCCimZ3P8QIDkGw1zcXCmLE66mO1ci897l3ksoQEW1ynp0e9lBp0EZeeG+XjBHv3381rZ5W/jx1Iru2bab6jnMB0H4fm578LQDK4WLwVdOTGo8gCOnD6vnVWmd9MYx4PdWxGpjvz5mZVIaIaJPz7PSwh6+K1tnQXPXYddR8Z4Q7iGYLdiJGcAiCvb7ZXhADUuOpTnZyWiz7p2Lc1nzBW9z1VD04DYD8wiIOO/862hob2OPCuxg8YjS191/GoEsN4WyrWk9+/+8BsPUJeyY9Rnp8JgiCfVg9v0DWFsMIkApPdbIT02LZP1Ue9oBWWicagzHZ2NfSxMDz/iSaLdiOGMFBBHt9Jxx9YNYXxEiVpzrZyWmxTM5Lxbit+YL963dQ1G8IAJuevIavF/8DV2lFx7ZKKbS3zXynO/5XShGJWGf6yuMzQUg91kf4V776An4//OGCriWFs8UbnKqyzclOTItlcl6qPOwBrdzy3VoKKod1LN/61NWdthPNFuxEjOAggr2+v5s+P6sLYkDqSjcnMjnNGt4Q6+S8dHnYvc0eCkcehvpmVccyp9NFXn4BAO0o/PU7APA310fM8pDp2LQAsUzUkNQ8QnfH+gj/hMHb+GKHj74lA4DsKoYRIFVlmxOZmGYNb4h1cl4mPeyi2YKdiBFsIdTj+4enf8f6zUXMXtm5HG+2FMRIZT7dRCanWcMbIu1v97jDhUB4lYvvTb4bb1M9vvZWyvc6At55KmQbTperY3ZxPDOGE8UOoYvFa5Et4i8IqSD4Ef5pozTzP2vgR3/bgNO5e8JYNhTDgNTm0k0kB7I1vCHS/naPO1zeXr9yMuTi2PL2imYLySJGsIVQj++n/KgiJZXK7CKbSjfHE95g97jdDa04TvwdXp+mr9eHMmdL75h3M+ufup78yqFopVj32K9p91R3ujlmihumzwnpFfB4Grhj6kQRQkGIgeBH+HuN3pNJx2RvpbJsKtscT3iD3eMOGIOBfL39vF6Uw8WOeTex8bGrcOQV0u9nv6XdUw0gmi2kBDGCLWR7GeRQZNOY4wlvCIx71uc7OgpxOBwqqXF7fdqI/21tR7nyAYz436ZaBh80iV0NW/jzUwsMEXv7PtYB3gY3G6ZPAsChHLT2NSrkpeuxk8SfCUJyZHMZ5FBk03jjCW8IjHvOfzd2FOJwOBxJj9vn81FQOYz2tlaUKx9naQVDL3mArU9dzeARo2mt7E9paZlotpASxAi2kM1lkCF0OrEnbpmcFUU84g1vCJzre597J6ZCHPGlUts9ccLf3oqvqY5di2d1iGQ6f61LbJcgpJZsLYMModOJnX/j37KiiEe84Q2B8/z+nJkxF+KIJ52aArS3De3z0ureSLunmnWP/Trt+YFFs3sWYgTnEKHSiYVLMZbuCneJhDfEGz4RKZWaz+ej4b0Z5J92A0XFvTqW65YG+gwclnCsWLIVguTxmCD0XEKlEwuXYizdFe4SCW+INztEpHRqfp+XDc/diDrkbABc5mQ3lyuvwwOciG6LZgvxIEZwCrHTEA1lMGqtwxqR6a5wF09YRuC8HDh6SEzhE+GMZev5bWvyMLy4lp0r36bsCENUd33xL7Qzn7qaXZ2qD8XjWQj16Gv7pnVseu6GrKhoFKvXQmrRC0J07DREQxmMkYp4pLvCXTxhGYHzMmTUmJjDJ8Idf+D8+ptqGeTYyaa1/4Ih+wDgbWvF621ny3drqXFXdWhsT9RsEN1OB2IEpxA7DVFrvO3YoU0cP/U+Th97YEgjMh0V7oIN/HhCSZ55/SOqt21k9rcbWTJlIBA5fCJcrHHg/M6Y/wHFNPGbg51c+9YsqlcsRDmctNTXkFc5FH+Lh4Kf/qajvU1zb+6YxJCIyPh8PvJKK7oIbSZiwpK5McDuMYvYCoK9hqg13nbcsHoevPpsDjzm5JBGZCrz71qxGvnxhJEse3Mezu2fs/zbFfzllyOA6OEToeKNAVw7VrLkxSfopT389uA8fvPGq2z+9r84XPl4ve04Cktx9BqAKizr0O2eqNkgup0OxAhOEXYaou5aDy/982P6OBq5+JBSlL8d1VzPrNc/5MNf7QF0NiLTkX83UQM/cF5uH1fM1FdrOhKcV5a6GDsUjp96H+9Ov6bjXIWLNQ4UMZl5RiU/f/YjfnlkH9a469irr4Nv63fS5ihAOR34Wzw4i3p1Sr6eV1rRIRx2THJY9dh1+FqaaPdUx+1xzpb4M5nsIfR07DREG2qrWfX+C1Q7qzn/4HJc/jZ6N1fx6Zuz+cuvjApnViMyXfl3EzHyA+flrnElXPtqFYGSFH1L8hg3zMeDV5/Nr++f3+lchYo3Pm+OUcTk4dMHc/6zc5l05EA2tLXz/b6NfF3vJq9yGDXuavzA9uemddJt0ezQiG4njxjBKcJOQ/SZ1z+iX14LdY3tPPSvXXzwTSP3nVjA5a+1dDIiJ4xyMGP+Byxc+nlKK9wlY+AHzsuAolZ+MsLBYQ9upqKsCIDqhmYq8rxdYp5DxRoHipj0LXZSoFs5clARt67y8/jPS/np0x5+PuVGXp0/h4Kf/qaTARyNQLqewKO4GncVG7/5ElA4XcYYfF4vbQ3VrHrsOsZcdg++liYGTb6fVvfGjpyVYAhRtF/q8mtdELIDOw3RZW/OY0h+PU2NLTz14XY++qaG+04s5PLXGjoZkaeMhiUvPsF3y95OSd5gK4ka+YHzMqiojeNGOBn34DeUlhn3F09DA/3zW7qcq1DxxicMbuSLHT76lFRQhodjBhdz+7u7eOTnvTh9biOX3v4gf7vl17Zo9pbv1uL3enEEafaK6Vd2lGEWzRZAjOCUYHchiLc/Xs3arc08OL6Aq16v5aRRLnoXKk7c09nJiATw+j9l0gH5KckbHG8sb6j9A+elsrScm/p4+fz5BubffTVaa86Z9gAzJxR3MqxDxRr7/Zqdtbt49JphPPNJHRfsl8frq+r52WgnA0sc7NlHseJf7yR0jIF0PYFHZyumX4ly5OHqPWB3laK2VpylffC1NEVsq7pqG7uqtjHgnNs7LVeAZ+FDXbaXR1uCkBnsLgSxeulC6rY08LfxhUx9fQcnjc6jdyGcuKejkxEJ0KZf46ID8lOSN9ga/pCIkW89L31LKvlln3YW19Vx0Z3z0Foz64bzmDGhpItRHRxv7Pf7aayt5QcDCpm/rIrz98vnvVW7+NloJ/v0z2Pivi5efei2hI4xlGYXVA6jafu6DmM6oNmDJt/fpQyzlbpdbqqrttP/nP8NWqPZ8uLtXbYXzc59xAhOAXYXgjjxiL05cUgTJ+xfxpnrN9K7tJj9R/fn1j28fGEakQHj+tTrpjN7pTsleYPjjeUNtX+48wKENKxDxRrf+9w7sOVTKktdLPymkY01bdQ2+3jm9GKue9vDDT/O58rXP6LaX05/r5fmqo0oh4PCyiFxH7OzsJgd827GUVSGy2XcpLzedpxFvaC9OeK+GnCVVZLf/3udlrdVrQ+5vTzaEoTMYHchiL0PH8teQ2s47IA+nPHdlxSW9mbQqKFctUc7H84zjMiAwfjwtAuZt2JjSvIGB8IfFr/0BBuWxu9tjnRegLBGdXC88ftzZrLXtpeZenQlkx5fzZaaNuqbvTx9ejFfbmvi+BGKZ162T7O3PnU1rfU7KejVD7BodhT82o+juLyLZre7N+HX/i7bi2bnPmIEpwA7C1hYvae76jxcelA+v36zkV/92BfSuI51glq8mSsixfLGauAv/OxrNm5r5q/vu9mjogSX0wFA382raW/xxOw573x+C6n3aU7fx8kKt+KwoYXs+f1RnL2rmsc+a8W94B6U04XPU0N+WQVgiCS0hR1ni3tzx6MzK87CYsZcdk/H47ftc2/uqEDX6t7YUdEoEG+GBp+nmm1PG54HlV/MwIl/iXquU0E2xbEJQrZhZwELq/e0qa6aSw/KZ+qbNUw+amBI4zrWCWrxZq6whj9cMmcep+xbFreRv3b5hyzb1siM97fQp0/vDo0r3LQQR0tNzEZ15/NbhscLZ+3TTr/exbS1+xjwvWGc/cPUa3Yg/zDQSbPB0G2/34dqru/QbDB0u+/xmSm6IrqdesQITgF2FrCwek/X1rSgFBwwgE5hEIkY1/FObIsUyxvrGF69Z6qlOMaPO7a3enYhumEdbOifet10PthaxazV7Tj9bdz52VYASisHUTH2yg6DtbQw8HVv6xCRYJGpcVehNeRVDGbQBXcA0Fy1EVfvAeyc/TsABg4dCeyuVX/T5Amd4soC8WZN278Fh4t885GcVVjTTS5N9hCEdGNnAQur93RnjQcFHDSATmEQiRjX8U5qs4Y/jB/expxl1bz2VXunbaKNY8pdsyzFMS7s2Dbg2Y3VqA429B+ediFvbd/IW/+A2l21uErt1WyApu3rqH3rb8BuzQZDt4FOmg2Gbg8453ZwODs0G0S3uztiBKcAO1OjdfUquwAX++5ZmXCFu3gntkWK5Y3HyA/Xb7Ke8ydumczhUx7gmode5v6bfsUQy+OpDq9sGIJF5qbJE/C0eDuJaTSChSjgGUbH3ETGkfg1oSdjZ2q0zl7PPPNVwsA9hyVc3S7eSW3BMc5Tjh/NourOYRjJ9Jus53zKXbM6PNtuXzEjpszsWJcJzQZMD3EOiTai23YgRrDN2J2jNxWlnOPNXBFvjHO4UItw/SZzjO5aD4dd+lcKHT5WLn69y/pIs4BDUVpaRo37W8OINdH+dtqrt3SU8bRuC6FFefCI0Wz8ZjVKOWgz2wqERngb3AwfuVfCxywIgn3YnaM3FWWc453UZzxPPgAAIABJREFUFm+Mc7hQi3D9JnOM1sIbrh0r8Tc5Oq23Q7PBmIAcrNmB7UMZjzdNnoDTlYdf06HZYOj2jnk346BrTLCQ+4gRbDPJpEZLR6njRDJXxOuptXrCJ/3sR0y5cxZ3/OqMsP1qrRM+7rtnvUNtXT3zLx7MtMUv4/c5ou8UgRumz+kS3hDAG2cZTwVo3+7Hj1r78TfWkO9yhRRhebQlCOknmdRo6Sh1nEjming9tVZP+GHjz2Xu3ddzypSbwvZrrfwW73Eve3MeeuvnfLlhJU9OHMy5j66hvamOvOLyuNoJEEmz2+PUbKfTia+1tdMyrf04lWLI97q2L5qd+4gRbCPJpkZLR6njRDJXxOOpDfaEN7a0UbN9U0de33CZIaxG8yW3P41G8/Stl0QN03j01Y+46ogiBuU3cfIIJzMWb+ebh6/E4TT6CZ60lk6Gjtq703tv/z0iCrKdj7YkdY8gRCfZ1GjpKHWcSOaKeDy1wZ7w1pZmXDtW8trMP0bMDGE97m0b1/HQtecy9b7nGTD0e1H7Onp4AY7Weob3dnLqaJg98wpc5UasbiY12xo7HCCSbotm5z5iBNtIrAZmKI9vOkodg72ZK0Jh9YSPH9nEE299xCsX9ePUJ75j/eYiZq/s/Cs7kBnCajS7t26gtkVH/THwq3vmU57v56ojivFrPycPa2JOvo8DjjuB8ZdcCxDWQ2AlWHxqd27n0zvPxaEclPet7Fgea2332p3bWf7XiZ32Dbd/qpDUPYIQnVgNzFAe33SVOrYzc0UorJ7wCSMbePbtOcy+aAgXPPkZszf3Yt6Klk7bBzJDWI97wcw/MthVx6sP3cbldzwdsa9xw3ws/bqO6eMLqNu51cgZvA2mPPAspeV9MqLZAA3VOzPq1RXNzgxiBNtIrAZmKI9vOkodQ2pijAMEe8JP3hOe+6SVyhIXU35UAYMP6XJMgcwQAaP58Tc+5L6fuvjzkjZe/ufHYX8MrNmwg3eXruZ3PyqkssSB1wfVnmYuOaiAx9+ew9FnXBLzDSmS+ER7lObxNFB84jX4fL6OZQPAmNksv+AFIauJ1cAM5fFNV6njVMQYBwj2hJ86ys8rn3qoKHFx0Y/24Os9Tu9yTIHMEIHjfm/OQ7jXLOWFc8o46/ml7Ni0PqQ3ONDXiCFt/Gy0i1F9XXyzs5ExAwsYP7SZJS892eG8iIbdmg1QI5rdIxEj2EZiMTBDeXy11rZWmMsUVk94u9ePy9fCBfvl8fSyWiYdWt7lmIKN5vEj4en/NDO0vIQzfpDHPzc0c/zU+3h3+jVdzsNFd8ynX347877QPL+qDr/Pj6fVh3I4KC3wd9yQGqp38umd53YZq8uhuiwLpm6Xu1Nd+QDBQunz+XC/PRN/2+4ZzVrDpu++5Y6pEwHkMZcgZCGxGJihPL5aa1srzGUKqyfc5/NR7Gvg/P3yeX5ZFRMP69/lmEKFjzw7fQ7HD4d9+udx3hgnM645m2mPv9vlPCx5+UmOrdjJ8k2aqvo2Zn/eYmp2CxqF3voa4y+5Nm2aXVA5jC3P3dCh2wHNvmnyBBqqd1JW0S9qO0LuI0Zwmgnl8QVsqzCXjsl14bB6wusbW8DbRq9CxaBejVw7tm+XYwo2mlV7ExP3zeOFL9qYcngR0z+up6ygmYde/IBbLt0tbM8v+pJNO6pxeqHNW0h+UTGNHjeVxXkM6l3AfeeN6hDvsop+CT9i8mt/zPv625qonPA/6EBVIb/hZdj0wm0o7efgG+bH1I7EhQlCdhHK4wvYVmEuHZPrwmH1hLc0enD5muhV6GBAr3p+OXZwl2MKDh/plac55XuG1lU3tnPOmDzmrqjlzafu4+zfdi49vGLhApZVN+IqLCW/qCKEZtfhqatJm2YDtNds3V0i2dRsp8tF9XM3xNyOaHZuI0ZwGgk3cS6/sBR3jT1xunZNrotmTIdab/WEn3rddDZv38nOukb8eQUcOqOqyzFZjeY6TzP+9hZ6Fyj6FCkuO8TPKXu5aPPDi+/8m1+d+RP6lpfgrvXw/H+r2aOynBkTBnHVgkZGHH4S+9W8xdSjd8eCWSdwpAut/R1J1v3tbSgFeaUVHRWKYkHiwgQhewg3cc5f2IflNfbE6do1uS6aMR1qvdUT/vC0C/FUbcTj97Njew1HPbgRh8PR6ZiCw0caatwU0s7e/ZzUN3txobnkwHweeu95xk++ppMHubw4jxnnjMkqzSaUZucXoKM7nTsQzc5tbDGClVInAQ8ATuAxrfWddrSbS8TigQ03cY7Be9sS/2vn5LpoxnS09Z2rwx0VsfKbu9bDfhNvo9ip8GpYX+vnkIcb6FUAQ3s5+OkwX0c/t8z5GF0ytJNn5tmFr/Kl8oe8IfVUJHWPEI2ertuxeGDDTZz7eo+xtsT/2jm5LpoxHW19wCDeXSHugi7bBbbpKHThdlPoUmyq15z8XCPtPuhXoijPo1OMb7A3XTS7K6LZmSFpI1gp5QRmAMcDm4FlSqlXtdZfJtt2LhGLBzadmRmSmVwXzZiOxdiOxyB/6IWFlLh8/GNSP8YM7cMXG2o49ZmdvH5BGb0KFbvaC/j1u8voXVGJc89jWT9/Bn+2eGZeWxu+ElKo+LBgwomP0uGTowcegdXtcuOdfQNoIydwW1DC9raGarQ3fM37VCGP4YRIiG7H5oFNZ2aGZCbXRTOmYzW2Y90ukOu3sncpr1w2nN6FDtauWcPv323ixfP7sNPj5exX53L0GZeEjJ/OhGaDodsBzVbKifb7aXNvwlopzosCbVSuG3PZPVHHYhei2ZnBDk/w4cA3Wut1AEqpucBpQI8R01gNvnRmZkhmcl00YzoWYztWg9xd62HO2x9x2SH5uHwttLR5wdvEBfvnsWBNG78+soi66haOGVzAQ298zujDK22LxQsQEJ9gz1AkMbY+Atu+aR1bX/wLSjnIqxhiVMnA+OMs7YO3wZ3QuAQhhfRo3Y7V2EtnZoZkJtdFM6ZjNbZj2S4412+fQgcNNW5GlMPpe+fx7PJmph5ZzPihjSx56UkKCouyQrPB0O2DfjeH7ZvW4fP52DbnJkB36LYCUA6cJb0jlm4Wug92GMGDgU2W95uBI2xoN2dIV3qzWMaQ7OS6aMZ0LMZ28DY/7N/C+XPe4YEFn3dKgF5ZVsB5Px5NAW288KXmic/aafU34fV6AfBrmL3KS32LZmdzE8P3Hh63ZybYY+D3eWmv2UbvIV3zUAZ7hiI9nrJOhBg4dCS7SsvZMe8WXGV9Qe0OKHPkF6NQOfmYK9YJO5mc2CMkTI/W7XSlN4tlDMkah9GM6ViNbet22zeuZ2zfdp6c9yDvvvGPjuJDpaVlHHHUMcY4v25ibVU7L9+zltbmRtA+HAr8WjFnlZf6Fj9662v0GTA4pGYXbVnM6EOPpWLAIEp69e5YnirNthIoirFN+6madzPOkj6ddFu5Cmj3uEOWXM5mRLPjJ20T45RSVwBXADw87VyuOO2odHWdUuz0wCaDXaEWVmN69YadeH1+9uvVwr6T7qKorJzmhjrO26uNylLjwgllbAcb5GX5cPZhA3jXeTRDjz6zo68Vf7+aBYuW8d6UYVSWunB7vBw1fRNFvcpxOXeXP/Z62+g/dDBX3Rv/pIngR0wd8W7Hjuu0PJRnKNLjqWCPw5jL7uGTO8+j78lX43R1vqyq5t0ac+nObIoLi3XCTrom9gjpxarZU268k0NOPDvDI7IHOz2wyWBXqIXVmN6+cT0+n4+jylu4/dLxuMoq8Ta4uWivZvqWGJPQwhnb1na2VPkYOGwEZx1WzavOY6j8sZHq8ZuHr+w4d1OPrmRXYzvnz6vDXzialpptAGitqff5aS/wkecq4IDDjsTXvA/52kuB8uLCixMfpQUO9t7xGq8taubYy27rGEeqNDsURf2H07xzcxfddjqdNL19X0y6LZqd25pthxG8BRhqeT/EXNYJrfUjwCMAfPSgDl6fq9jlgU0Wu0ItrMb0Fnc9eaUVQB75lQMYM+k2VjxzG3O/WM2S7eGN7WCDfIu7gbzSPFSvlZ2M4LYmDxMOKux07oKLarhrPVw97yuO/cUfkj62YNHc56gTee3hPzNx2v/Z4hnKL+2D0+XqUu2oNahyXCSyJS7M7hjCWEhHCVqhg6i6bdXsFz/drKsb0x/bngrs8sAmi12hFlZjunZXLa7SPkAphZWDGHbR3Wx89nrmfbGKt7dFNrY7t9OAq3QrAP5en4FpBPsaa/jJ3iVsqWlj2foGttS00NvXQLOznB+PG0uebiPfoRncp5hhlcWMHFDO8IEVlJeOCDn2qpoG3quqC3tsqdbsMZfdw4rpV4bU7XUxtiGanduabYcRvAwYrZT6HoaIngecb0O7OUGqJ7ulG6sxPfLCexlzeeeJAftPuo1Vj17HJ8/uruxz+JUz+GJHKyMvvNeyZS8qywpYOvOqkO0AeFubmb1SRTx3t8z+mCXLN/HOkq6xXvEmNA8Wzddm/hHXztUsefEJvlv2dkjPkNY6qV+67Y11tLo34amryalfynbGEMZCukrQCh30WN1O9WS3dGM1pm+aPIEhQem6hl10N+se+zXXWbyad0ydyMaq2k5PtLTWFBcPYPL//In7b7iCfkechbehGuX3UvXPx1BAa0Mt8z9p5pX/VpPvUhS4HDgdisFOD9Mv2R1Nc/iVM3A3tHYZ687qOvpVlHe893ga8eaV8s/3F2aVZm995a/4fd64980kotmJkbQRrLX2KqWmAm9jpNp5Qmu9KumR5QipnOyWK7gbWjsZuas3VuH1aT6fezMjL7yXLe56/Ot34HIq9h7Wv2O7ssqBnYzpYOYvXk3ZAeNp/uiBkHkYP73z3JjzMwY/Aj3ngFKenbGUh84fxZUvzOWiQ8pDeoaAkL90Qz0C8za4qfr/9u48vK3qTh/4e7R4le3YkZ3F2ZyFLWSBBtppC7RlnwkwdEtSoMNO+RVaIDOZaUNIaUtp6ZS2k1AKYQrDkEDYUijDvjWQlhBCQhaSEMjiJd5kyZbkVbo6vz9kyZItyVqufO+V3s/z5AHL0tWBJK+Pzv2e79l4R9TKr9/jwMyyAbz/4hM47cIlhrh1lE4NYaLnJUMPNZr5JJ9zO5ub3fTM7/fB1dYMZ0sjOpobUHnKeYDig4BEZ1cXAhJw7X0L9//Hlejx9qGvyI5C+2ycVDcpfA15+D0cemz0o42H/0wIefXu66Ie//i1jbB/7Voc/tMPRzxX7cwGksttv8eBadZOtJomGOZ2PzM7farUBEspXwTwohrXIuPzKxLF1VNgtVVi7nX3oG3NChRXT0Fve+PoLx7k6PTiuX3dOOuac4H7f5/xmIbfAi32e7DsZAu2HXbDJnrx6PsKntwbfbu3qPFtmHpd+K9LJuFfHr0fc798ASZMrQMQf3dyJE+nE4/9aCnuWzwJ339hE9wuJxp2bYnqn6lHyd4uHquNPZQdzO3cEQgE4O10wtffC9fezfB1tiDQ54UAICDR72pBw8bbUVdThi/UlGJjkQknffkCmK0FAIDdh1tRXD0FDc0H8Pmb78Fba1agrG5+SpmdDkUKCJMp5vfUzmwgmNvJZ3Y33thwPzM7hlzKbJ4YR0nbsm41Bvp64fO6o0ofmttdmJvgdQVFxWh4+Fb4vC6Y7OXhx+1lhXFfE1kG4XK0oenIwfD3zGZzeHdvLAHFj3Urr4kKuchboIFAAN2dDthLTJg8zo2nbjgR39k4sm/lm4/fj+OaN2FyQQ8umaXg+T/8BNfd/T9R75WoJiryk/LimR48+MrjmFMJ7HjlcZzx9auSPtVprCV7uzgbG3sA7Wo0ifRISokerxuutma424+hz9mI3o6WYP0tfLDChwKhYPK4YhT4ezBxfAWKZ82Gtbh0KLMVgSde+yB8TUenNzwBjiWdzAaGyiBCd/9Cht8FBIDeTgfM5cHHmNmZYWanj5NgA4tXdxWqxc2UvawQe9ctD3/d7XBj0tKfjwi0pruvS3idL113JwBg77rl4VtpobFH1xEH3/Nfl341qgxi19obUTh4tCUA9A87kGK4QE8nLK3OqL+QkbdAQ0EZ68jO0PNDn3Rv/6YNfa4G/OCLJfjzI+9jza3LcNVP/hDVeihWTdTwT8qLZ/rxv+/24+5zy/H//s8bd2UhnY0GaodwsreLs7GxJ5JRazSJ4gkdshNJygCKCgrx7e8tR5+jAT0dzTD5e2GFHwXwoQA+jLcV4MRqG2ZUl6J2zjhMGn8CLBbziOtvePV9NLwwVCI2Vpkd+nkTKoMI3f0LibWi3H54D2wzTwXAzM4UMzt9nAQbWLy6q8iJayaGT6RnXn4v5tZNSPk6sVaQmxxuTL3srnAwh+qId274MZatfhimIhsCAQWWT/fB7/fBN9APAcBSkHglwtfdBZvixm8uPT5usX4yf4FDn3SL/R4UFgE1NgsuPk7gkR3bwmGYqCYq8pOyoigo8nfh8vlW/L3eh+/MK8ADESsLoUC86IaVaW00MPoO3Xyt0aTcFwgE4HF1wNXWDE/rYTiaDqNq4XkQkBCQMEHCZDHD9eEruKjgA0xbWIna6pkoLLCm9X5aZPZHT9yO4vNXISDMUAIB9B88Bp9fQW+/D4BAcWHsaYbH0YrqBVOY2cxsTXESTBkzmUTUxLvZ4YbVVomComIAwEBfL6Ze9Vv0tjeGA7ltzQr4lYijKgfriAGgoKoWtVevQcPDP4S1qhbm4nK0rF8BqfhhsVjh8zrRb6+BxTTyEAq/x4ErTrQmLNZP5i/wwR1bsL2lBw+9HbwFZxJAh3cAMyoEtr+0Aaecc2nCmqh9297G5kNNeHxnD/p7vPD1eTGh1ITacol1/1yODbvdUcFsad2Nv9x/Z8obDXJlhy6R0Ugp0ePpgqu9Be72JvQ6GtDrbAmv3gbLFAKYPK4Yp0+wYdbMCjxfVox553x9xLX2HtyCr5wy8jCIbFEzs62DPeNnXPVbHP7TbbBWTYa5pAwt6/8dUvHDajGHyyoKTBJ71y2HlBKOTg+8R3Yzs5nZmuIkmDI2qaosasfwUJmGF3vXLYfP60ZveyMsZhH1Op/fj92DdWMDfgXurS9AURQEetxRz6u97G4AwTKI2hlzcOihm8NNzCNvLwYUP2yKG/98Qgla6g9j2alT0i7Wv+Gex6JuwR1tbMFtzzbhF2cX4urnPdj0+5UJa6JOPO0rKHEfxvSzL8PWlzfiguk+HHb58R9nFKGj24+vzTDj+befx5cvvRIHNm/Cf10yCd99+H1ccMYJAJLfaJArO3SJ9Gagvy/YTaG1EX2ORvQ4j8Hs70UB/CgQflilD9XlhTjJXorp9lJMPb4Sk8bPhSnORi8ACb83ltTKbF9rI/o8LggR/bzpl90FIFgGMa9uwoiyimanFyZLga4z+9LZfnzjRCt+9tcefNYxwMzOUZwEk+qSvSUnIYJdI/r9ED1dULwdsFTUQOl2wTfQD8iRZ6q0NByCy9EW7m/pcrRh4tKfw2w2w3J0Ky5WXsfEaVXob6/HxAyL9UO34B7fWQ+XoxWXnWxBVbHARceZ8T+7tuPxtuqYt+dOu3BJ1Cf98qoJ+L8jXai2+rH0OcBWVgqgFFUTpoQDcXJBD5adbMHre52Y/ZXapDYa5NIOXaKxFFAUuF0OOFuPwdt2BN1tDUC/d7AO1w8rfCgrAOomlOGM6lJMPWUcaqtnpV2moHepZjYABPp8MFmsMJdWQul2obffByklAsNie1998OClyLIK24ILUDZ9Hoqcn+o2s19uCeDpgx5UWzGY21XM7BzESTAlvcFu+Ea5yMczI+F5fxPGffk76Dv6EQBAWAogCorR/OhtsFiCP3h8XicAoMg+BTOvDdZChzbN9TvqMXD4Qzzp7sOTe47B7/Vg3Phg2BU1vI3P9nyQ8iaE0C24l/70G3z66kP497PKYS814V/PCOC1I27M/urXY26UePPx+6M+6e8edzpMfS7ct7gU33+hO7yjOdSOZ/WSCvS5GnHBLAu+u6kVj+7yw2wObnpJtNEgl3boEqlFSolud+dgmUIjetsb0OdqRaHwwyoGN5sJBbXjSvClCTbMnF2BaV+aAltJpjk2tpLJ7exl9iAhgr8wmNmFpWj53+UoGNy05/O6AACl9smYO7jZrm3NCiDggygZh57t+szsTyZ9BadduGSwXdpQbkspmdk5hpNgA1Mr4OJtsHv9F1eHP703Oz0IDH7EN0kFk6orw++VTieKgqJitD25Cu7y8ejvdkNCoL/pY4iCEgDBye6Eb/8UAXdr+DjLUP1vaAI83LQrfh3+98gTkkJnz6cbNDv/+hdcPMOMjm4/OrqDj4VujQ0P1Fif9B/9Y7Cx+/DbX1GBWFqHagDf7XDgk0mXJjXOXNqhS5Ss/r5eOFuPobOtCX0dTejpOAaLMthNQfpQIPyoLi/EXHsp6mrKMO2kStRUnqyLUgQ1J6WxcnvLutVoOnwMMy+/N6uZDQTLIYTZEpXZk5bdhf62I/jcnMkAhjZphybAQPBDikTw90Kvmf2djZvQ39c7omwBADM7x3ASbGBqtEFLJCDM4ZANDDZTB4CGh2/F3OvuARC/E8XpN96H3UcdCAQk/L4B1N91TfAbErBazZhUVYZeixlfuvp2/P2FJ1B++tchAwEAQMv6FWi8718gpQKL2Ro+ycdmKxvRXmg0amxCqJowBS+3BPDyi4BfUdDp6kRl5ThUTZoy4rnDP+kDQBm8uHh2GYDo21+ZBmIu7dAlAgDF74fb2Y6O1iZ0tx1FT3sjMNAd3GQGHwrgR3mRCXXVNsyssaF2+jjUVs9GgdUYP8qyndkDfb2YtPTnmFs3IWuZ/dWbg9f58GATYAr+f29ZvwIN9/1L8PmBAPZOGAcgOOEevlot/T4UVE9DPFpn9vhSK86epuAvbzyBX1w7HcBQbgeKKrHDxczOJcZIDjKc3Yda4Bv84yUsQ03ZpTIAKBKHHrsNMy+/F59sfh5lC85DkX0onAorJ2L+TfeHN8CFNr95vR64HG3YtTYYNuaiklHHMbz5+dpbvoWbfvdUWrfYgKEViulnXx4z9IZPbL0eD745x4wS9AKIvv3FQKR8IqWEt8sVbBfW3oie9gYMuNuDG83gg1X4USQUTBlfink1pag7oRJTz5yK0mJjlSkYlcPTjwF/AEKI6MyWAfj6+3HosdVRPYKtFkt4kt1VOSE8OQ5tgos8OKNtzYrw6/rcDtRMmw+lzxtzHFpnNhDM7W+dIEeULXwy6Stcsc0xnARTUnx+Bej3wdfVhj6PC6/+NriaoPR0Yubl9464xRYQZtR8+2cosE+Nus6xP92MQF+w+4Mp4IPjk23o3rkZVltV+DnDJ7derwczrw02gG86cjB8cMaxR26BuagExx65Jdw2LcRmKxtxm+vi2QGs/1tT2kdgJrNCMXxi+8CKy/FySz1efg4AhkKWt78o1/T1dMPZdgxd7cfQ52hAd0czrIF+FAzW4VqlDzXlRZhfYwuWKcyvRPW4iSM6C5B6evv98HW1jsjs4n+8E/Om20esTAshMOX70SesBXwDOPZAcFU4spwj1FYNQLi1WqRQuUbkijQAHPjPJWh7+qe6zWwgmNsvNdXjpftYtpDrOAkmAEONzyMpgQD21bfhxGk1kBhc0ZUSZlsVJv/LbwEA/W1HMHfO5JQO6FACAUxf9ms4u3pQXDURnq4uDHS1Q5hDmymc2P7LJbCYRv5wNJvN4RPjfF4nbEUWoMgCm30WfrT28ajnhjY7hJqflygeXH1qIR5KcARmIum0tuFqL+UCxe9HR0sTnK1N6HXUo7u9ERjoCZcoWIUPFYUmzKopw4zqUkw9rRKT7cfDGuNUM1JHs9MTdTQxEFys8PmViEfkiMwecDTAagIcr9+T9HtJiREnxSk+H5SuDpjNJvi8Lrw6eApdgSn654jFLMInxslAAAKATceZDTC38wknwXkk3m7idmcXBtavDH+qDxEm84iJcbpaHl8JOdCDQK8bUgLNHR6YbVXoRQEKKydi/OLbwhvgQoYfhAEAE6fODP97v70m3C84lsjbXH3dXliUHpQXmWATgZQ3XMTaPLH08adxYMffccXK30cdvZnKcZh6OHeeaDQtL/4WFX43Tq0pxYwTKzHlrOkoKSoY/YWUkUQdIKD44XghemOc0usGoE5mA8HcDvQHd5a19Q19oCkoKkZRpRn2xcsxb1grteELIpHHNXc2fYbOcVVxc5uZTWONk2AdSrZlWarXaHK4UWqfHD4XPmRoB2/0ysCWdavR/MTtMNnL0dbaCZhMkIEALOMmYMDRAAAQCQK346XfA4HgqoTidaJmyc+BgIK+IztQVD0NJbNOw7FHbknqvycdoU/zoVZkG5ZUYHypFR3dvpR7M8baPHFebTf+vPvDEUdvpnIcptGPz6T88INLvwD0OLQehm6pkdmxrtMUcZJbZG7vXbcck6orY2a24+mfYK+9HK1OD3x+BVIiKrODXTICcccQWrAAInJb8cNsLYCtOtj1oeHhW5P+b4rkOLwP1qLSuN9nZtNY4yRYh+K1LEul5CDWNQKHW0esHGxZtxrdjmCNbuTmhVDohjY5zLz8Xsy97jd4a80KTL1yqK1N6DbXcIrPB7jbYS6tino84OvDgKMeVV/4RtL/LZlSozfj8M0TgUAA3Z2dmF1diAObg+EspUxpVzOPzyTKDWpkdqzrhOppIyedyWY2ECxhaOszR2U2EDu3251dkAEJv7MxKreFMMFcVQulq3XEa1LV39MNk3n0EhlmNo0VToI1kvA21xgKtdQBELV5Id1P+mFmM2q++RPAHPwj5nj+HlgqJsD56h9gm39eSpey2cpilkbYbGVJvV6N3ozDa8Qij+dc+44jqodksvVnPD6TyDhyPbOrqyrQd9atAERUbkvFB39XG1Ldvji8J7KUEo5OL6qnzhr1tcxsGiucBGtErZWDsRIKNMXjwqE1V4YfN5kETFVlI34QmE1eswJ2AAAgAElEQVQmFIyvDbfaEWYL+g5vR8HEOTAXl6f03sM3T6RK7U0O8WrNAgFg9WVV4ccS3b7j8ZlExmK0zAaCud3c3hKV2UAwt+dNt494foHVCpRPiMptYTJDplFnPLwM5J1dh/Gs+0Sc9IWzR30tM5vGCifBeabP3YG3Ins2elxo/vM9MBUUYcI//iD8uM/rwt51y8OT28hAi7Ui4vD04/Qb74tb/yYDCvqbP0HpiWdBBvxRHR6EDKBt4x1oAxCQQ7VqQgaw8srFsNnKMp4IZ2L4Roh4tWZ7WhWML50QfizR7Tsen0lEyYrM7VBmQwDCUoiJ//RDACMzG0ic2w5Pf8z2lvHIgBIuo/B5XTBJBY0bfozmYSfxmaQS82fBG7uPYeZFV6b2H54mZjYli5PgPGIxB29o2RcPrVwM+BUUVNXCseHfonb5muzl4bqy4dJZEQn0etB7aDu8u16HyWxG5WB/yKkzhtrkrLxycbgfcKRYpRBjafhGiFi36jwuN3wKcEaSfSV5fCYRjSayvVgot0OZXVxoQcPDt4ZzO1FmA+nltigoQevG2wEAZgHU2oN38RbUVeP9+78f3iuSzDVbekyoLbHFfS81MbMpWZwE65Aa58vHu4bVYo6a7O4+3Iriwmz9MRCQ/gH0HNyKqnNuQPHMz6Hxj1dHTXz1LtZGCDVu1cW7hqfTiXUrr2H7HSIDUSOz413HhOjczm5mA6HcBoCJ31qN5keXA33u8MQ3Hb39A+gvqFBzkHExsykVnATrkBrny8e7xvCG56GVhtCttJBMN3uYpALHhn+DDCgY8HbCWl4N73tPwiyEYSbAQGYbIdLpJ8n2O0TGo0ZmJ7pOZG5Hrg5H5nammW0vK8RHT9w+ol98qc2GcbbM/hvf2X0U9hO+kNH4ksXMplRwEqyRqOMn210IiGDbGJNJhAMv1R6T6Qg1Mh/tVlqq5s2cCIenHw6nCxXjJ4Tb4tjsc0Z55ejuvmkZvF7PiMfVrh3OdCNEquHI9jtE+qW3zAbUze337/9+nA4Y3own2G9/3IKn3l2LnjW/GvE9NXObmU2p4iRYI5FBmUpdld5sWbcaA329AACf1x31w+BX11+IbeYFmPP5c1V7v7tvWoaGI5/BaovuP2wuKgFiTIxTFbkSkMlGiHTCMZkVDJ5WRKSNXMxsYCi3szmBf2jTX+HqVbKS28xsygQnwXlG7Xrjboc73LPSYhbhVYpdf7wFz33sxVnXJj8BjuwH3NXhCHeKCHWJAIDO9hZMXPYLFNqnRb322CO3AEWZ/3GOXAnIZCNEqrfkkl3B4K03ovyjZm5HZjYwlNvpTOCjVsedHgQCwVZqJqmEF0QqS6zo9QUwcenPs5LbzGzKBCfBeSbRJ/1kj/4csSIy7Ox4AHB1eXDaktRu00XeEovXKWL7L5dEfd3naIQMBDDgccLlRXiynM4ttuErAVf8cmNan9xD17n9mzY4mo5iyYIJuPzpxCsLyaxg8NYbUX6Kl9un33jfiH0eQOyyjNDX8TI703HFWx3f9vsbYRl2VLJauc3MpkxxEpwnkpngqtUMvnnvVpiKbCgpG/1QjFj1vZ3tLQhIoPDIwajHzTGO25SBAKz2qTDbKlFz0XLUzgjWHKfTVk2t04BC1yn2e9Dv60WR34OL5oiE10tmBYOnFRHlj7HMbDXGVd/WicDhkUcrOzo9qJwUXQahVm4zsylTnATnibEKy4FuN9oaj6CgJLkjjb1ez4gV311rb4Tf7xtx6yx0wEY2qHka0MEdW7C9pQcPve1AVbGAs7cBpePsKE9wS260Fj48rYgov+j1hLp446q/65qoY5xDpASESPXQ5dExs0kNnATrgFo9JrMp3qf/5nYX5kZ8/de1/wZRVA5/T1f4FheQ+Q7gPkcjAn4/AgEFbc/dAymDtWfCUoiJ37kbUvHHXClOlpqnAd1wz2Mjzqn/ZNKlGa0A8LQiIv3IpcwObZSL3NgMZN7pwtt+DP5eDyQkXI42mAZz22QtQtX534f0D2SU28xsUgMnwTqQ7ZY6aoj36b/5V9eHfxg0tnZAUQKY8PXVACTMluBffrPZDO8rv83o/WUgAGtVLSyllai5ZOjY52MbfoT2Df+OQlsFJk6dmfb11TwNKBsrADytiEg/ciWzm50e+PwKar79MwhIBCzBKYHFLOB4ZWQ7s1QEAgEoXidMJZWoXXonFEUBALQ8cTvan/4JrLaqjHKbmU1q4CSYMjKpqgyHHrsNHV3dmLHsHigVdpRMjA61dMoYTNai4M7hQQMeJ0zFZSi0VYTrxwCgxWTG/JvuT3v8ofY1l61co9otqmysAKhx4hERUSizgeBmtrY+MyomTY96Tm97I0wpXlcqfjQ8fGv46z6PC5AS5vKaqIluvz3YQSjWxudkeTqdMFkLccPvn1Mlt5nZ+YuTYApLdIsv1m21SKs2bEVhRTV6fFKVsVT/0w+jJru71t6I8Ytvi3oMAEzCFHMzhc2WXE1yNtrXZGMFgL0miWi4TDJbbRZrAb568z3hr99aswKFdZ9DydyvjnhuZDvM4Y8nQ+3cZmbnL06CDSbZNmaxvj9aDVui18dqwxPy1OZ9KJ13PkxbDgA+f9znxRIrDP0eB9o23oH+8fbwYz6vM2btWMV4O+565IWU3jMUTl/7zs1484k/4H+umIEfv6HehoVsrACw1ySRMekxszMRb1wFJhn1+IC7E1YZu6tPOvtDPJ1OPPaLW+D3+QBvGx5Qse0YMzt/cRJsMOnuGM5WDZuiKHh+XzfOvOY84I//lfLrRwvDyBZqbc/9Gm2Dj5uLSjD32pH/H5IRCqenfn0b6mw+bDvsxkVzCnUbVuw1SWRcesvsTI02rtCkP6D40HvwPfQf2YE2ZJbZQDC3zc070eX14fjJNsyumaDbtmPMbOPgJJiSEu/Tf19vLxZ9e6gOzFxUElXLCwRXcafOmJXW+4ZaqLU0HApvrACCmysOPXRz0rfPQkLh9LvFNfjunw7gV98swx1vdeCX35qDv/xFn2HFXpNElKpkO1gUFBVH1fICgM/rwoK66rTeNzTpdz39EEpPvSjcHi3dzAaCub3v7afx8zPN+Ombfejo6oaz26fbtmPMbOPgJJiSEuvT/1Ob92GbeUH4UAybrSx4DvywYzBt9lkZtUcDMGIHcb+9JuUyCGAonKrhxGXzrXi/wY9/mmPB63udUavBeqnnYq9JIkpHMivJ9rJCwOMFioY9Xl2d8Up0RXk5JtYdF/463cwGgrl9Xm03FtYIfPMkC/7WCDy5rQ3f+0pt1AY2PeQ2M9tYMpoECyG+BeAnAE4EcLqU8gM1BkX619HVjec+9uKsa88NP5bMRDfWCXFA5n2EkxF5NKa/vQ1XL7TggvU9uPOrJVj5ZissZeNRMbgRQi/1XGPRa1IPPzho7DC3KSSZiW46Nc09rjZYxk3MeHzA0CrwTV/2wVYgceZ0M57+uB8fvtOCR3cF+wzbdJTbzGxjyXQleA+ArwN4QIWxkIHcvv49nLbsJym/LtYJcUB6xxynKvJozEKbFUIGcPHxVmzYb8Z3z5wWbo6up3quseg1qYcfHDSmmNuUtHRqmtsP7YVt5mmqvH9oFXhGpQk9fqCyyITz5hTgw45iVHz58nBm6SW3mdnGktEkWEq5D8jOkYhGle5O4GTp4aSipzbvQ9mCC8NlEEYRCqeH/upAQFEAqaCqWKC124MDnpKoVWC91HNlu9ekXn5w0NhhbkfLh8wea96OVlSfMlmVax3csQWbD7jxv+8PZbazV8IvFEwvHZpY6iW3mdnGwppglWX7vHetdwx3dHUPdoM4d/QnqyDTfpKRIsMp3hGZ+VbPpeYPDt6iIyPK9cwea+NtBdi/8w14j+6JejydzAaGcjvRscb5lNvMbHWNeiiMEOJ1IcSeGL8uSeWNhBDXCyE+EEJ88OBzW9IfMWlq1YatUd0gjCgUmMtOHQrMA5s3wdvlSljPle0xrVt5Dbxdrqy+z/D3jPf/IR2Rt+hIW2rkdlRmP/NGNodLOeSxH30LJWUVIx73ej24+6ZlaV1ztKzSIreZ2blh1JVgKeU5aryRlPJBAA8CAP62Rp1jxWhMhQ7FGMsyiGzUEMcLzM3P/gl//8t6bFb68fjOHphMQ58Rs33euxY1Xmpu4OAtOn1RI7ejMnvn4xI9jkwvSXnglR0N8MOMOSrmdqLMPrJvJ1o+3YvttpKs1uHGGhMz2/hYDkFJGeoGcV7U48l0e4h8Tmd7C7b/cgmA4JHHFYOnwg2/VRZ6jcvRhqYjB8OPm83mEe3SUhVv44JPvoBy8wAqSs2YfeFlYxZsWoWRmhs49FKPR0SJJVMDHfmc5nYXmu6+DgBgMglMqioLPz/WtffXt6FXMama24ky29zTjimlJmY2MzstmbZIuxTAGgDVAP5PCLFTSnm+KiMjXVm1YSsWLb1jxOPJrNQmek68vpGh1+xaeyMK7dPCj/c76tMZfpRYGxc8nU488m/fQqmUuP0MC25/++kxCzatwkitDRz5VI+XC5jb+S2ZGujI58wd9pxDj90W99rt7j5UnnI+/Pu3q5rbzOwgZrb6Mu0OsQnAJpXGkhNycSfwU5v3wTb/ApTGqPNKVeTJby5HG1ZeuRid7S0QJkt4VTj0vb0Pjb4xRa2+w9te2ogpBW58pc6KhZPMOLd2bIItF8JoLPpiknqY29FyMbPVtK++DX4lWMHY7HBj5uX3orndBZgt4VXhkGOtHZjxhVnA/u1xr8fM1h4zewjLIVSWazuBYx2KkQlFUcIrBFZbVXi1d/zi21A7Y074eU1HDqLjhXtHvZ4aNcOeTif2vvk0ygd6cMUCG8YVC1xS58NNKawseDqd+N9f/BACAles/H3SYZgLYTQWfTGJsiXXMlttfkWiuHoKAMBqq8Tc6+5B25oVsC9ejrl1E6KeW3/nFbDVLQT++mTc6+kls0PXSTW3mdm5hZNgSmjVhq04benqlF7T0nAovMoLIFzXazabU35/c1EJjj1yS/hrn9eJfntN2u12YrWEiVxRsJcGN8PNqDSltLKw7aWN6D68AxVFppTCMBfCKNt9MYkou/bVt6FpcJUXAJocbgQOt8JiTq2XtJQBWG2VCXM71ipwItnK7NB1Us1tZnZu4SSY4gqVQaTaDUJRlPAqL4BwXW86dWFzr42uXUtUR5yMWDt6D+7Ygvqj3dhZr+B3f+8NP1eYzJjkHT3YQqsS44tTr02LDCP2bCQiLfgVGV7lBYC2NStQXD0Fve2No7xySEBRAAQnzYlyO7Q4kqxsZDaQfm4zs3MLJ8EUU7JlELEOs3A52lBknxL+OrQq4PM6AQTLIEKPx2M2m+HzOkdcO90VYCD+jt5MPxWrVZvGozCJKNti1UA3O9wotQ+d8FZQVIyGh2+FzxvsP2u1VYYfj6fr2CGYLBbVDjcCspfZgDq5zcw2Pk6CKaZkyyBibWRYeeVizIxYCQitCoTCMVY92HATp85Ej70mo1Xf4bKxo1fN2jT2bCSibItVAz3z8nsx97o7w19/afDfQ5Pl0ApxIo4j+1Bhn6T7zAbU2wfCzDY+ToJphHTLIJIRa+XY73GgbeMd6I/oDhF6bjrXi/XabO3oVbM2jT0biUhvYq0cKx4XWp9cBVNEdwiHqwv2qXOGv3wErTMbUG8fCDPb+DgJpigdXd14fl83zrxGnW4Qw6XSAkfN62VrR69atWlGb7lDRLkpme4Z/QM+fG/9fnzxiv8Y9blaZzagzj4QZnZu4CQ4TyRzShAQLIP4XIxDMVKR7Cf9sZStHb1q1aZp2XKHmzuI9CfZzFZDpr2S//5xA6pO+LyqY8pmFwY19oFo3SaNua0OToLzRDKnBD21eR9K552f9qEYmTZBV6uJeix6bgmjdcsdbu4g0p9kMlsNmUy2Q691dXbBUl4DYQqOl5mdfcxtdXASTAAiu0Gcl/Y1Mm2CrkYTdSPSMuy5uYMov2Uy2Q69ds+rGzHh3OvDjzOzs4u5rR5OggnAyG4Q2VyVjXf90FHJw3tMUvZwcwdRbsh2+USs6zc53HA9sArjjj8t4+tT8pjb6uEkmMJlEJHdIFJdlb37pmVwOdqwa230X0RzUQlidZaMdf1kj0omdai1uYO1aUTaS3VFNzSpbXK40bZmRfjxgqLicIu00a4fONyK1md+huJp8zIcPSWLua0uToLznKIog90g4pdB7H1oOZS+HgDB4y9DJ/6ENrp5vR64HG2o/uZPYK2qBRA8N8hSUBg8OrOIf8z0SK3NHaxNI9KXLetWY6Av2PXA5x06Djm00S00+Z209OcY71dQUDUZgEBxoQUND9+a0nspA/2wTT1J1fFTfMxtdXF2kifi7f7t6+3Fom8nDj2lrweTr/wdAKDfUY/aGcFekJGHX+xaeyOEyQJhKQAASP+AmsOnLFBjcwdr04iyI5OODQN9vZh61W8BAL3tjZhbNwFA5OEXvwkfjexrbYSwFKSd2VJKmAZzn7KPua0uToLzRKyasKc278P7pvmqHYohTCb4HA0AABnwI2CxwOd1wmafldTrs3FUMsWnZns31qYRqUvtNmjxmEwmDDgaIAMKYDHD53Vh77rlSU22hb8PAV8/M3sMMbfVxUlwnkq2G8Teh5ZjwONEb1s9gODktunIQZjN5hHPLbJPCf97aMW4316T9Ea6bByVnAnWTCXGhvFE+rNl3Wr0eVxwtwZXCmVAwe7DrbCYRczn26onAwiuGM+rmwCTvRyHHrstqfey+9vROZ6ZbSTM7WicBBtUpjuBV23YikUJDsUIHXjR52iDqbgMlnHB22mhWt9+R33aY4+8fqzH9YI1U4npoWE8kVFku3tDqHyi2+GGubgc1sHMDtX69rY3wqTC9SN1uLpQNWV2BldVFzN7dMztaJwEG1QmvR2ffmf/qIdihFZvV165GN4+P6wFiW+NmYtKgpvgBvm8TvTba+JOatU+Pllto9VMccVBHw3jiYwi24dfhCbSMy+/F219ZhQXWhM+v6CoOGoTnM/rgsleHrcMItZE/bp1W3H6VT/NYNTqYWYnh7kdjZPgPNPR1Y0/7/WkdCjG8AkuEJzkTp0RrPU99NDNwTZoEV0gbPZZup/oJpKoZsrT6cR9t34bNabOvP30DGjfMJ6IYhs+wQWCk9wFddUAghPvcQBQNPR9e3V1SivSjk4vZPlkFUarDmZ2cpjb0TgJzjOjlUHEEuvwikMP3az6JDfbB3Qka7SaqXeffRjorMePLizF6refzttaKiLSp1h9fveuW67qZrvXdhzBSy9uwfN/GVkPzMwmo+AkOI+EDsVIVAahJbWOTc70tleimqnTLlyCHa8+gaVzrZhdEcA5k7x5v7JARPnn/c+cGPArmHXd2hHfY2aTUXASnCeS7QYxnBYb2FoaDkFRlPDXLkcbVl65OOnVhUw3RySqmerv7UFpwIuLj7NiaoUJF9f14wdcWSAincikv3AqPLIIQgQ7TjCzyag4CTaoVIMunTIIQJsNbIqioNA+Lfy11VaFmdeuSWp1QY0m4PFqpjydTqz53vn4zvEm1I0zobRAYEaF5MoCEY1qrCanY9FfuKm9E+bx0wFsB8DMJuPiJNigUgm6ZLpBpEsvdbwh2WwCvu2ljSiWPXh05wBe/MQHkwB8AcDR04MJ7r8yUIkorrE6/GI0arRqe+XDI6idvwR49tmMx8PMJi1xEpzj0ukGkQq16njVkO0m4Ad3bIHbX4BvzhW49tSh1Zv//khBy8lnZXx9IqJsU6NV20cNbsz9WnIngSbCzCatcRKc49Itg9BCqP7Y5WiD1VYVftxcVJLU67PdBPyGex7DAysux8st9Xj5xWFj9+Vnj0Uiyi9SSrhlMYQQzGwyPE6Cc1g2yyCyIfKAjliry6MZrQm4Gs3S2WORiPLZoSYHiibOAZD9zAYy7xzBzKZEOAnOUdkug8imdDtSjBZ2PFKTiCgzr+ysx7SFV0U9lq3MBpjblF2cBOcoI5VBDJeNDXWp7kDmEZtERCN93NyDBRdMjXosW5ugU8ltZjalg5PgHDSWZRBa9BFOR6o7kLn6QES5KJNWbVJKdCO5el81pJLbzGxKByfBOWasyyC0aIOWqlR3IKvRt5KISI8yadW270gLSqaepOJo4kslt5nZlC6T1gMgda3asBWLltyq9TB0JdEO5ETPD64+xH8eEVE+eXlHPWYsPGNM3iuV3GZmU7q4EpxDntq8z1DdIMZKMjuQQ7Ldt5KIyKg+6/BhoX3CmLxXsrnNzKZMcBKcIzq6uvHcx15DdoPItlRa5GS7byURkREFAgF0i9Ixe79kc5uZTZngJDhHjNYNQs3jjfV2VLKaUlk1JiLKFjWON1bzWh992oSKugUpve9YYGZTJjKaBAshfg3gIgADAD4DcJWUslONgVHykukGoebxxno6KlltbKxOuY65bQxqHG+s5rVe3tmE6V/7dsrvnW3MbMpEphvjXgNwspRyPoBPAPwo8yFRKkLdII77AssgiCgpzG1KWUOXgrJxVaM/kchAMpoESylflVL6B798D8CUzIdEqWA3CCJKBXObUuXzK+gx66v3O5Ea1GyRdjWAl+J9UwhxvRDiAyHEBw8+t0XFt81fT7+zHyUnn8duEHnO0+nEupXXwNvl0nooZDxxczsqs595Y4yHRXry/v4GVM1ZpPUwcgYzWz9GnQQLIV4XQuyJ8euSiOesBOAHsD7edaSUD0opF0kpF11/yZfUGX0eC5VBHP8P52s9FNJY5ElJRIA6uR2V2d84e6yGTjr0+kdNmDH/C1oPI2cws/Vj1I1xUspzEn1fCHElgMUAzpZSSpXGRaMYrRvEcGoeb2yUo5LzAU9KoliY28aXyfHGal+rpUdgcinzXQ3MbH3JtDvEBQBWADhLStmjzpBoNOmUQajZuszobdBySfRJSd3sjUmjYm4bQybHG6t5rb5+H3ot5aqNJd8xs/Ul05rgtQDKALwmhNgphPijCmOiBFgGQSGhFYVlpw6dlHRg8ybWmdFomNuUtC17jsJ+4j9oPYycwMzWn0y7Q8yWUk6VUi4c/PU9tQZGsbEbBIUkOimJKB7mNqXijb3NmHHyaVoPIycws/WHJ8YZSC51g8jlU+fGCk9KIqJs6+i3oq6omJmtAma2/nASbBChMoizrjVOGUSi0MzlU+fGCk9KIiI1DT9eORAIoMPdg9e2f8rMVgEzW384CTaIVLtB6AFDk4jIOIYfr9z6yQ6UWqvQ/vo6DUdFlD1qHpZBWZJLZRBERGQMrobPUDrlRK2HQZQ1nATrHLtBEBGRFvyKApPFqvUwiLKGk2CdYzcIIiIaawO9Xohi9gem3MaaYB3L5TIInjpHRKRfHYc/RumMheGvmdmUizgJ1ikjdoMYLlFosqUOEZG+RB6v3OHqQsG4CRBCMLMpZ3ESrFN3PL4Vi5YYqxvEcAxNIiLjiDxe+dp1W/H5q36q4WiIso81wTr09Dv7UTw3N8sgiIhI31qdbohxU7QeBlHWcRKsM+wGQUREWnrtwyOYPP9MrYdBlHWcBOsMu0EQEZGWPjjSicl1x2k9DKKs4yRYR3K5GwQRERmDWxbDZOL0gHIf/5TrBMsgiIhIa/UtThRUz9B6GERjgpNgnWAZBBERae3VHUcxZeFZWg+DaExwEqwDLIMgIiI92NXkRU3tDK2HQTQmOAnWGMsg8pun04l1K6+Bt8ul9VCIKM9JKeGRRRBCaD0U3WJm5xZOgjUWPBSDZRD5attLG2Fp3Y33X3xC66EQUZ77tLEdxZNP0HoYusbMzi2cBGuIh2LkN0+nEwc2b8JvLq3Fgc2buLJARJp6+cN6TFvI/sDxMLNzDyfBGmEZBG17aSMumgPMrinGRXPAlQUi0tSBtl6MnzBZ62HoFjM793ASrBF2g8h9iWrHQisKy04N3gVYdmoFVxaISDOBQABelGg9DE0xs/MPJ8EaYDeI/JCodiy0ojC+1Aog+E+uLBCRVvYeboFt+jyth6EpZnb+sWg9gHzjdHfjz3vcOOs6lkHkstCqwX2X1uL7L2zC6f+4FLaKyvD3D+7Ygh1tfdi4qzHqdbaWLfjashvHerhElOde2dmIujMu1XoYmmFm5ydOgsfYqg1bsWjpHVoPg7IsunasG++/+ERUUN5wz2Majo6IKNph5wAWVtm1HoZmmNn5ieUQY4jdIPJDrNqxfW8/jftXfJf1Y0SkO4oSQLfJpvUwNBOv3rel/hB7Auc4ToLHSEdXN55jNwjdU6MReqzasfNqu+E9/CHrx4hId7Z/0oCKWadqPYy0ZCuzL5oD/OX+O9kTOMexHGKMrNqwFZ9jGYTuRW6MSLfOa3jtWCAQQHdnJ2ZXF+LA5pG1ZkREWnr1oybUnX+Z1sNISzYyGwjmtqdrO569fk7MGmHKDZwEj4Fn3tnHbhAGMNrGiGQNrx178/H7cVzzJtx0hh1r33FkFNZERGo75gFOMeDPp2xlNjCU2/FqhCk3sBwiy5zubjy7h2UQRpCNRujsLUlEejbg86PXUqb1MNKSrcMrmNv5g5PgLLt9/VactvQ2rYdBo8hW6LG3JBHp2Xsf16PyuNO0HkbKsjlRZW7nD5ZDZBHLIIwjUehlcguMvSWJSM9e392Muouv1noYKctWZgPM7XzCSXCWON3d2LTHw0MxDCJbocfekkSkZ+19JkwtKdV6GCnL5kSVuZ0/OAnOEh6KYSwMPSLKNz19A+grGKf1MNLCzCY1sCY4C555Zx8PxSAiIl17Z/dRVJ/4Ra2HQaQZToJV1tHFbhBERKR/b3/cghknfU7rYRBpJqNJsBDiZ0KIXUKInUKIV4UQk9UamFGt2sBuEESkX8xtCnEOWGEtLNR6GESayXQl+NdSyvlSyoUAXgCQ10Wwz767n90giEjvmNsEd3cv/MV2rYdBpKmMNsZJKd0RX5YCkJkNx7ic7m48u9vNbhBpuPumZfB6PSMet6Kgm2QAAAp5SURBVNnK8KO1j2swIqLcxdwmAHjro6OoPvmstF7LzKZckXF3CCHEXQC+C6ALwFcTPO96ANcDwAMrluD6S76U6Vvryqr172HR0tVaD8OQvF4PZl67ZsTjhx66WYPREOW+ZHI7KrNvvwbXX7hg7AZIWffu/jbM/E56v6fMbMoVo5ZDCCFeF0LsifHrEgCQUq6UUk4FsB7ATfGuI6V8UEq5SEq5KNcmwM++ux/FJ5/PMggi0gU1cjsqs79x9lgOn8ZAp1IIi8Wq9TCINDXqSrCU8pwkr7UewIsA8mo5NHQoxpnXsgyCiPSBuU2JdHR1Q5ZN1HoYRJrLtDvEnIgvLwGwP7PhGM+qDVvxuSW3aj0MIqKkMLfp9R2HMXHeGVoPg0hzmdYE/1IIcTyAAICjAL6X+ZCM49l39/NQDCIymrzObQK2fubE8f9wktbDINJcpt0hvqHWQIyGZRDqsdnKYm6osNnKNBgNUW7L59ymIHegCCazOe3XM7MpV2TcHSJf3bFhKz63hO011cCWOkREY+NYexdMVdMyugYzm3IFj01Ow7Pv7kcRyyCIiMhgXtt5BLULztR6GES6wElwikKHYhz/DyyDICIiY/nwaBcmTZ+t9TCIdIGT4BTdsWErFi29TethEBERpcwjiyGE0HoYRLrASXAKWAZBRERGdaS5AwU1s7QeBpFucBKcJJZBEBGRkb3y4VFMXch6YKIQToKTtGr9e1jEQzGIiMig9hzrRvXkzDpDEOUSToKT8Oy7+1F88vkoLR+n9VCIiIhSJqWEFyWsByaKwEnwKFgGQURERrf/aCtKppyo9TCIdIWT4FGsYjcIIiIyuFd2NGD6gi9rPQwiXeEkOIFNW/ajmN0giIjI4D7rGEBlzSSth0GkK5wEx+F0d+OZXSyDICIiYwsEAvCgWOthEOkOJ8Fx3LFhK7tBEBGR4e369Bgq6hZoPQwi3eEkOIbwoRjsBkFERAb38keNmL7gS1oPg0h3OAkeht0giIgol9R3KigbN17rYRDpDifBw7AMgoiIcoXfr6DHVKb1MIh0iZPgCCyDICKiXLJtfz3GzT5V62EQ6RInwYNYBkFERLnmtd3HULfgi1oPg0iXOAkedAcPxSAiohzT7BUosbEcgigWToIRUQbBQzGIiChH9A/40Gsp13oYRLqV95NglkEQEVEu+tveethP+LzWwyDSrbyfBLMbBBER5aI39jRjxjxOgoniyetJMLtBEBFRrnL0mVFYxOOSieLJ20kwyyCIiChXdff2Y6CwSuthEOla3k6CWQZBRES56q+7jqJ6LlujESWSl5PgTVtYBkFERLlr875WTD/xFK2HQaRreTcJdrq78cwulkEQEVHucvqssBYUaj0MIl3Lu0kwyyCIiCiXdXp6oJTWaD0MIt3Lq0kwyyCIiCjXvfnREUycd4bWwyDSvbyZBLMMgoiI8sHfPnFg6pyTtR4Gke7lzSSYZRBERJQPupRCmC0WrYdBpHt5MQl+9t39KDzpXJZBEBFRTmtzeYCKWq2HQWQIOT8JDh2KccIXL9B6KERERFn1+o4jmDT/TK2HQWQIOT8JZhkEERHli22HXKidebzWwyAyBFUmwUKI5UIIKYSwq3E9tbAbBBFRbHrNbcqMWxbDZMr59S0iVWT8N0UIMRXAeQDqMx+OetgNgogoNr3mNmWmsc0Fi3261sMgMgw1Pi7+FsAKAFKFa6mGZRBERHHpMrcpM698eAS1rAcmSlpGk2AhxCUAmqSUH6k0HlWwGwQRUWx6zW3K3EcNHkycNlPrYRAZxqiTYCHE60KIPTF+XQLgxwDuSOaNhBDXCyE+EEJ88OBzWzIdd1wudw+7QRBRXlMjt6My+5k3sj9oyoiUEh4UQwih9VCIDENImd7dMCHEPABvAOgZfGgKgGMATpdStiR88Ucbs3YL7j+fegc159yAsorKbL0FEeWxS0+ZYthZRtq5/ckrEr2d2R8gpa3N2YW1H/TjlHMu1XooRLoyu7oM86ZUxMzttCfBIy4kxBEAi6SUDlUuqBIhxPVSyge1HsdojDBOjlE9RhinEcYIGGeceqTH3DbK76cRxmmEMQLGGCfHqB49jTMf+qhcr/UAkmSEcXKM6jHCOI0wRsA446TkGOX30wjjNMIYAWOMk2NUj27Gqdrh4lLKGWpdi4iIso+5TUT5LB9WgomIiIiIouTDJFgXdSdJMMI4OUb1GGGcRhgjYJxxUnKM8vtphHEaYYyAMcbJMapHN+NUbWMcEREREZFR5MNKMBERERFRlLyYBAshfiaE2CWE2CmEeFUIMVnrMQ0nhPi1EGL/4Dg3CSF0edydEOJbQoi9QoiAEGKR1uOJJIS4QAhxQAjxqRDiP7QeTyxCiD8JIdqEEHu0Hks8QoipQoi3hBAfD/5e/1DrMQ0nhCgSQrwvhPhocIx3aj0mUo8RMhswRm4zszPDzFaHXjM7L8ohhBDlUkr34L//AMBJUsrvaTysKEKI8wC8KaX0CyF+BQBSyn/XeFgjCCFOBBAA8ACAf5VSfqDxkAAAQggzgE8AnAugEcA2AMuklB9rOrBhhBBnAvACeFRKebLW44lFCDEJwCQp5YdCiDIA2wH8s57+X4rgsVilUkqvEMIK4F0AP5RSvqfx0EgFRshswBi5zczODDNbHXrN7LxYCQ6F6aBSALqb+UspX5VS+ge/fA/Bk5x0R0q5T0p5QOtxxHA6gE+llIeklAMAngBwicZjGkFKuRmAU+txJCKlbJZSfjj47x4A+wDUajuqaDLIO/ildfCX7v5eU3qMkNmAMXKbmZ0ZZrY69JrZeTEJBgAhxF1CiAYAlwG4Q+vxjOJqAC9pPQiDqQXQEPF1I3QWAkYkhJgB4BQAW7UdyUhCCLMQYieANgCvSSl1N0ZKn8EyG2Bup4qZnQXM7NTkzCRYCPG6EGJPjF+XAICUcqWUciqA9QBu0uMYB5+zEoB/cJyaSGaclPuEEDYAzwC4ZdjKnC5IKRUp5UIEV99OF0Lo8lYlxWaEzE5mnIPP0TS3mdkEMLPTodqJcVqTUp6T5FPXA3gRwOosDiem0cYohLgSwGIAZ0sNi7VT+H+pJ00ApkZ8PWXwMUrDYM3WMwDWSymf1Xo8iUgpO4UQbwG4AIBuN69QNCNkNmCM3GZmEzM7PTmzEpyIEGJOxJeXANiv1VjiEUJcAGAFgIullD1aj8eAtgGYI4SoE0IUAFgK4HmNx2RIgxsY/hvAPinlvVqPJxYhRHVoJ74QohjBzTW6+3tN6TFCZgPM7Qwxs1XCzE5fvnSHeAbA8QjukD0K4HtSSl194hRCfAqgEEDH4EPv6XQ39KUA1gCoBtAJYKeU8nxtRxUkhPhHAL8DYAbwJynlXRoPaQQhxOMAvgLADqAVwGop5X9rOqhhhBBfBvAOgN0I/p0BgB9LKV/UblTRhBDzAfwPgr/XJgBPSil/qu2oSC1GyGzAGLnNzM4MM1sdes3svJgEExERERFFyotyCCIiIiKiSJwEExEREVHe4SSYiIiIiPIOJ8FERERElHc4CSYiIiKivMNJMBERERHlHU6CiYiIiCjvcBJMRERERHnn/wNYYw25XvR18AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "40c30e638b6defe125180b9832a675e2",
          "grade": false,
          "grade_id": "cell-b1bde9222e35b3fc",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "tnli65o0_l1Q"
      },
      "source": [
        "1) Why does the Perceptron (`model1`) only achieve about 50% accuracy? \n",
        "\n",
        "2) What is the architectural property of the Multi-Layer Perceptron that allows it to more accurately learn the relationship between X and y? \n",
        "- Hint: recall that each layer represents a vector space and they usually have a different number of dimensions, $\\mathbb{R}^N$.\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e01b50ff508342b905c5a4cdbd7d2dc4",
          "grade": true,
          "grade_id": "cell-302694c508c8da0e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ZkRShqJS_l1R"
      },
      "source": [
        "The perceptron didn't have any hidden layers, so the best model is can make is a logistic regression along linear classes. From the graph, you can see it attempts to draw a line through the data to determine the classes. However, the classes in this data would be better separated by 2 lines, not 1. \n",
        "\n",
        "The multi-layer perceptron predicts better because hidden layers can model non-linear data. It can do more complex calculations and gather nuance on predicting. Each layer has a activation function that can manipulate the vector space to help the model's ability to separate the classes with a hyperplane. With training, it can continually forward and back propagate to improve the weights and vector space until accuracy and minimal loss are achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBsWqrAe_l1R"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "- Implement a Multilayer Perceptron architecture of your choosing using the Keras library. \n",
        "- Train your model and report its baseline accuracy. \n",
        "- Then `hyper-parameters tune two parameters each with no more than 2 values each`\n",
        "    - Due to limited computational resources on CodeGrade `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE`\n",
        "- Report your optimized model's accuracy\n",
        "- Use the Heart Disease Dataset provided (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network.\n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyper-parameters tune your model. \n",
        "    - **Use `n_jobs` = 1**\n",
        "- When hyper-parameters tuning, show you work by adding code cells for each new experiment.\n",
        "- Report the accuracy for each combination of hyper-parameters as you test them so that we can easily see which resulted in the highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "90YydAJF_l1S",
        "outputId": "9217c09b-29bf-4413-b08c-c003ba56a570"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load data\n",
        "data_path = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>132</td>\n",
              "      <td>353</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>132</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>128</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>115</td>\n",
              "      <td>564</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>118</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "180   55    1   0       132   353    0  ...      1      1.2      1   1     3       0\n",
              "139   64    1   0       128   263    0  ...      1      0.2      1   1     3       1\n",
              "85    67    0   2       115   564    0  ...      0      1.6      1   0     3       1\n",
              "62    52    1   3       118   186    0  ...      0      0.0      1   0     1       1\n",
              "10    54    1   0       140   239    0  ...      0      1.2      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "22de1dc5d17d7a0bc674d082c33e8b65",
          "grade": false,
          "grade_id": "cell-85dc40f19f5a1d6b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3UjCimWU_l1a"
      },
      "source": [
        "# Create an input matrix named 'X' store it in a 2D numpy array\n",
        "\n",
        "# Create an output vector for the labels named 'Y', store it in 1D numpy array\n",
        "\n",
        "X = np.array(df.drop(columns='target'))\n",
        "Y = np.array(df['target'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "825d4f808810a2a8d6301d7453afe478",
          "grade": true,
          "grade_id": "cell-c17c686c974edc2e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "uI175v50_l1c"
      },
      "source": [
        "# Visible Testing\n",
        "assert X.shape[0] == 303, \"Did you drop/lose some rows in X? Did you properly load and split the data?\"\n",
        "assert X.shape[1] == 13, \"Did you drop/lose some columns in X? Did you properly load and split the data?\"\n",
        "assert len(Y)== 303, \"Did you drop/lose some rows in Y? Did you properly load and split the data?\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYqfBhId_l1c"
      },
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "475835631ff6a34028443dbf604bd922",
          "grade": false,
          "grade_id": "cell-cfc5517cd0b6fa64",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "SUsrKczv_l1d"
      },
      "source": [
        "# Create a function named 'create_model' that returns a complied keras model -  required for KerasClassifier\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential([\n",
        "      Dense(128, input_dim=13, activation='relu'),\n",
        "      Dense(64, activation='relu'),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dense(1, activation='sigmoid')                \n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlGfZ39v55If",
        "outputId": "84427d97-9f13-4414-e632-b2d507d3cefd"
      },
      "source": [
        "#Baseline model\n",
        "\n",
        "baseline = create_model()\n",
        "baseline.fit(X, Y,\n",
        "             epochs=10,\n",
        "             batch_size=32,\n",
        "             validation_split=.2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 28ms/step - loss: 2.1352 - accuracy: 0.4982 - val_loss: 1.0788 - val_accuracy: 0.5574\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.1034 - accuracy: 0.5202 - val_loss: 0.7777 - val_accuracy: 0.6721\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.8610 - accuracy: 0.5796 - val_loss: 0.5119 - val_accuracy: 0.7213\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.8291 - accuracy: 0.5003 - val_loss: 0.5509 - val_accuracy: 0.7049\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.7111 - accuracy: 0.5696 - val_loss: 0.5034 - val_accuracy: 0.7705\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6440 - accuracy: 0.6590 - val_loss: 0.4448 - val_accuracy: 0.7869\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5934 - accuracy: 0.6732 - val_loss: 0.4289 - val_accuracy: 0.8033\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5770 - accuracy: 0.6494 - val_loss: 0.4756 - val_accuracy: 0.8033\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5497 - accuracy: 0.7208 - val_loss: 0.4424 - val_accuracy: 0.7705\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5390 - accuracy: 0.7308 - val_loss: 0.4191 - val_accuracy: 0.8197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd3e8ec2d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipLWVOe867GC"
      },
      "source": [
        "##The baseline accuracy to beat is .73"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7b906697afb0a3b52cd19e9548eae6a7",
          "grade": true,
          "grade_id": "cell-fac25126eaf1eee4",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Ov-ELyLJ_l1g"
      },
      "source": [
        "# Visible Testing\n",
        "assert create_model().__module__ == 'tensorflow.python.keras.engine.sequential', \"create_model should return a keras model that was created using the Sequential class.\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0412c74b7803790452d4914d99995dd2",
          "grade": false,
          "grade_id": "cell-fbc3d0a07230078c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "iVjezMWk_l1h"
      },
      "source": [
        "# Pass 'create_model' into KerasClassifier, store KerasClassifier to a variable named 'model'\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0442c29a94065e922c5ae929976a52ab",
          "grade": true,
          "grade_id": "cell-464e7506993775f2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5os_FGyX_l1i"
      },
      "source": [
        "# Visible Testing\n",
        "assert model.__module__ == 'tensorflow.python.keras.wrappers.scikit_learn', \"model should be a instance of KerasClassifier.\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f88603ef37a4d3d2ef8699a41ac9a0b2",
          "grade": false,
          "grade_id": "cell-985c0425f3b1304d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ysTp9nmO_l1j"
      },
      "source": [
        "# Define the grid search parameters inside a dictionary named 'param_grid' \n",
        "# Use 2 hyper-parameters with 2 possible values for each \n",
        "\n",
        "param_grid = {'batch_size': [32, 64],\n",
        "              'epochs': [5, 50]}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a551fd8278b30c1318c036f6ad43b503",
          "grade": true,
          "grade_id": "cell-c765b5db5489d7a2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sU74UUHT_l1p"
      },
      "source": [
        "assert len(param_grid.keys()) == 2, \"Did you create a param dict with 2 hyper-parameters as keys?\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ea6312f4bc1f42809196b696037dd52",
          "grade": false,
          "grade_id": "cell-7cfb4315eab5031c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWASqnHi_l1q",
        "outputId": "0fb929c6-454d-4493-d845-074cbde5888f"
      },
      "source": [
        "# Create Grid Search object and name it 'gs'\n",
        "# Run Grid Search \n",
        "\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = gs.fit(X, Y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 18.7375 - accuracy: 0.5877\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2.4259 - accuracy: 0.4786\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.2962 - accuracy: 0.5754\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9602 - accuracy: 0.5398\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.7041\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.6066\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 3.6109 - accuracy: 0.5608\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.2551 - accuracy: 0.6499\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8202 - accuracy: 0.6464\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.2996 - accuracy: 0.5460\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7670 - accuracy: 0.6326\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5726 - accuracy: 0.7541\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 16.4942 - accuracy: 0.4620\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 3.9818 - accuracy: 0.6089\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.9699 - accuracy: 0.5649\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.0029 - accuracy: 0.6425\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.8459 - accuracy: 0.6423\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7287 - accuracy: 0.6557\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 5.3876 - accuracy: 0.4410\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.3185 - accuracy: 0.4332\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7813 - accuracy: 0.5804\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5633 - accuracy: 0.7332\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5145 - accuracy: 0.7432\n",
            "WARNING:tensorflow:5 out of the last 27 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e1ec8c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6452 - accuracy: 0.6667\n",
            "Epoch 1/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 3.1528 - accuracy: 0.4475\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.2899 - accuracy: 0.5502\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7046 - accuracy: 0.6217\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 0.6609\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.6032\n",
            "WARNING:tensorflow:6 out of the last 29 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e0d0fcb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7648 - accuracy: 0.4833\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2.6525 - accuracy: 0.5614\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8154 - accuracy: 0.5551\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7294 - accuracy: 0.6291\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6447 - accuracy: 0.6515\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6660 - accuracy: 0.6125\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5818 - accuracy: 0.6635\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7325\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5656 - accuracy: 0.7159\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6067 - accuracy: 0.6889\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7305\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.7154\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5290 - accuracy: 0.7386\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.6864\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5441 - accuracy: 0.7250\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5115 - accuracy: 0.7783\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5589 - accuracy: 0.7231\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7078\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.8118\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7642\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4859 - accuracy: 0.7738\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5129 - accuracy: 0.7568\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5430 - accuracy: 0.7556\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5378 - accuracy: 0.7524\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.6940\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4751 - accuracy: 0.7930\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7143\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5551 - accuracy: 0.6907\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5775 - accuracy: 0.6991\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6173 - accuracy: 0.6784\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5446 - accuracy: 0.7475\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4835 - accuracy: 0.7471\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.7848\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5395 - accuracy: 0.7598\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5492 - accuracy: 0.7237\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4737 - accuracy: 0.7681\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.8195\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4682 - accuracy: 0.7803\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4594 - accuracy: 0.7744\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4895 - accuracy: 0.7465\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.7571\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8189\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5729 - accuracy: 0.7291\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4715 - accuracy: 0.7708\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4970 - accuracy: 0.7722\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8327\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8158\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4715 - accuracy: 0.7906\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.7915\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.8016\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8013\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e0d0f200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4652 - accuracy: 0.8033\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.6039 - accuracy: 0.5283\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9477 - accuracy: 0.5918\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7887 - accuracy: 0.6237\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7290 - accuracy: 0.6033\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6212 - accuracy: 0.6575\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.6366\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6831 - accuracy: 0.6071\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5857 - accuracy: 0.6927\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.6682\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5457 - accuracy: 0.7258\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.6994\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.7311\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5502 - accuracy: 0.6837\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4994 - accuracy: 0.7638\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6326 - accuracy: 0.6199\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5191 - accuracy: 0.7140\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5130 - accuracy: 0.7346\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7360\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.6970\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5231 - accuracy: 0.7637\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.7819\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4749 - accuracy: 0.7841\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.7413\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.7737\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.7072\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8047\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.7487\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4834 - accuracy: 0.7839\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5267 - accuracy: 0.7253\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7775\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4088 - accuracy: 0.8089\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4376 - accuracy: 0.8047\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4504 - accuracy: 0.7726\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.7531\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.7896\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.8172\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5676 - accuracy: 0.7547\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.7644\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7831\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8282\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7934\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.7932\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3941 - accuracy: 0.8446\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5220 - accuracy: 0.7522\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8297\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8434\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.7829\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5587 - accuracy: 0.7331\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4984 - accuracy: 0.7655\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.7938\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dea58710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4442 - accuracy: 0.8525\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 5.1526 - accuracy: 0.5166\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.8873 - accuracy: 0.5215\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8980 - accuracy: 0.5231\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.0829 - accuracy: 0.4914\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7150 - accuracy: 0.6652\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6942\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6263 - accuracy: 0.6754\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5765 - accuracy: 0.6772\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5658 - accuracy: 0.6980\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5294 - accuracy: 0.7406\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5638 - accuracy: 0.6958\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6537 - accuracy: 0.6771\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7338\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.7272\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5375 - accuracy: 0.7386\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5146 - accuracy: 0.7389\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5983 - accuracy: 0.6296\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.7507\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7611\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.7476\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.7480\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7828 - accuracy: 0.6147\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.7066\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6283 - accuracy: 0.6667\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7155\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4910 - accuracy: 0.7632\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4907 - accuracy: 0.7533\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.7708\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6855\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5192 - accuracy: 0.7379\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5657 - accuracy: 0.7284\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5890 - accuracy: 0.6858\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6162 - accuracy: 0.6811\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7395 - accuracy: 0.6321\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5204 - accuracy: 0.7282\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4783 - accuracy: 0.7555\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7840\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7919\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8154\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8141\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.7512\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5282 - accuracy: 0.7306\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7923\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4124 - accuracy: 0.8347\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8165\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4933 - accuracy: 0.7435\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4713 - accuracy: 0.7577\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4149 - accuracy: 0.8103\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3827 - accuracy: 0.8562\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8271\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dcff38c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.4160 - accuracy: 0.7869\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 4.7305 - accuracy: 0.5503\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.4559 - accuracy: 0.6133\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6857 - accuracy: 0.6339\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6647 - accuracy: 0.6380\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7553 - accuracy: 0.6623\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8123 - accuracy: 0.6125\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6983 - accuracy: 0.6550\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5432 - accuracy: 0.7185\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5822 - accuracy: 0.6923\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5702 - accuracy: 0.6889\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.7015\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7311\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5476 - accuracy: 0.7390\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5837 - accuracy: 0.6448\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6235 - accuracy: 0.6760\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7577\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7961\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7745\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.7159\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4863 - accuracy: 0.7584\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4867 - accuracy: 0.7572\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6008 - accuracy: 0.6926\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.8051\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4522 - accuracy: 0.7644\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4684 - accuracy: 0.7698\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4718 - accuracy: 0.7784\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7632\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4033 - accuracy: 0.8151\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.7598\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8142\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5161 - accuracy: 0.7550\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8271\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8302\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8614\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.8523\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3534 - accuracy: 0.8769\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.7764\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4107 - accuracy: 0.8231\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.7729\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5497 - accuracy: 0.7183\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.8205\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3859 - accuracy: 0.8342\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3776 - accuracy: 0.8248\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8686\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.7895\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.7610\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4209 - accuracy: 0.8070\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8289\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8226\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8268\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3fdf698c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4673 - accuracy: 0.7667\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 13.3037 - accuracy: 0.5019\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2.1729 - accuracy: 0.4522\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.6559 - accuracy: 0.5449\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9010 - accuracy: 0.6474\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9507 - accuracy: 0.5403\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.6280\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.6096\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7058 - accuracy: 0.6312\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6735 - accuracy: 0.6653\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.6825\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9562 - accuracy: 0.5805\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9146 - accuracy: 0.5405\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7250 - accuracy: 0.5996\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6574 - accuracy: 0.6822\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5448 - accuracy: 0.6890\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5767 - accuracy: 0.6833\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5422 - accuracy: 0.7083\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5782 - accuracy: 0.6897\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5221 - accuracy: 0.7465\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5216 - accuracy: 0.7432\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.7884\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7315\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4907 - accuracy: 0.7773\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5429 - accuracy: 0.7046\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7571\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6578 - accuracy: 0.6799\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6985 - accuracy: 0.6322\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7965\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.6287\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4836 - accuracy: 0.7433\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.7667\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8254\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.7755\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.7991\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4430 - accuracy: 0.8008\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8015\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.7876\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.7732\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4444 - accuracy: 0.8364\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7629\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5692 - accuracy: 0.7220\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5618 - accuracy: 0.7214\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5748 - accuracy: 0.7184\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7324\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.7973\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4888 - accuracy: 0.7589\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.7952\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4298 - accuracy: 0.7824\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8335\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4100 - accuracy: 0.8234\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e814f5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3827 - accuracy: 0.8000\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.3149 - accuracy: 0.4711\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1202 - accuracy: 0.5428\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8630 - accuracy: 0.6487\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6589 - accuracy: 0.6900\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5905 - accuracy: 0.6967\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dcff3c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.7496 - accuracy: 0.6393\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.9049 - accuracy: 0.4623\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.0066 - accuracy: 0.5408\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2202 - accuracy: 0.5864\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9384 - accuracy: 0.5684\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8664 - accuracy: 0.5781\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e81cf7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.6081 - accuracy: 0.7049\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 15.0765 - accuracy: 0.5734\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.1013 - accuracy: 0.4375\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2633 - accuracy: 0.6623\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.5096 - accuracy: 0.6353\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2211 - accuracy: 0.6281\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e820aa70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.7774 - accuracy: 0.6721\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 7.7428 - accuracy: 0.4793\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.7434 - accuracy: 0.5311\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2627 - accuracy: 0.5516\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.6358 - accuracy: 0.4871\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8317 - accuracy: 0.6621\n",
            "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3de9f2b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.0053 - accuracy: 0.6000\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 14.5429 - accuracy: 0.4404\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.1264 - accuracy: 0.5439\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.5574 - accuracy: 0.5676\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2400 - accuracy: 0.5318\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2222 - accuracy: 0.5784\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e847ecb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.9234 - accuracy: 0.6167\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.3455 - accuracy: 0.5155\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1232 - accuracy: 0.6045\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0095 - accuracy: 0.6130\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6587 - accuracy: 0.6738\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.6823\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6237 - accuracy: 0.6710\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5815 - accuracy: 0.7157\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5955 - accuracy: 0.6963\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6476 - accuracy: 0.6915\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6055 - accuracy: 0.6897\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6396 - accuracy: 0.6718\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5700 - accuracy: 0.7407\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5444 - accuracy: 0.7441\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5160 - accuracy: 0.7480\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5334 - accuracy: 0.7292\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5100 - accuracy: 0.7616\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.7977\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5829 - accuracy: 0.6502\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5181 - accuracy: 0.7152\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.8014\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.7455\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5299 - accuracy: 0.7553\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.5933\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5206 - accuracy: 0.7380\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.7746\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.4914 - accuracy: 0.7648\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4590 - accuracy: 0.8113\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5150 - accuracy: 0.7302\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.7291\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5475 - accuracy: 0.7169\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.7633\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5144 - accuracy: 0.7345\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4305 - accuracy: 0.8162\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4483 - accuracy: 0.7880\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.7806\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 0.7541\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7744\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5425 - accuracy: 0.7040\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.8007\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5665 - accuracy: 0.7184\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4519 - accuracy: 0.7943\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.4454 - accuracy: 0.7636\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4278 - accuracy: 0.8124\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4601 - accuracy: 0.7751\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4843 - accuracy: 0.7467\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8458\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5271 - accuracy: 0.7516\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6143 - accuracy: 0.6642\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6022 - accuracy: 0.7104\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5361 - accuracy: 0.7302\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3e82f8050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6108 - accuracy: 0.7213\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.3699 - accuracy: 0.5136\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5345 - accuracy: 0.5471\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0086 - accuracy: 0.5555\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6768 - accuracy: 0.6972\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7398 - accuracy: 0.6235\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7794 - accuracy: 0.6438\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7523 - accuracy: 0.6559\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.6424\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6070 - accuracy: 0.7038\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6050 - accuracy: 0.6819\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6234 - accuracy: 0.6442\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5532 - accuracy: 0.6980\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.7439\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5455 - accuracy: 0.7238\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5252 - accuracy: 0.7040\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5295 - accuracy: 0.7213\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5255 - accuracy: 0.7159\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5451 - accuracy: 0.7032\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5274 - accuracy: 0.7296\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5395 - accuracy: 0.6960\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5930 - accuracy: 0.6666\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5630 - accuracy: 0.7462\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5204 - accuracy: 0.7215\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5144 - accuracy: 0.7388\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4880 - accuracy: 0.7630\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5969 - accuracy: 0.6780\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5283 - accuracy: 0.7170\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5020 - accuracy: 0.7341\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4945 - accuracy: 0.7449\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4843 - accuracy: 0.7618\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.7623\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5509 - accuracy: 0.6985\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5493 - accuracy: 0.7076\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.7018\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4417 - accuracy: 0.7963\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.7744\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4730 - accuracy: 0.7426\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.7963\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.8035\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.7872\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4695 - accuracy: 0.7377\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4841 - accuracy: 0.7467\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.7930\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4191 - accuracy: 0.7996\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4214 - accuracy: 0.7948\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.7901\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4516 - accuracy: 0.7754\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4576 - accuracy: 0.7575\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4515 - accuracy: 0.7819\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4650 - accuracy: 0.7773\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dc71e3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.4849 - accuracy: 0.7541\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 5.7006 - accuracy: 0.4521\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.5646 - accuracy: 0.5315\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1937 - accuracy: 0.4920\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0125 - accuracy: 0.5736\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8059 - accuracy: 0.5486\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6794 - accuracy: 0.6698\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6884 - accuracy: 0.6323\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6391 - accuracy: 0.6613\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6419 - accuracy: 0.6824\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6486 - accuracy: 0.6797\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6313 - accuracy: 0.6352\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5996 - accuracy: 0.6949\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5749 - accuracy: 0.6753\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5832 - accuracy: 0.6941\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5629 - accuracy: 0.7295\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5181 - accuracy: 0.7355\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5154 - accuracy: 0.7412\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5542 - accuracy: 0.7023\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5978 - accuracy: 0.6915\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5455 - accuracy: 0.6914\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.7231\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5133 - accuracy: 0.7476\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4875 - accuracy: 0.7665\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5125 - accuracy: 0.7836\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.7486\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4992 - accuracy: 0.7851\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4958 - accuracy: 0.7678\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.7765\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4971 - accuracy: 0.7470\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.7798\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4892 - accuracy: 0.7572\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4820 - accuracy: 0.7998\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5025 - accuracy: 0.7730\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4979 - accuracy: 0.7829\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4772 - accuracy: 0.7889\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5032 - accuracy: 0.7656\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.6682\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5084 - accuracy: 0.7423\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4942 - accuracy: 0.7870\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4978 - accuracy: 0.7383\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5158 - accuracy: 0.7470\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5022 - accuracy: 0.7502\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4677 - accuracy: 0.8072\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4766 - accuracy: 0.7598\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4943 - accuracy: 0.7375\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.4713 - accuracy: 0.7613\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4623 - accuracy: 0.7871\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4294 - accuracy: 0.8089\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3923 - accuracy: 0.8433\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4251 - accuracy: 0.8391\n",
            "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dbec1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.4526 - accuracy: 0.7869\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 3.9503 - accuracy: 0.4926\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3839 - accuracy: 0.5463\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9573 - accuracy: 0.5195\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7812 - accuracy: 0.6197\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8354 - accuracy: 0.6368\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7242 - accuracy: 0.6359\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5814 - accuracy: 0.7018\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6461 - accuracy: 0.6132\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5926 - accuracy: 0.6759\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6108 - accuracy: 0.6472\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5707 - accuracy: 0.6735\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5561 - accuracy: 0.7121\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6116 - accuracy: 0.6589\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5666 - accuracy: 0.7029\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5933 - accuracy: 0.6679\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6160 - accuracy: 0.6693\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7702\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5727 - accuracy: 0.7134\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5637 - accuracy: 0.6836\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5709 - accuracy: 0.7165\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5383 - accuracy: 0.7227\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5593 - accuracy: 0.6840\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5699 - accuracy: 0.7118\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5585 - accuracy: 0.7291\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5412 - accuracy: 0.6828\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5299 - accuracy: 0.7263\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5081 - accuracy: 0.7380\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7491\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5083 - accuracy: 0.7594\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5164 - accuracy: 0.7585\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5212 - accuracy: 0.7249\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5045 - accuracy: 0.7447\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5120 - accuracy: 0.7285\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5022 - accuracy: 0.7466\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4897 - accuracy: 0.7670\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5112 - accuracy: 0.7275\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.7738\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4890 - accuracy: 0.7658\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5138 - accuracy: 0.7379\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5020 - accuracy: 0.7313\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4866 - accuracy: 0.7390\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4847 - accuracy: 0.7653\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5002 - accuracy: 0.7772\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5931 - accuracy: 0.7020\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5712 - accuracy: 0.7079\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5968 - accuracy: 0.6752\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5036 - accuracy: 0.7597\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.7681\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4911 - accuracy: 0.7394\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5510 - accuracy: 0.7084\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dfc46050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.5981 - accuracy: 0.6500\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 3.7375 - accuracy: 0.5745\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.4553 - accuracy: 0.5479\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6291 - accuracy: 0.5668\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9502 - accuracy: 0.5966\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7346 - accuracy: 0.6347\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8078 - accuracy: 0.6269\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7492 - accuracy: 0.5796\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6268 - accuracy: 0.6477\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6120 - accuracy: 0.6713\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6711 - accuracy: 0.6641\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6094 - accuracy: 0.6472\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6249 - accuracy: 0.6354\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5735 - accuracy: 0.7104\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5895 - accuracy: 0.6654\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5802 - accuracy: 0.6739\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5759 - accuracy: 0.6614\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7468\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5351 - accuracy: 0.7167\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5432 - accuracy: 0.7158\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5407 - accuracy: 0.6882\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5548 - accuracy: 0.6743\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5528 - accuracy: 0.6954\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5178 - accuracy: 0.7608\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5286 - accuracy: 0.7309\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5219 - accuracy: 0.7399\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5091 - accuracy: 0.7769\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5249 - accuracy: 0.7197\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5279 - accuracy: 0.7409\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5038 - accuracy: 0.7845\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.7360\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5087 - accuracy: 0.7254\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5001 - accuracy: 0.7723\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4833 - accuracy: 0.7682\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5134 - accuracy: 0.7467\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4904 - accuracy: 0.7881\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4754 - accuracy: 0.7602\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4669 - accuracy: 0.7849\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4677 - accuracy: 0.7659\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4765 - accuracy: 0.8082\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4901 - accuracy: 0.7752\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4662 - accuracy: 0.7813\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4635 - accuracy: 0.7924\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4713 - accuracy: 0.8070\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5070 - accuracy: 0.7431\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7323\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4554 - accuracy: 0.8264\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4714 - accuracy: 0.7546\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4565 - accuracy: 0.7950\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4837 - accuracy: 0.7968\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5239 - accuracy: 0.7351\n",
            "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd3dc71e5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.4355 - accuracy: 0.7833\n",
            "Epoch 1/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 7.4825 - accuracy: 0.4482\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.6265 - accuracy: 0.5179\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.0134 - accuracy: 0.5675\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.7294 - accuracy: 0.6192\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7472 - accuracy: 0.6463\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6412 - accuracy: 0.6522\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.6546\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7019\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.6883\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5199 - accuracy: 0.7251\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.7619\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7141\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.6864\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6418 - accuracy: 0.6625\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.6290\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7596\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4961 - accuracy: 0.7746\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5219 - accuracy: 0.7492\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4530 - accuracy: 0.7569\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4788 - accuracy: 0.7682\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4714 - accuracy: 0.7883\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7119\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7359 - accuracy: 0.6435\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8234 - accuracy: 0.6669\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.7353\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4621 - accuracy: 0.7790\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.7389\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4855 - accuracy: 0.7566\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.7782\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.8442\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.7901\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.7987\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4103 - accuracy: 0.8243\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.7971\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4138 - accuracy: 0.7895\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8085\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.7226\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7792\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.6864\n",
            "Epoch 40/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5138 - accuracy: 0.7587\n",
            "Epoch 41/50\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.4582 - accuracy: 0.7848\n",
            "Epoch 42/50\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4990 - accuracy: 0.7598\n",
            "Epoch 43/50\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.5822 - accuracy: 0.7719\n",
            "Epoch 44/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.7199\n",
            "Epoch 45/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8278\n",
            "Epoch 46/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8217\n",
            "Epoch 47/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3862 - accuracy: 0.8129\n",
            "Epoch 48/50\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8208\n",
            "Epoch 49/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3597 - accuracy: 0.8615\n",
            "Epoch 50/50\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3959 - accuracy: 0.8274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF4xrYfd_l1r",
        "outputId": "b5158189-430c-4efb-abf5-513248336a7a"
      },
      "source": [
        "# your grid_result object should be able to run in this code \n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8018579244613647 using {'batch_size': 32, 'epochs': 50}\n",
            "Means: 0.6332786858081818, Stdev: 0.08878535930382364 with: {'batch_size': 32, 'epochs': 5}\n",
            "Means: 0.8018579244613647, Stdev: 0.02838502064625798 with: {'batch_size': 32, 'epochs': 50}\n",
            "Means: 0.646612024307251, Stdev: 0.037876211481441636 with: {'batch_size': 64, 'epochs': 5}\n",
            "Means: 0.7391256809234619, Stdev: 0.050438508297470715 with: {'batch_size': 64, 'epochs': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_LLFu1X_l1s"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}