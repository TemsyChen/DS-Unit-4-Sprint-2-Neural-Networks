{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# *Data Science Unit 4 Sprint 2 Assignment 4*\n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Apply regularization techniques to your model. \n",
    "\n",
    "**Don't forgot to switch to GPU on Colab!**\n",
    "\n",
    "## Objective \n",
    "\n",
    "In lecture, you were exposed to weight shrinkage also known as weight decay or Lp space reguarlization, Max Norn weight constraints, and dropout. \n",
    "\n",
    "In this assignment, you will run 3 experiments in order to perform a deep analysis on the effects that various regularization techniques have model performance and on the learned model weights. By the end of this assignment, these regularization techniques should no longer feel like blackboxes to you, i.e. completely mysterious as to how they work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptJ2b3wk62Ud"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "# native libraries \n",
    "import os\n",
    "from time import time \n",
    "\n",
    "# data analysis libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# deep learning libraries \n",
    "from keras import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers import ReLU\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# regularizers \n",
    "from keras.regularizers import l2, l1\n",
    "from keras.constraints import MaxNorm\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load in and normalize image data set\n",
    "    \"\"\"\n",
    "    \n",
    "    # load in our dataset \n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "    # normalize pixel values between 0 and 1 \n",
    "    max_pixel_value = X_train.max()\n",
    "    X_train, X_test = X_train /max_pixel_value , X_test / max_pixel_value\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is equal to the number of nodes in the output layer\n",
    "N_labels = len(np.unique(y_train))\n",
    "N_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Build Model\n",
    "\n",
    "Let's build out the model that we'll be using all throughout our experiments. \n",
    "\n",
    "Remember that **the whole point of regularization is to prevent overfitting.**\n",
    "\n",
    "\n",
    "![](https://hackernoon.com/hn-images/1*vuZxFMi5fODz2OEcpG-S1g.png)\n",
    "\n",
    "Overfitting happens when are model's are too complex, so in order to see a benefit from the use of regularization techniques we need to build a relatively complex model. \n",
    "\n",
    "Having said that, you might not have the computational resource to be able to train a complex model in a reasonable amount of time. So if this describes you, then you might want to consider using `build_simple_model`. Otherwise, I recommend that you use `build_complex_model`. \n",
    "\n",
    "This notebook will be using  `build_complex_model` to run our experiments. \n",
    "\n",
    "**NOTE:** Whichever function you end up using to build a model, take time to read through the code and make sure you understand what is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complex_model(Lp_reg=None, reg_penality=None, dropout_prob=0.0, maxnorm_wc=None):\n",
    "    \"\"\"\n",
    "    Build and return a regularized 3 hidden layer FCFF model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Lp_reg: None or object\n",
    "        If object, Lp_reg is either l1 or l2 regularization \n",
    "        If None, that means that l1 or l2 regularization will not be used.\n",
    "     \n",
    "    reg_penality: None or float\n",
    "        If float, reg_penality is a value typically between 1.0 and 0.0001\n",
    "        This is the regularization strength for l1 or l2 \n",
    "        \n",
    "        \n",
    "    dropout_prob: float\n",
    "        This is the probability that dropout regularization will exclude a node from a training iteration. \n",
    "        If this value is 0.0, that means that dropout will not be used. \n",
    "        \n",
    "    maxnorm_wc: None or float\n",
    "        If float, maxnorm_wc is the weight constraint that is used for Max Norm regularization\n",
    "        If None, that means that Max Norm regularization will not be used.\n",
    "        \n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    model: complied Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # if reg_type is not None, then pass in the penality strength to whatever form of Lp space regularization this is \n",
    "    if Lp_reg is not None:\n",
    "        Lp_regularizer = Lp_reg(reg_penality)\n",
    "    else:\n",
    "        Lp_regularizer = None\n",
    "                \n",
    "    if maxnorm_wc is not None:\n",
    "        wc = MaxNorm(max_value=maxnorm_wc)\n",
    "    else:\n",
    "        wc = None\n",
    "\n",
    "\n",
    "    # instantiate Sequential class\n",
    "    model = Sequential([\n",
    "\n",
    "    # flatten images \n",
    "    Flatten(input_shape=(28,28)),\n",
    "\n",
    "    # hidden layer 1\n",
    "    Dense(500, kernel_regularizer=Lp_regularizer , kernel_constraint=wc), # remember that Keras refers to weight matrix as a kernel, i.e. weights = kernel\n",
    "    # act func 1\n",
    "    ReLU(negative_slope=0.01),\n",
    "    Dropout(dropout_prob),\n",
    "\n",
    "    # hidden layer 2\n",
    "    Dense(250, kernel_regularizer=Lp_regularizer, kernel_constraint=wc),\n",
    "    # act func 2\n",
    "    ReLU(negative_slope=0.01),\n",
    "    Dropout(dropout_prob),\n",
    "\n",
    "    # hidden layer 3\n",
    "    Dense(64, kernel_regularizer=Lp_regularizer, kernel_constraint=wc),\n",
    "    # act func 3\n",
    "    ReLU(negative_slope=0.01),\n",
    "    Dropout(dropout_prob),\n",
    "\n",
    "    # output layer   \n",
    "    Dense(N_labels, activation=\"softmax\")  \n",
    "\n",
    "    ])\n",
    "    # complie model \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                 optimizer=\"adam\", \n",
    "                 metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, only use `build_simple_model` instead of `build_complex_model` if you're working on a machine with very limited computational resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "# def build_simple_model(Lp_reg=None, reg_penality=None, dropout_prob=0, maxnorm_wc=None):\n",
    "#     \"\"\"\n",
    "#     Build and return a regularized 1 hidden layer FCFF model \n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     Lp_reg: None or object\n",
    "#         If object, Lp_reg is either l1 or l2 regularization \n",
    "#         If None, that means that l1 or l2 regularization will not be used.\n",
    "     \n",
    "#     reg_penality: None or float\n",
    "#         If float, reg_penality is a value typically between 1.0 and 0.0001\n",
    "#         This is the regularization strength for l1 or l2 \n",
    "        \n",
    "        \n",
    "#     dropout_prob: float\n",
    "#         This is the probability that dropout regularization will exclude a node from a training iteration. \n",
    "#         If this value is 0.0, that means that dropout will not be used. \n",
    "        \n",
    "#     maxnorm_wc: None or float\n",
    "#         If float, maxnorm_wc is the weight constraint that is used for Max Norm regularization\n",
    "#         If None, that means that Max Norm regularization will not be used.\n",
    "        \n",
    "        \n",
    "#     Return\n",
    "#     ------\n",
    "#     model: complied Keras model\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if Lp_reg is not None:\n",
    "#         Lp_regularizer = Lp_reg(reg_penality)\n",
    "#     else:\n",
    "#         Lp_regularizer = None\n",
    "\n",
    "#     # instantiate Sequential class\n",
    "#     model = Sequential([\n",
    "\n",
    "#     # flatten images \n",
    "#     Flatten(input_shape=(28,28)),\n",
    "\n",
    "#     # hidden layer 1\n",
    "#     Dense(128,  kernel_regularizer=Lp_regularizer, kernel_constraint=maxnorm_wc), # remember that Keras refers to weight matrix as a kernel, i.e. weights = kernel\n",
    "#     # act func 1\n",
    "#     ReLU(negative_slope=0.01),\n",
    "#     Dropout(p_dropout),\n",
    "\n",
    "#     # output layer   \n",
    "#     Dense(N_labels, activation=\"softmax\")  \n",
    "\n",
    "#     ])\n",
    "#     # complie model \n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "#                  optimizer=\"adam\", \n",
    "#                  metrics=[\"accuracy\"])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be using Sklearn's GridserchCV class, we need to wrap our Keras models in `KerasClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to wrap KerasClassifier around build_model for sklearn's GridsearchCV compatibility \n",
    "model = KerasClassifier(build_fn = build_complex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Experiment 1: Identify the relationship between model performance and L2 penalty strength\n",
    "\n",
    "![](https://www.researchgate.net/publication/334159821/figure/fig1/AS:776025558495234@1562030319993/Ridge-regression-variable-selection.png)\n",
    "\n",
    "We are going to run a gridsearch soley on the l2 regularization penalty value and see the effect this has on model performance. \n",
    "\n",
    "By running a gridseach on only a single hyperparameter (while using the same data and model) we can isolate the effect of that hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    # take note that Lp_reg penalty/strength values are in powers of 10 \n",
    "    \"reg_penality\": [10.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001], \n",
    "    # Since we only want to test l2, provide l2 as the sole option \n",
    "    \"Lp_reg\": [l2],\n",
    "    # default is 1, in order to change it we must provide value here because we can't provide a parameter value for model.fit() directly when using gridsearch\n",
    "    # protip: consider chanign epochs to 1 if the gridsearche run-time are too long for you\n",
    "    \"epochs\": [3] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "start=time()\n",
    "# Create and run Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "end=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the mean accuracy from the CV splits for determining best model score \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "# move l2 penalty values outside of dictionary and into a list\n",
    "param_values = [dic[\"reg_penality\"] for dic in params]\n",
    "\n",
    "# plot accuracy vs l2_reg_penalty\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.grid()\n",
    "\n",
    "# this plot is using the std of the CV splits to plot error bars however those values are so small that they aren't visable\n",
    "plt.errorbar(param_values, means, yerr=stds, ecolor=\"orange\")\n",
    "plt.xscale(\"log\") # use a log scale for ease of reading, recall that l2_reg_penalty were in powers of 10 \n",
    "plt.title(\"L2 Regularization: Model Accuracy vs L2 Penalty Strength\")\n",
    "plt.ylabel(\"Validation Accuracy\", )\n",
    "plt.xlabel(\"L2 Penalty Strength usng a Log Scale\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Write down some observations. What do you notice from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "089b55b5a84d9c96c51fd341d9e6c74f",
     "grade": true,
     "grade_id": "cell-010212fc0915b976",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compare Weights between Best and Worse Model \n",
    "\n",
    "Next, we are going to compare the hidden layer weights between the best and worse performing model while taking note of the respective l2 penalty strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best l2 penalty term \n",
    "best_lr_penalty = grid_result.best_params_[\"reg_penality\"]\n",
    "\n",
    "# get the best trained model \n",
    "best_model = grid_result.best_estimator_.build_fn(Lp_reg=l2, reg_penality=best_lr_penalty)\n",
    "\n",
    "# get the weights from the best trained model \n",
    "best_weights = best_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model using the l2_reg_penalty value at scored the lowest \n",
    "worse_l2_reg_penalty = 10.0\n",
    "\n",
    "worse_model = build_complex_model(Lp_reg=l2, reg_penality=worse_l2_reg_penalty)\n",
    "\n",
    "# fit model \n",
    "worse_model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# get weights from worse performing model \n",
    "worse_weights = worse_model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Understanding how Weights and Biases are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a minute to understand that`.get_weights()` returns a list with 8 elements (if you're using `build_complex_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **weights matrices and bias vectors between each layer** and we have 5 layers. \n",
    "\n",
    "- Input\n",
    "- Hidden 1\n",
    "- Hidden 2\n",
    "- Hidden 3\n",
    "- Output \n",
    "\n",
    "So that means we should have 4 weight matrices, but we see 8. **There are also 4 weight vectors for the biases between each layer.** So that accounts for the 8. \n",
    "\n",
    "\n",
    "#### Index for Weight Matrices \n",
    "If you index for a weight matrix, you can see its shape and that they are indeed matrices. \n",
    "\n",
    "Notice how you can see the dims of the layers that the matrices are sandwiched between?\n",
    "\n",
    "Input layer has 784 dims and hidden layer 1 has 500 nodes/dims. Given this understanding, the numbers you see in the shapes should make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewteen input and 1st hidden layer\n",
    "best_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewteen 1st and 2nd hidden layer\n",
    "best_weights[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewteen 2st and 3nd hidden layer\n",
    "best_weights[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewteen 3rd hidden layer and output layer\n",
    "best_weights[6].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index for the bias vectors\n",
    "\n",
    "The shapes of the bias vectors should exactly match up the dims/nodes of each layer (excluding the input layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hidden layer 1 \n",
    "best_weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hidden layer 2 \n",
    "best_weights[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hidden layer 3\n",
    "best_weights[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output layer\n",
    "best_weights[7].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Back to our Analysis of L2 space regularization (also known as Ridge)\n",
    "\n",
    "Let's do a comparison of the first weight matrix (between the input and 1st hidden layer) for the best and worse performing model as well as with the initial weight values that are randomly sampled from the GlorotUniform distribution.\n",
    "\n",
    "[**Check out the Keras docs for the Dense layer**](https://keras.io/api/layers/core_layers/dense/), you'll see that GlorotUniform is the default weight initializer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "caa9a1024b11f3eaa91fe681fc12e24d",
     "grade": false,
     "grade_id": "cell-7882876b8973bc7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# index for the 1st hidden layer weights in best_weights and save to best_hidden_weights\n",
    "\n",
    "# index for the 1st hidden layer weights in worse_weights and save to worse_hidden_weights\n",
    "\n",
    "# Keras models randomly samples from the GlorotUniform distribution for the initial values of model weights \n",
    "# instantiate GlorotUniform and sample 128 weights and save to initial_weight_values\n",
    "# hint: use shape=(1, 128)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all weights to a dataframe for ease of analysis \n",
    "cols = [\"best_hidden_weights\", \"worse_hidden_weights\", \"initial_weight_values\"]\n",
    "data = [best_hidden_weights, worse_hidden_weights, initial_weight_values]\n",
    "df = pd.DataFrame(data=data).T\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the statistics for each weight column \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions for each weight column \n",
    "df.hist(figsize=(20,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations \n",
    "\n",
    "Take a look at the statistical table and the plots. Then answer the following questions. \n",
    "\n",
    "**How do the hidden layer weights from the best performing model compare to the initial weight values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5eacb66f89b216ae3b4bb3c4b7bd6d38",
     "grade": true,
     "grade_id": "cell-6add7cc400c4c716",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the effect of using a small l2 penalty value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92ac1689b72d727ab7f4d261c5e76daa",
     "grade": true,
     "grade_id": "cell-5b4f11bba2d49639",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the effect of using a large l2 penalty value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3048a6d2805f61d6fb58372c42c3ac54",
     "grade": true,
     "grade_id": "cell-0a30b62e5e119555",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given what you know about L2 regularization, are you surprised by these results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8a2034f67badfe53f601873f7026dc0",
     "grade": true,
     "grade_id": "cell-c04d067161064011",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Experiment 2: Identify the relationship between model performance and Max Norm Weight Constraint\n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-9d0dbf8074761b541ba80543ddfc9f73.webp)\n",
    "\n",
    "Recall from lecture that the **Norm** of a vector is just another word for the **length** of a vector.\n",
    "\n",
    "MaxNorm weight constraint puts a limit on the length of a weight vector.\n",
    "\n",
    "$$ \\text{Max_value_of_norm} >= {\\displaystyle \\left\\|x\\right\\|_{p}=\\left(|x_{1}|^{p}+|x_{2}|^{p}+\\dotsb +|x_{n}|^{p}\\right)^{1/p}.}$$  \n",
    "\n",
    "The effect that Lp regularization and Max Norm Weight Constraint have on the weights should be the same, but they go about it in different ways. \n",
    "\n",
    "Lp regularization (l1/Lasso and l2/Ridge) shrink the value of the weights. Where as Max Norm Weight Constraint puts a limit on how big the weight vector can be which, in effect, keeps the individual weight values small enough to keep the norm below that limit. \n",
    "\n",
    "In this experiment, we are going to run another gridseach but instead of using Lp space regularization as we did in the previous experiment, we are going to use MaxNorm and see what kind of effect that this type of regularization has on model performance and the learned weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already built our model, we just need to update the `hyper_parameters` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    \n",
    "    \"maxnorm_wc\": np.linspace(0.5, 10.0, num=20), \n",
    "    # default is 1, in order to change it we must provide value here because we can't provide a parameter value for model.fit() directly when using gridsearch\n",
    "    # protip: consider changing epochs to 1 if the gridsearche run-time are too long for you    \n",
    "    \"epochs\": [3] \n",
    "}\n",
    "\n",
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "# Create and run Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "end=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the mean accuracy from the CV splits for determining best model score \n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "\n",
    "# move l2 penalty values outside of dictionary and into a list\n",
    "param_values = [dic[\"maxnorm_wc\"] for dic in params]\n",
    "\n",
    "# plot accuracy vs l2_reg_penalty\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.grid()\n",
    "plt.errorbar(param_values, means, yerr=stds, ecolor=\"orange\")\n",
    "plt.title(\"L1 Regularization: Model Accuracy vs L1 Penalty Strength\")\n",
    "plt.ylabel(\"Validation Accuracy\", )\n",
    "plt.xlabel(\"Max Norm for Weight Vector \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfe7bd54a7a14ba7ee63c88d6d1828b4",
     "grade": false,
     "grade_id": "cell-f67372e0b9b30614",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# get the best l2 penalty term from grid and save to best_max_norm_val\n",
    "\n",
    "# get the best trained model from grid and save to best_model\n",
    "\n",
    "# get the weights from the best trained model and save to best_weights\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_norm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that the norm of our weights are indeed below the maximum allowed value \n",
    "np.linalg.norm(best_weights[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67f65bd636e3b3b3bc7d20c02ba6b666",
     "grade": false,
     "grade_id": "cell-e752c1a8c853985d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# train a model using the max_norm_val value that scored the lowest \n",
    "\n",
    "# build a model using build_complex_model and worse_max_norm_val and save it to worse_model\n",
    "\n",
    "# fit model \n",
    "\n",
    "# get weights from worse performing model \n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a38d5d4db707124b31a662fb4743b049",
     "grade": false,
     "grade_id": "cell-5c1aa4543e68487d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all weights to a dataframe for ease of analysis \n",
    "cols = [\"best_hidden_weights\", \"worse_hidden_weights\", \"initial_weight_values\"]\n",
    "data = [best_hidden_weights, worse_hidden_weights, initial_weight_values]\n",
    "df_maxnorm= pd.DataFrame(data=data).T\n",
    "df_maxnorm.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maxnorm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions for each weight column \n",
    "df_maxnorm.hist(figsize=(20,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations \n",
    "\n",
    "Take a look at the statistical table and the plots. Then answer the following questions. \n",
    "\n",
    "**How do the hidden layer weights from the best performing model compare to the initial weight values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f43fe1110cdcea8d1e4b432fe78d4e49",
     "grade": true,
     "grade_id": "cell-40a44d19694941b8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the effect of using the weight constraint value in MaxNorm in the best performing model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1c59c58a5abdbc0b509983821198dba",
     "grade": true,
     "grade_id": "cell-4f9e1e134124e512",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the effect of using the weight constraint value in MaxNorm in the worse performing model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0062b4ddfad487c39633c37f4710b752",
     "grade": true,
     "grade_id": "cell-4c289ce70c34048a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given what you know about MaxNorm regularization, are you surprised by these results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c36931d3532a8cbcb4ea0c956378728",
     "grade": true,
     "grade_id": "cell-77366a912217da5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Experiment 3: Identify the relationship between model performance and Dropout\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png)\n",
    "\n",
    "In the 3rd and final experiment, we will use gridsearch to see how model performance is affect by varying the value of the the dropout probability. \n",
    "\n",
    "Recall from lecture that dropout tends to perform best when used with weight constraint. Since this is the case, we will gridsearch both dropout probability and the weight constraint for MaxNorm. \n",
    "\n",
    "If interested, feel free to read through the original publication on [**Drop Out**](https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf). \n",
    "\n",
    "**Key Take aways:** \n",
    "\n",
    "1. During training, dropout will probabilistically \"turn off\" some neurons in the layer that dropout is implemented in. \n",
    "2. During inference (ie. making predictions on the test set) all neurons are used (i.e. no dropout is applied).\n",
    "3. Dropout works best when used with MaxNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    # for the sake of runtime, let's vary maxnorm_wc between 0.5 and 5.0\n",
    "    \"maxnorm_wc\": np.linspace(0.5, 5, num=10),\n",
    "    # take note that l1_reg_penalty values are in powers of 10 \n",
    "    \"dropout_prob\": np.linspace(0.0, 0.6, num=7), \n",
    "    \"epochs\": [1] # default is 1, in order to change it we must provide value here because we can provide a parameter value for model.fit() directly when using gridsearch\n",
    "}\n",
    "\n",
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time()\n",
    "# Create and run Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "end=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the mean accuracy from the CV splits for determining best model score \n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "\n",
    "# move l2 penalty values outside of dictionary and into a list\n",
    "param_values = [dic[\"dropout_prob\"] for dic in params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 2 indepdent variables this time around (dropout_prob and maxnorm_wc) which affect the validation accuracy, it's best to use a different plot. A heat map will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_prob_list = [  dic[\"dropout_prob\"]  for dic in params]\n",
    "maxnorm_wc_list = [  dic[\"maxnorm_wc\"]  for dic in params]\n",
    "data = [means, dropout_prob_list, maxnorm_wc_list ]\n",
    "\n",
    "cols = [\"val_acc\", \"dropout_prob\", \"maxnorm_wc\"]\n",
    "df_exp3 =pd.DataFrame(data=data).T\n",
    "df_exp3.columns = cols\n",
    "df_exp3.dropout_prob = df_exp3.dropout_prob.round(2)\n",
    "\n",
    "# pivot dataframe in preperation for heat map\n",
    "df_exp3 = df_exp3.pivot(\"maxnorm_wc\", \"dropout_prob\", \"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a heatmap with the val_acc values in each cell\n",
    "f, ax = plt.subplots(figsize=(18, 8))\n",
    "sns.heatmap(df_exp3, annot=True,  linewidths=.5, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "\n",
    "We can see the dropout probabilities in the horizontal axis and the maxnorm weight constraint values in the virtical axis. The values in the cells are the validation accuracy that corresponds to a pair of regularization values.\n",
    "\n",
    "Take a look at the heat map and answer the following questions. Note that depending on which model you used (the simple or complex one) your answers might be different from that of others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What range of dropout probability values tend to produce the highest validation accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f0013d4e07104a03b4d51664a308f53",
     "grade": true,
     "grade_id": "cell-4e0cb7a9240b1531",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What range of maxnorm weight constraints tend to produce the highest validation accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fd88f0bb870a910b925d60b38d17694",
     "grade": true,
     "grade_id": "cell-99539755d7d328f7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When taken together, what pair of dropout probability and maxnorm weight constraints tend to produce the highest validation accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fee1e09ed8f6d354bd7b6e2986c2b811",
     "grade": true,
     "grade_id": "cell-5e19a56b4a2d975d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you think that using dropout was helpful in increasing model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de9c1bcff3c5eb6266cc80632d0956f0",
     "grade": true,
     "grade_id": "cell-d2a2f7b284c801dc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "### Experiment 4: Train, Save, and Load a Keras model\n",
    "\n",
    "Let's get some practice with how to save and load trained Keras models \n",
    "\n",
    "For this experiment, review the section on Saving and Loading models from the guided project in order to help you to: \n",
    "\n",
    "- Build a model of your choosing\n",
    "- Gridsearch the model with a method of your choosing\n",
    "- Save the trained model to file\n",
    "- Load the trained model from file\n",
    "- Just as we did in the Guided Project, evalute the loaded model using a test set and make sure the results of the loaded model match that of the saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_434_Deploy_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
